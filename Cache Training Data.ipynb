{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/ritvik/Desktop/JPLProject/mapping-colombia-wetlands/')\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import pickle\n",
    "from common_functions import *\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rc('image', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the base folder where all data will be stored\n",
    "BASE_ROI_FOLDER = 'C:/Users/ritvik/Desktop/JPLProject/data/GoogleEarthEngineData/Sentinel2/roi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Download File from Google Drive"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following three functions are used to download a public file from Google Drive given the file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_google_drive(file_id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : file_id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : file_id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Cache Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interior_exterior_edge_values(mode, arr_train_labels, split_arr_features_train, no_data_value, kernel_size=3, thresh_learning=0.1, data_limit=100000):\n",
    "    \"\"\"\n",
    "    mode: a list with any of \"interior\", \"exterior\", \"edge\". Designates which data to return.\n",
    "    arr_train_labels: the array of training data wetland labels\n",
    "    split_arr_features_train: the features to use for training\n",
    "    no_data_value: the vaule used to signify no data at a pixel\n",
    "    kernel_size: the kernel size used in the process of identifying interior and enxterior points\n",
    "    thresh_learning: between 0 and 1, this threshold is used to distinuish interior (wetland) and exterior (non wetland) pixels\n",
    "    data_limit: the max number of interior/exterior/edge points to return\n",
    "    \"\"\"\n",
    "    \n",
    "    #get number of bands\n",
    "    num_bands = split_arr_features_train.shape[-1]\n",
    "    \n",
    "    #let the edge detection threshold be half of the learning threshold\n",
    "    thresh_edge_detection = thresh_learning / 2\n",
    "    \n",
    "    #apply a filter to the baseline wetlands image to find edge regions\n",
    "    kernel = -np.ones((kernel_size,kernel_size)) / (kernel_size**2 - 1)\n",
    "    kernel[kernel_size//2,kernel_size//2] = 1\n",
    "    \n",
    "    vicinity_score = abs(apply_convolution(arr_train_labels, kernel))\n",
    "    \n",
    "    interior, exterior, edges = None, None, None\n",
    "    \n",
    "    if 'interior' in mode:\n",
    "        #interior points are wetlands confidently below the threshold\n",
    "        interior_condition = (arr_train_labels==1)&(vicinity_score < thresh_learning)\n",
    "        interiors = np.where(interior_condition == 1)\n",
    "        interior_vicinity_scores = vicinity_score[interiors]\n",
    "        \n",
    "        if len(interiors[0]) == 0:\n",
    "            print('empty interior')\n",
    "            return None\n",
    "        \n",
    "        #get the values in the SAR image lining up with interior points, sample according to confidence\n",
    "        chosen_interiors = sample_by_vicinity_scores(interiors, interior_vicinity_scores, thresh_learning)\n",
    "        interior_vals = split_arr_features_train[chosen_interiors].reshape(-1, num_bands)\n",
    "        interior_vals = interior_vals[(interior_vals != no_data_value).all(axis=1)]\n",
    "        interior_sampled = np.random.choice(np.arange(interior_vals.shape[0]), min(data_limit, interior_vals.shape[0]), replace=False)\n",
    "        interior_vals = interior_vals[interior_sampled]\n",
    "        \n",
    "    if 'exterior' in mode:\n",
    "        #exterior points are non-wetlands confidently below the threshold\n",
    "        exterior_condition = (arr_train_labels==0)&(vicinity_score < thresh_learning)\n",
    "        exteriors = np.where(exterior_condition == 1)\n",
    "        exterior_vicinity_scores = vicinity_score[exteriors]\n",
    "        \n",
    "        if len(exteriors[0]) == 0:\n",
    "            print('empty exterior')\n",
    "            return None\n",
    "        \n",
    "        #get the values in the SAR image lining up with exterior points, sample according to confidence\n",
    "        chosen_exteriors = sample_by_vicinity_scores(exteriors, exterior_vicinity_scores, thresh_learning)\n",
    "        exterior_vals = split_arr_features_train[chosen_exteriors].reshape(-1, num_bands)\n",
    "        exterior_vals = exterior_vals[(exterior_vals != no_data_value).all(axis=1)]\n",
    "        exterior_sampled = np.random.choice(np.arange(exterior_vals.shape[0]), min(data_limit, exterior_vals.shape[0]), replace=False)\n",
    "        exterior_vals = exterior_vals[exterior_sampled]\n",
    "        \n",
    "    if 'edge' in mode:\n",
    "        #edge points are above the threshold\n",
    "        edge_condition = (vicinity_score >= thresh_learning)\n",
    "        edges = np.where(edge_condition == 1)\n",
    "        \n",
    "        if len(edges[0]) == 0:\n",
    "            print('empty edge')\n",
    "            return None\n",
    "    \n",
    "    return interior_vals, exterior_vals, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary of Folder Name to Google Drive File ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = {'inland_wetland_0_0': '1iA5XwNA7S8exKZXlbWFnxdLUWB8xGsGP', \n",
    "            'inland_wetland_0_1': '1XIUJJrW0i0pSrWcQ1uPwpR55OEF9FVG4',\n",
    "            'inland_wetland_0_2': '1Fo-iZ45nCewahN75C9Eg50Sp1HeIi5BO',\n",
    "            'inland_wetland_0_3': '1Bih2hBv_zZpDxnWSPg_n3y59VftCcWTK',\n",
    "            'inland_wetland_0_4': '156G5gd8LKh6kUqZVWhL8oVwr_1hZ-3-B',\n",
    "            \n",
    "            'inland_wetland_1_0': '', \n",
    "            'inland_wetland_1_1': '',\n",
    "            'inland_wetland_1_2': '',\n",
    "            'inland_wetland_1_3': '',\n",
    "            'inland_wetland_1_4': '',\n",
    "            \n",
    "            'inland_wetland_2_0': '',\n",
    "            'inland_wetland_2_1': '',\n",
    "            'inland_wetland_2_2': '',\n",
    "            'inland_wetland_2_3': '',\n",
    "            'inland_wetland_2_4': '',\n",
    "            \n",
    "            'inland_wetland_3_0': '', \n",
    "            'inland_wetland_3_1': '',\n",
    "            'inland_wetland_3_2': '',\n",
    "            'inland_wetland_3_3': '',\n",
    "            'inland_wetland_3_4': '',\n",
    "            \n",
    "            'inland_wetland_4_0': '', \n",
    "            'inland_wetland_4_1': '',\n",
    "            'inland_wetland_4_2': '',\n",
    "            'inland_wetland_4_3': '',\n",
    "            'inland_wetland_4_4': ''\n",
    "            \n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_training_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inland_wetland_0_0...\n",
      "Getting formatted features and labels...\n",
      "Getting Interior/Exterior/Edge info...\n",
      "Storing in dictionary...\n",
      "=======================================\n",
      "Processing inland_wetland_0_1...\n",
      "Downloading training data from drive...\n",
      "Getting formatted features and labels...\n",
      "Getting Interior/Exterior/Edge info...\n",
      "Storing in dictionary...\n",
      "=======================================\n",
      "Processing inland_wetland_0_2...\n",
      "Downloading training data from drive...\n",
      "Getting formatted features and labels...\n",
      "Getting Interior/Exterior/Edge info...\n",
      "Storing in dictionary...\n",
      "=======================================\n",
      "Processing inland_wetland_0_3...\n",
      "Downloading training data from drive...\n",
      "Getting formatted features and labels...\n",
      "Getting Interior/Exterior/Edge info...\n",
      "Storing in dictionary...\n",
      "=======================================\n",
      "Processing inland_wetland_0_4...\n",
      "Downloading training data from drive...\n",
      "Getting formatted features and labels...\n",
      "Getting Interior/Exterior/Edge info...\n",
      "Storing in dictionary...\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "for roi_folder, fid in file_ids.items():\n",
    "    if fid == '':\n",
    "        continue\n",
    "    print('Processing %s...'%roi_folder)\n",
    "    features_file_name = '%s/%s/features_%s.tiff'%(BASE_ROI_FOLDER, roi_folder, roi_folder)\n",
    "    baseline_file_name = '%s/%s/baseline_%s.tiff'%(BASE_ROI_FOLDER, roi_folder, roi_folder)\n",
    "    \n",
    "    #check if data already downloaded\n",
    "    if features_file_name.split('/')[-1] not in os.listdir('%s/%s'%(BASE_ROI_FOLDER, roi_folder)):\n",
    "        print('Downloading training data from drive...')\n",
    "        download_file_from_google_drive(fid, features_file_name)\n",
    "     \n",
    "    #check if training data already cached\n",
    "    if roi_folder not in stored_training_data:\n",
    "    \n",
    "        ds_features = gdal.Open(features_file_name, gdal.GA_ReadOnly)\n",
    "        ds_labels = gdal.Open(baseline_file_name, gdal.GA_ReadOnly)\n",
    "\n",
    "        print('Getting formatted features and labels...')\n",
    "        arr_labels, split_arr_features, gt = preprocess_data_set_pair(ds_features, ds_labels, 'Marsh') \n",
    "\n",
    "        print('Getting Interior/Exterior/Edge info...')\n",
    "        interior_vals, exterior_vals, edges = get_interior_exterior_edge_values(['interior', 'exterior'], arr_labels, split_arr_features, NO_DATA_VALUE)\n",
    "\n",
    "        print('Storing in dictionary...')\n",
    "        stored_training_data[roi_folder] = {}\n",
    "        stored_training_data[roi_folder]['interior'] = interior_vals\n",
    "        stored_training_data[roi_folder]['exterior'] = exterior_vals\n",
    "\n",
    "        pickle.dump(stored_training_data, open(\"stored_training_data.p\", \"wb\"))\n",
    "\n",
    "        print('=======================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
