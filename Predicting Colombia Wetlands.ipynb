{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import ee\n",
    "import requests\n",
    "import os\n",
    "from osgeo import gdal, ogr, osr\n",
    "import shutil\n",
    "from time import sleep\n",
    "from skimage.restoration import denoise_tv_bregman\n",
    "from math import ceil\n",
    "from googleapiclient.discovery import build\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.signal import convolve\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is about 30 meters\n",
    "RESOLUTION = 1/3600\n",
    "\n",
    "#the value we use to signify no data at a pixel\n",
    "NO_DATA_VALUE = 65535\n",
    "\n",
    "#bands to despeckle\n",
    "BANDS_TO_DESPECKLE = ['HH', 'HV']\n",
    "\n",
    "#store the training data here\n",
    "TRAINING_DATA_FOLDER = 'training_data'\n",
    "\n",
    "#store the prediction data here\n",
    "PREDICTION_DATA_FOLDER = 'prediction_data'\n",
    "\n",
    "#create training data folder if not exists\n",
    "if TRAINING_DATA_FOLDER not in os.listdir():\n",
    "    os.mkdir(TRAINING_DATA_FOLDER)\n",
    "\n",
    "#create prediction data folder if not exists\n",
    "if PREDICTION_DATA_FOLDER not in os.listdir():\n",
    "    os.mkdir(PREDICTION_DATA_FOLDER)\n",
    "    \n",
    "#features to extract from GEE in the training process\n",
    "FEATURES = {\n",
    "            ('collection','JAXA/ALOS/PALSAR/YEARLY/SAR'): ['HH', 'HV', 'qa'], \n",
    "            ('collection', 'LANDSAT/LC08/C01/T1_8DAY_NDVI'): ['NDVI'], \n",
    "            ('collection', 'LANDSAT/LC08/C01/T1_8DAY_NDWI'): ['NDWI'], \n",
    "            ('image','CGIAR/SRTM90_V4'): ['elevation']\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the shapefile storing training polygons\n",
    "TRAINING_POLYGONS_FILE = 'C:/Users/ritvik/Desktop/JPLProject/mapping-colombia-wetlands/training_polygons/training_polygons.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the shapefile storing prediction polygons\n",
    "PREDICTION_POLYGONS_FILE = 'C:/Users/ritvik/Desktop/JPLProject/mapping-colombia-wetlands/prediction_polygons/prediction_polygons.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the list of bands to use for training. Choose from:\n",
    "#Landsat: ['NDVI', 'NDWI']\n",
    "#ALOS-2: ['HH', 'HV']\n",
    "#CGIAR: ['elevation']\n",
    "\n",
    "SELECTED_BANDS = ['NDVI', 'NDWI', 'HH', 'HV', 'elevation']\n",
    "NUM_FEATURES = len(SELECTED_BANDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#any pixels above this elevation (in meters) will be disregarded from training \n",
    "MAX_CONSIDERED_ELEVATION = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the folder id in Google Drive where to temporarily store the GEE data before locally downloading\n",
    "GOOGLE_EARTH_ENGINE_GDRIVE_FOLDER_ID = '1KvlrUHs_rN7xPlw53qtd9pweeLwmrJSP'\n",
    "\n",
    "#the name of that same Google Drive folder\n",
    "GDRIVE_FOLDER_NAME = 'GoogleEarthEngine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data date range\n",
    "DATE_RANGE = ['2017-01-01', '2020-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to use for prediction. Choices are 'histogram' or 'random_forest'\n",
    "METHOD = 'histogram'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Manipulate data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_google_drive(file_id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "    \n",
    "    max_tries = 10\n",
    "    curr_try = 0\n",
    "    status_code = -1\n",
    "    \n",
    "    while status_code != 200 and curr_try < max_tries:\n",
    "        if curr_try > 0:\n",
    "            sleep(30)\n",
    "        session = requests.Session()\n",
    "        response = session.get(URL, params = { 'id' : file_id }, stream = True)\n",
    "        status_code = response.status_code\n",
    "        curr_try += 1\n",
    "        \n",
    "    if status_code != 200:\n",
    "        return\n",
    "    \n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : file_id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_ids_from_google_drive():\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    result = service.files().list(q=\"parents in '%s'\"%GOOGLE_EARTH_ENGINE_GDRIVE_FOLDER_ID).execute()\n",
    "\n",
    "    file_name_to_file_id = {info['name'].split('-')[0]: info['id'] for info in result['files'] if len(info['name'].split('-')) == 2}\n",
    "    \n",
    "    return file_name_to_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file_from_google_drive_by_file_id(fid):\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    \n",
    "    service.files().delete(fileId=fid).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Download Data From Google Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(polygon_features, features, gdrive_folder, date_range, selected_bands):\n",
    "    \"\"\"\n",
    "    This function accepts the below parameters and querys Google Earth Engine for data. The data is stored in \n",
    "    Google Drive.\n",
    "    \n",
    "    Inputs:\n",
    "        polygon_features: a list of pairs like (index, polygon feature) indicating which polygons of data to download\n",
    "        features: the features to extract from GEE\n",
    "        gdrive_folder: the name of the folder in Google Drive where the downloaded data will live\n",
    "        date_range: the start and end date to gather data\n",
    "        selected_bands: list of bands to subset\n",
    "        \n",
    "    Output:\n",
    "        list of tasks which are ready to be started\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store all started tasks\n",
    "    tasks = {}\n",
    "    \n",
    "    #work through each sub-region \n",
    "    for curr_idx, polygon_feature in polygon_features:\n",
    "        \n",
    "        skip_polygon = False\n",
    "        \n",
    "        filtered_imgs = []\n",
    "\n",
    "        #store the reference coordinates\n",
    "        x1 = polygon_feature.GetGeometryRef().GetEnvelope()[0]\n",
    "        y1 = polygon_feature.GetGeometryRef().GetEnvelope()[2]\n",
    "        ref_coords = (x1,y1)\n",
    "        \n",
    "        #get polygon area coordinates\n",
    "        area_coords = [list(pair) for pair in polygon_feature.GetGeometryRef().GetBoundary().GetPoints()]\n",
    "\n",
    "        #create an area of interest from Earth Engine Geometry\n",
    "        area_of_interest = ee.Geometry.Polygon(coords=area_coords)\n",
    "\n",
    "        #iterate over each data source\n",
    "        for data_type_source, bands in features.items():\n",
    "            data_type = data_type_source[0]\n",
    "            data_source = data_type_source[1]\n",
    "                \n",
    "            print('Working on data source: %s...'%data_source)\n",
    "            \n",
    "            if data_type == 'collection':\n",
    "                #access the Earth Engine image collection with the specified bands\n",
    "                data = ee.ImageCollection(data_source).select(bands)\n",
    "\n",
    "                #filter on date range and area of interest\n",
    "                data_filtered = data.filterBounds(area_of_interest).filterDate(date_range[0], date_range[1])\n",
    "                \n",
    "                #limit on cloud cover if LANDSAT\n",
    "                if data_source == 'LANDSAT/LC08/C01/T1_SR':\n",
    "                    data_filtered = data_filtered.filterMetadata('CLOUD_COVER', 'less_than', 20)\n",
    "                    \n",
    "                #ensure there is at least 1 image\n",
    "                num_items = data_filtered.size().getInfo()\n",
    "                if num_items == 0:\n",
    "                    skip_polygon = True\n",
    "                    break\n",
    "\n",
    "                #if LANDSAT NDVI band, get quality mosaic by that band\n",
    "                if 'LANSAT' in data_source:\n",
    "                    mosaic = data_filtered.qualityMosaic(data_source.split('_')[-1])\n",
    "                #otherwise just do a simple median\n",
    "                else:\n",
    "                    mosaic = data_filtered.median()\n",
    "                   \n",
    "            elif data_type == 'image':\n",
    "                mosaic = ee.Image(data_source).select(bands)\n",
    "\n",
    "            #add this mosaic to the list\n",
    "            filtered_imgs.append(mosaic)\n",
    "        \n",
    "        if skip_polygon:\n",
    "            print('Skipping %s'%fname)\n",
    "            tasks[fname] = None\n",
    "            print('==================================')\n",
    "            continue\n",
    "            \n",
    "        #generate file name\n",
    "        features_str = '_'.join([item[1] for item in features.keys()]).replace('/','_')\n",
    "        fname = '%s-%s'%(curr_idx, features_str)\n",
    "        print(fname)\n",
    "        \n",
    "        #add the various layers on top of each other to create a data cube with all features\n",
    "        final_img = ee.Image()\n",
    "        \n",
    "        for img in filtered_imgs:\n",
    "            final_img = ee.Image.addBands(final_img,img)\n",
    "        \n",
    "        #use the ALOS qa band to filter out invalid pixels\n",
    "        if 'qa' in features[('collection','JAXA/ALOS/PALSAR/YEARLY/SAR')]:\n",
    "            qa_band = final_img.select('qa')\n",
    "            qa_mask = qa_band.eq(0)\n",
    "            final_img = final_img.where(qa_mask, NO_DATA_VALUE)\n",
    "            \n",
    "        #use the SRTM elevation band to filter out invaild pixels\n",
    "        if 'elevation' in features['image','CGIAR/SRTM90_V4']:\n",
    "            elevation_band = final_img.select('elevation')\n",
    "            elevation_mask = elevation_band.gt(MAX_CONSIDERED_ELEVATION)\n",
    "            final_img = final_img.where(elevation_mask, NO_DATA_VALUE)\n",
    "            \n",
    "        #if any of the selected bands has NO_DATA_VALUE, mark that whole pixel as NO_DATA_VALUE\n",
    "        for b in selected_bands:\n",
    "            b_values = final_img.select(b)\n",
    "            b_mask = b_values.eq(NO_DATA_VALUE)\n",
    "            final_img = final_img.where(b_mask, NO_DATA_VALUE)\n",
    "         \n",
    "        #store the result with just the needed bands\n",
    "        selected_bands = sorted(selected_bands)\n",
    "        result = final_img.select(*selected_bands).float()\n",
    "          \n",
    "        #define the task to gather the data\n",
    "        task = ee.batch.Export.image.toDrive(image=result,\n",
    "                                             region=area_of_interest.getInfo()['coordinates'],\n",
    "                                             description=str(curr_idx),\n",
    "                                             folder=gdrive_folder,\n",
    "                                             fileNamePrefix=fname,\n",
    "                                             crs_transform=[RESOLUTION, 0.0, ref_coords[0], 0.0, -RESOLUTION, ref_coords[1]],\n",
    "                                             crs='EPSG:4326')\n",
    "        \n",
    "        #store the task\n",
    "        tasks[fname] = task\n",
    "        \n",
    "        print('==================================')\n",
    "        \n",
    "    return list(tasks.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tasks_in_batches(tasks, batch_size, FOLDER):\n",
    "    \"\"\"\n",
    "    Executes a list of tasks in batches\n",
    "    \n",
    "    Inputs:\n",
    "        tasks: list of tasks\n",
    "        batch_size: number of tasks to execute per batch\n",
    "        FOLDER: the folder to store the downloaded rasters\n",
    "    \"\"\"\n",
    "    \n",
    "    #create mapping of polygon to fname\n",
    "    polygon_to_fname = {}\n",
    "\n",
    "    #process the tasks in small batches to avoid memory running out\n",
    "    for batch_idx in range(ceil(len(tasks) / batch_size)):\n",
    "\n",
    "        #get the current batch of tasks\n",
    "        curr_tasks = tasks[batch_size*batch_idx:batch_size*(batch_idx+1)]\n",
    "        print('Processing Batch %s'%(batch_idx+1))\n",
    "\n",
    "        #start all tasks in that batch\n",
    "        for name,task in curr_tasks:\n",
    "            if task != None:\n",
    "                task.start()\n",
    "\n",
    "        print('Started all tasks in batch')\n",
    "\n",
    "        #wait until all tasks in that batch are done\n",
    "        curr_states = [task.status()['state'] for name,task in curr_tasks if task != None]\n",
    "        while 'RUNNING' in set(curr_states) or 'READY' in set(curr_states):\n",
    "            print('Current states: %s'%curr_states)\n",
    "            sleep(30)\n",
    "            curr_states = [task.status()['state'] for name,task in curr_tasks if task != None]\n",
    "\n",
    "        #once all tasks done, get their file ids on google drive\n",
    "        file_name_to_file_id = get_file_ids_from_google_drive()\n",
    "\n",
    "        #for each file...\n",
    "        for fname, fid in file_name_to_file_id.items():\n",
    "\n",
    "            #get feature file name\n",
    "            features_file_name = '%s/features_%s.tiff'%(FOLDER, fname)\n",
    "\n",
    "            #check if data already downloaded\n",
    "            print('Downloading %s from Drive'%fname)\n",
    "            download_file_from_google_drive(fid, features_file_name)\n",
    "\n",
    "            print('Deleting %s from Drive'%fname)\n",
    "            delete_file_from_google_drive_by_file_id(fid)\n",
    "\n",
    "        print('================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Despeckling and Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_db(img):\n",
    "    return 10 * np.log10(img)\n",
    "\n",
    "def db_to_img(img):\n",
    "    return 10**(img / 10)\n",
    "\n",
    "def tv_denoise(arr, idxs_to_despeckle, weight):\n",
    "    copy_arr = arr.copy()\n",
    "    for idx in idxs_to_despeckle:\n",
    "        #get the layer\n",
    "        layer = copy_arr[:,:,idx]\n",
    "        \n",
    "        orig_valid_mask = ~np.isnan(layer)\n",
    "        \n",
    "        #denoise\n",
    "        img_db = img_to_db(layer)\n",
    "        img_db_tv = denoise_tv_bregman(img_db, weight)\n",
    "        img_tv = db_to_img(img_db_tv)\n",
    "        img_tv[orig_valid_mask & np.isnan(img_tv)] = layer[orig_valid_mask & np.isnan(img_tv)]\n",
    "        \n",
    "        #set denoised into copy of array\n",
    "        copy_arr[:,:,idx] = img_tv\n",
    "        \n",
    "    return copy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sep_metric(feat_file_name, num_clusters_options, band_names_options):\n",
    "    \"\"\"\n",
    "    Get the separability metric for the given array and given possible clusters\n",
    "    \n",
    "    Inputs:\n",
    "        feat_file_name: path to file to analyze\n",
    "        num_clusters_options: list of number of clusters to try\n",
    "        band_names_options: name of the bands to try for clustering\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #read the array\n",
    "    ds = gdal.Open(feat_file_name, gdal.GA_ReadOnly)\n",
    "\n",
    "    #get the band names\n",
    "    band_names = [ds.GetRasterBand(idx+1).GetDescription() for idx in range(ds.RasterCount)]\n",
    "\n",
    "    #get gt and read array\n",
    "    gt = ds.GetGeoTransform()\n",
    "    arr = ds.ReadAsArray()\n",
    "    arr = np.stack([arr[i] for i in range(arr.shape[0])], axis=-1)\n",
    "    ds = None\n",
    "\n",
    "    #transform array to 2d\n",
    "    arr_2d = arr.reshape(-1,arr.shape[-1])\n",
    "    arr_2d[arr_2d == NO_DATA_VALUE] = np.nan\n",
    "    valid_indices = np.where(np.all(~np.isnan(arr_2d), axis=-1))[0]\n",
    "    arr_2d_valid = arr_2d[valid_indices]\n",
    "    \n",
    "    sep_metric_dict = {}\n",
    "    \n",
    "    for k in num_clusters_options:\n",
    "        for b in band_names_options:\n",
    "            \n",
    "            band_idx = band_names.index(b)\n",
    "        \n",
    "            #define model\n",
    "            model = KMeans(n_clusters=k)\n",
    "\n",
    "            #fit model and predict clusters\n",
    "            cluster_preds = model.fit_predict(arr_2d_valid[:,[band_idx]])\n",
    "\n",
    "            mu_vals = np.zeros(k)\n",
    "            dev_vals = np.zeros(k)\n",
    "\n",
    "            for cid in range(k):\n",
    "                cluster_data = arr_2d_valid[cluster_preds == cid]\n",
    "                mu, dev = cluster_data.mean(), cluster_data.std()\n",
    "                mu_vals[cid] = mu\n",
    "                dev_vals[cid] = dev\n",
    "\n",
    "            sep_metric_vals = []\n",
    "            for c1 in range(k):\n",
    "                for c2 in range(c1+1,k):\n",
    "                    sep_metric_vals.append(abs(mu_vals[c1] - mu_vals[c2]) / (dev_vals[c1] + dev_vals[c2]))\n",
    "                    \n",
    "            sep_metric_dict[(b,k)] = {'sep_metric_vals': sep_metric_vals, 'cluster_preds': cluster_preds}\n",
    "    \n",
    "    best_sep_entry = sorted(sep_metric_dict.items(), key=lambda info: -np.min(info[1]['sep_metric_vals']))[0]\n",
    "    print('Best Separating Params: %s'%str(best_sep_entry[0]))\n",
    "    \n",
    "    result = np.ones(arr_2d.shape[0])*-1\n",
    "    result[valid_indices] = best_sep_entry[1]['cluster_preds']\n",
    "    result = result.reshape(arr.shape[:2])\n",
    "\n",
    "    ds = np_array_to_raster('%s_suggested.tiff'%feat_file_name.split('.')[-2], result, gt, no_data=-1, nband=1, gdal_data_type=gdal.GDT_Float64)\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Process Feature Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_files(feat_file_names, confidence_levels=None, preprocess=True):\n",
    "    \"\"\"\n",
    "    This function accepts a list of file names and processes those rasters. \n",
    "    \n",
    "    Inputs:\n",
    "        feat_file_names: a list of names of the downloaded training data files\n",
    "        confidence_levels: a list of confidence levels associated with each file in feat_file_names\n",
    "        preprocess: whether to remove some pixels that likely do not belong\n",
    "        \n",
    "    Outputs:\n",
    "        the processed training data and auxilary data such as geotransforms\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store the numpy array of training data for each file\n",
    "    data = {}\n",
    "    \n",
    "    #this will store auxilary data for each file\n",
    "    feat_file_data = {}\n",
    "    \n",
    "    #iterate over each file\n",
    "    for feat_file_name in feat_file_names:\n",
    "\n",
    "        #open file and get geotransform\n",
    "        ds = gdal.Open(feat_file_name, gdal.GA_ReadOnly)\n",
    "        try:\n",
    "            gt = ds.GetGeoTransform()\n",
    "        except AttributeError:\n",
    "            print('Could not process %s'%feat_file_name)\n",
    "            continue\n",
    "\n",
    "        #despeckle any bands which need to be despeckled\n",
    "        idx_to_despeckle = [idx for idx in range(ds.RasterCount) if ds.GetRasterBand(idx+1).GetDescription() in BANDS_TO_DESPECKLE]\n",
    "        arr = ds.ReadAsArray()\n",
    "        arr = np.stack([arr[i] for i in range(arr.shape[0])], axis=-1)\n",
    "        data_mask = (arr == NO_DATA_VALUE) | np.isnan(arr)\n",
    "        arr[data_mask] = NO_DATA_VALUE\n",
    "\n",
    "        arr = tv_denoise(arr, idx_to_despeckle, 1)\n",
    "        arr[data_mask] = np.nan\n",
    "\n",
    "        ds = None\n",
    "        \n",
    "        arr_2d = arr.reshape(-1,arr.shape[-1])\n",
    "        valid_indices = np.where(np.all(~np.isnan(arr_2d), axis=-1))[0]\n",
    "    \n",
    "        if preprocess:\n",
    "            #define model\n",
    "            model = KMeans(n_clusters=2)\n",
    "\n",
    "            #fit model and predict clusters\n",
    "            try:\n",
    "                cluster_preds = model.fit_predict(arr_2d[valid_indices])\n",
    "            except ValueError:\n",
    "                print('Could not process %s'%feat_file_name)\n",
    "                continue\n",
    "\n",
    "            #get main cluster indices\n",
    "            main_cluster = np.median(cluster_preds)\n",
    "            main_cluster_indices = np.where(cluster_preds == main_cluster)[0]\n",
    "\n",
    "            #refine chosen indices\n",
    "            valid_indices = valid_indices[main_cluster_indices]\n",
    "\n",
    "        #add this training data to the list\n",
    "        data[feat_file_name] = arr_2d[valid_indices]\n",
    "        \n",
    "        #add auxilary information\n",
    "        feat_file_data[feat_file_name] = {'chosen_indices': valid_indices, 'shape': arr.shape, 'gt': gt}\n",
    "        \n",
    "    #if confidence levels supplied, then sample according to those levels\n",
    "    if confidence_levels != None:\n",
    "        num_pixels_to_sample = np.median([item.shape[0] for item in data.values()])\n",
    "        data = {fname: ds[np.random.choice(ds.shape[0], int(num_pixels_to_sample*confidence_levels[fname]))] for fname,ds in data.items()}\n",
    "\n",
    "    return data, feat_file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_histograms(training_datasets):\n",
    "    \"\"\"\n",
    "    Creates a set of histograms, one for each class\n",
    "    \n",
    "    Inputs:\n",
    "        training_histograms: a dictionary mapping class_id to a dataset\n",
    "        \n",
    "    Outputs:\n",
    "        a dictionary mapping class_id to a histogram\n",
    "        a list of histogram bin cuttoffs for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    min_feature_values = [min([min(dataset[:,idx]) for dataset in training_datasets.values()]) for idx in range(NUM_FEATURES)]\n",
    "    max_feature_values = [max([max(dataset[:,idx]) for dataset in training_datasets.values()]) for idx in range(NUM_FEATURES)]\n",
    "    \n",
    "    histogram_ranges = []\n",
    "    training_histograms = {}\n",
    "    num_bins = 5\n",
    "\n",
    "    for idx in range(NUM_FEATURES):\n",
    "        width = (max_feature_values[idx] - min_feature_values[idx]) / num_bins\n",
    "        histogram_ranges.append(np.arange(min_feature_values[idx], max_feature_values[idx]+width*.99, width))\n",
    "    histogram_ranges = np.array(histogram_ranges)\n",
    "\n",
    "    for class_id in class_ids:\n",
    "        training_histograms[class_id] = np.histogramdd(training_datasets[class_id], bins=histogram_ranges, density=True)[0] \n",
    "        \n",
    "    return training_histograms, histogram_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(test_set, training_histograms, histogram_ranges, class_ids, frac=0.25):\n",
    "    \"\"\"\n",
    "    This function accepts a data set and histograms and classifies each pixel and assigns a score\n",
    "    \n",
    "    Inputs:\n",
    "        test_set: the test set of features which we would like to classify\n",
    "        training_histograms: a dictionary of histograms, one for each class\n",
    "        histogram_ranges: the bin cuttoffs for the histograms\n",
    "        class_ids: a set of class ids\n",
    "        frac: between 0 and 1. Higher values mean we require more confidence to classify a pixel as non-NaN\n",
    "        \n",
    "    Output:\n",
    "        an array of predicted classes and corresponding scores\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store the probabilities for each class\n",
    "    class_id_to_probs = {}\n",
    "    \n",
    "    #any probability density below this is considered as 0\n",
    "    min_allowable_density = min([frac*np.max(training_histograms[cid]) for cid in class_ids])\n",
    "    \n",
    "    #iterate over each histogram\n",
    "    for class_id, histogram in training_histograms.items():\n",
    "        \n",
    "        #get the position of each test pixel in the context of this histogram\n",
    "        transposed_ranges = np.transpose(histogram_ranges)\n",
    "        expanded_dims_ranges = np.expand_dims(transposed_ranges, axis=1)\n",
    "        extended_ranges = np.concatenate([expanded_dims_ranges for _ in range(test_set.shape[0])], axis=1)\n",
    "        diffs = extended_ranges - test_set\n",
    "    \n",
    "        #get the probability density at each position\n",
    "        indices = np.argmax(diffs > 0, axis=0) - 1\n",
    "        indices[indices < 0] = 0\n",
    "        indices = indices - np.all((diffs>0)==False, axis=0)\n",
    "        indices = tuple(np.transpose(indices))\n",
    "        probs = histogram[indices]\n",
    "        \n",
    "        #store this in the dictionary\n",
    "        class_id_to_probs[class_id] = probs\n",
    "      \n",
    "    #create matrix of probabilities for each class\n",
    "    prob_mtx = np.stack([class_id_to_probs[cid] for cid in class_ids], axis=-1)\n",
    "    \n",
    "    #sort matrix of probs\n",
    "    sorted_probs = np.sort(prob_mtx, axis=1)\n",
    "    \n",
    "    #any pixel where all classes have 0 probability is NaN\n",
    "    nan_indices = np.where(sorted_probs[:,-1] < min_allowable_density)[0]\n",
    "    \n",
    "    #compute scores based on ratio of most likely class to second most likely class\n",
    "    scores = 1/(1+np.exp(-(sorted_probs[:,-1] / sorted_probs[:,-2])))\n",
    "    \n",
    "    #apply NaN pixels\n",
    "    scores[nan_indices] = np.nan\n",
    "\n",
    "    #get the predicted class\n",
    "    pred_class = np.argmax(prob_mtx, axis=1).astype(float)\n",
    "    \n",
    "    #apply NaN pixels\n",
    "    pred_class[nan_indices] = np.nan\n",
    "    \n",
    "    return pred_class, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Array to TIFF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raster(output_path, columns, rows, nband=1, gdal_data_type=gdal.GDT_Int32, driver=r'GTiff'):\n",
    "    ''' \n",
    "    returns gdal data source raster object \n",
    "    '''\n",
    "    \n",
    "    # create driver\n",
    "    driver = gdal.GetDriverByName(driver)\n",
    "\n",
    "    output_raster = driver.Create(output_path, columns, rows, nband, eType = gdal_data_type)    \n",
    "    \n",
    "    return output_raster\n",
    "\n",
    "def np_array_to_raster(output_path, arr, geotransform, no_data=None, nband=1, gdal_data_type=gdal.GDT_Int32, spatial_reference_system_wkid=4326, driver=r'GTiff'):\n",
    "    ''' \n",
    "    returns a gdal raster data source\n",
    "\n",
    "    keyword arguments:\n",
    "\n",
    "    output_path -- full path to the raster to be written to disk\n",
    "    numpy_array -- numpy array containing data to write to raster\n",
    "    upper_left_tuple -- the upper left point of the numpy array (should be a tuple structured as (x, y))\n",
    "    cell_resolution -- the cell resolution of the output raster\n",
    "    no_data -- value in numpy array that should be treated as no data\n",
    "    nband -- the band to write to in the output raster\n",
    "    gdal_data_type -- gdal data type of raster (see gdal documentation for list of values)\n",
    "    spatial_reference_system_wkid -- well known id (wkid) of the spatial reference of the data\n",
    "    driver -- string value of the gdal driver to use\n",
    "    '''\n",
    "\n",
    "    rows, columns = arr.shape[0], arr.shape[1]\n",
    "\n",
    "    # create output raster\n",
    "    output_raster = create_raster(output_path, columns, rows, nband, gdal_data_type) \n",
    "\n",
    "    spatial_reference = osr.SpatialReference()\n",
    "    spatial_reference.ImportFromEPSG(spatial_reference_system_wkid)\n",
    "    output_raster.SetProjection(spatial_reference.ExportToWkt())\n",
    "    output_raster.SetGeoTransform(geotransform)\n",
    "    \n",
    "    for band_idx in range(1,nband+1):\n",
    "        output_band = output_raster.GetRasterBand(band_idx)\n",
    "        if no_data != None:\n",
    "            output_band.SetNoDataValue(no_data)\n",
    "        if nband > 1:\n",
    "            output_band.WriteArray(arr[:,:,band_idx-1])\n",
    "        else:\n",
    "            output_band.WriteArray(arr)\n",
    "        output_band.FlushCache() \n",
    "        output_band.ComputeStatistics(False)\n",
    "\n",
    "    if os.path.exists(output_path) == False:\n",
    "        raise Exception('Failed to create raster: %s' % output_path)\n",
    "\n",
    "    return output_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Download Prediction Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "dataSource = driver.Open(PREDICTION_POLYGONS_FILE, gdal.GA_ReadOnly)\n",
    "layer = dataSource.GetLayer()\n",
    "\n",
    "prediction_polygon_features = [layer.GetNextFeature() for _ in range(layer.GetFeatureCount())]\n",
    "prediction_polygon_features_to_process = [(idx,f) for idx,f in enumerate(prediction_polygon_features) if 'features_%s.tiff'%(idx) not in os.listdir(PREDICTION_DATA_FOLDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = get_training_data(prediction_polygon_features_to_process, FEATURES, GDRIVE_FOLDER_NAME, DATE_RANGE, SELECTED_BANDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_tasks_in_batches(tasks, 3, PREDICTION_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_feat_file_names = ['%s/%s'%(PREDICTION_DATA_FOLDER, fname) for fname in os.listdir(PREDICTION_DATA_FOLDER) if 'suggested' not in fname and 'predicted' not in fname]\n",
    "prediction_data, prediction_feat_file_data = process_feature_files(prediction_feat_file_names, confidence_levels=None, preprocess=False)\n",
    "print('Processed Prediction Regions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Get Suggested Number of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters_options = [3,4]\n",
    "\n",
    "sep_bands = [b for b in SELECTED_BANDS if b not in BANDS_TO_DESPECKLE]\n",
    "\n",
    "for feat_file_name in prediction_feat_file_names:\n",
    "    print('Processing %s'%feat_file_name)\n",
    "    get_sep_metric(feat_file_name, num_clusters_options, sep_bands)\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Download Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "\n",
    "dataSource = driver.Open(TRAINING_POLYGONS_FILE, gdal.GA_ReadOnly)\n",
    "\n",
    "layer = dataSource.GetLayer()\n",
    "\n",
    "training_polygon_features = [layer.GetNextFeature() for _ in range(layer.GetFeatureCount())]\n",
    "training_polygon_features_to_process = [(idx,f) for idx,f in enumerate(training_polygon_features) if 'features_%s.tiff'%idx not in os.listdir(TRAINING_DATA_FOLDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tasks = get_training_data(training_polygon_features_to_process, FEATURES, GDRIVE_FOLDER_NAME, DATE_RANGE, SELECTED_BANDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_tasks_in_batches(tasks, 5, TRAINING_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_feat_file_names = ['%s/%s'%(TRAINING_DATA_FOLDER, fname) for fname in os.listdir(TRAINING_DATA_FOLDER) if 'suggested' not in fname and 'predicted' not in fname]\n",
    "confidence_levels = {name: training_polygon_features[int(''.join([i for i in name if i.isdigit()]))].GetField('confidence') for name in training_feat_file_names}\n",
    "\n",
    "#this creates a dictionary mapping training data file name to the corresponding processed data set\n",
    "input_to_dataset, _ = process_feature_files(training_feat_file_names, confidence_levels, preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dictionary maps prediction data file name to class id to input data file name\n",
    "pred_to_class_to_input = {'%s/%s'%(PREDICTION_DATA_FOLDER, pred_fname): {} for pred_fname in os.listdir(PREDICTION_DATA_FOLDER) if 'suggested' not in pred_fname and 'predicted' not in pred_fname}\n",
    "\n",
    "#for each training polygon...\n",
    "for idx_input, input_poly in enumerate(training_polygon_features):\n",
    "    #check if its data downloaded successfully\n",
    "    if 'features_%s.tiff'%idx_input not in os.listdir(TRAINING_DATA_FOLDER):\n",
    "        continue\n",
    "        \n",
    "    #get file name\n",
    "    input_fname = '%s/features_%s.tiff'%(TRAINING_DATA_FOLDER, idx_input)\n",
    "  \n",
    "    #for each prediction polygon\n",
    "    for idx_pred, pred_poly in enumerate(prediction_polygon_features):\n",
    "        #check if its data downloaded successfully\n",
    "        if 'features_%s.tiff'%idx_pred not in os.listdir(PREDICTION_DATA_FOLDER):\n",
    "            continue\n",
    "        \n",
    "        #get file name\n",
    "        pred_fname = '%s/features_%s.tiff'%(PREDICTION_DATA_FOLDER, idx_pred)\n",
    "        \n",
    "        #check if this training polygon inside the prediction polygon\n",
    "        if input_poly.GetGeometryRef().Centroid().Within(pred_poly.GetGeometryRef()):\n",
    "            \n",
    "            #get class id\n",
    "            class_id = input_poly.GetField('class_id')\n",
    "            \n",
    "            #add this training file to files belonging to this prediction file\n",
    "            if class_id in pred_to_class_to_input[pred_fname]:\n",
    "                pred_to_class_to_input[pred_fname][class_id].append(input_fname)\n",
    "            else:\n",
    "                pred_to_class_to_input[pred_fname][class_id] = [input_fname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each prediction file, construct full dataset\n",
    "for pred_fname, mapping_data in pred_to_class_to_input.items():\n",
    "    for class_id in pred_to_class_to_input[pred_fname]:\n",
    "        pred_to_class_to_input[pred_fname][class_id] = \\\n",
    "        np.concatenate([input_to_dataset[fname] for fname in pred_to_class_to_input[pred_fname][class_id] if fname in input_to_dataset], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Classify Prediction Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty histograms and histogram ranges dictionaries\n",
    "histograms_dict = {}\n",
    "histogram_ranges_dict = {}\n",
    "\n",
    "random_forest_clf_dict = {}\n",
    "\n",
    "\n",
    "for pred_fname in pred_to_class_to_input:\n",
    "    \n",
    "    class_ids = sorted(list(pred_to_class_to_input[pred_fname].keys()))\n",
    "    \n",
    "    #create histograms and histogram ranges for this prediction polygon\n",
    "    histograms, histogram_ranges = create_histograms(pred_to_class_to_input[pred_fname])\n",
    "    histograms_dict[pred_fname] = histograms\n",
    "    histogram_ranges_dict[pred_fname] = histogram_ranges\n",
    "    \n",
    "    #store constructed datasets\n",
    "    features = np.concatenate([pred_to_class_to_input[pred_fname][cid] for cid in class_ids], axis=0)\n",
    "    labels = np.array([single for item in [[cid]*len(pred_to_class_to_input[pred_fname][idx]) for idx,cid in enumerate(class_ids)] for single in item])\n",
    "    \n",
    "    #fit a random forest classifier\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(features, labels)\n",
    "    random_forest_clf_dict[pred_fname] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if METHOD == 'histogram':\n",
    "    #get the predicted classes and scores for histogram method\n",
    "    pred_class_scores = [get_classes(d, histograms_dict[fname], histogram_ranges_dict[fname], sorted(list(histograms_dict[fname].keys())), 0) for fname,d in prediction_data.items()]\n",
    "elif METHOD == 'random_forest':\n",
    "    #get the predicted classes and scores for random forest method\n",
    "    pred_class_scores = [[random_forest_clf_dict[fname].predict(d), np.sort(random_forest_clf_dict[fname].predict_proba(d), axis=1)[:,-2:]] for fname,d in prediction_data.items()]\n",
    "    scores = [2/(1+np.exp(1-item[1][:,-1] / item[1][:,-2]))-1 for item in pred_class_scores]\n",
    "    pred_class_scores = [[pred_class_scores[idx][0], scores[idx]] for idx in range(len(scores))]\n",
    "    \n",
    "print('Got Predicted Classes and Scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#this kernel will be used for the low pass filter\n",
    "ksize = 5\n",
    "kernel = np.ones((ksize,ksize)) / ksize**2\n",
    "\n",
    "#for each prediction file...\n",
    "for idx, feat_file_name in enumerate(prediction_feat_file_names):\n",
    "    \n",
    "    #get the shape of the file\n",
    "    feat_file_shape = prediction_feat_file_data[feat_file_name]['shape'][:2]\n",
    "    shape = feat_file_shape + (2,)\n",
    "    \n",
    "    #create empty matrix to store result\n",
    "    result = np.empty(shape)\n",
    "    result = result.reshape(-1, 2)\n",
    "    result[:] = np.nan\n",
    "    \n",
    "    #these are the valid indices \n",
    "    indices = prediction_feat_file_data[feat_file_name]['chosen_indices']\n",
    "\n",
    "    #load the predicted classes and scores\n",
    "    result[indices, 0] = pred_class_scores[idx][0]\n",
    "    result[indices, 1] = pred_class_scores[idx][1]\n",
    "\n",
    "    #shape back into a 3d matrix\n",
    "    result = result.reshape(shape)\n",
    "    mask = np.isnan(result[:,:,0])\n",
    "    \n",
    "    #get unique class ids\n",
    "    class_ids = np.sort(np.unique(result[:,:,0][result[:,:,0] >= 0]).astype(int))\n",
    "    class_scores = np.zeros(feat_file_shape + (len(class_ids),))\n",
    "    \n",
    "    #get the matrix of T/F wheter each pixel is predicted as each class\n",
    "    class_mtxs = {cid: (result[:,:,0] == cid) for cid in class_ids}\n",
    "    \n",
    "    #for each class id...\n",
    "    for cid in class_ids:\n",
    "        #convolve with the low pass filter\n",
    "        conv_mtx = convolve(class_mtxs[cid], kernel, mode='same')\n",
    "        conv_mtx[np.isnan(result[:,:,0])] = np.nan\n",
    "        class_scores[:,:,cid] = conv_mtx\n",
    "    \n",
    "    result[:,:,0] = np.argmax(class_scores, axis=-1)\n",
    "    result[:,:,0][mask] = np.nan\n",
    "\n",
    "    ds = np_array_to_raster('%s/%s_predicted_%s.tiff'%(PREDICTION_DATA_FOLDER, METHOD, idx), result, prediction_feat_file_data[feat_file_name]['gt'], no_data=-1, nband=2, gdal_data_type=gdal.GDT_Float64)\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
