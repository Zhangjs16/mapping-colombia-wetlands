{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import ee\n",
    "import requests\n",
    "import os\n",
    "from osgeo import gdal, ogr, osr\n",
    "import shutil\n",
    "from time import sleep\n",
    "from skimage.restoration import denoise_tv_bregman\n",
    "from math import ceil\n",
    "from googleapiclient.discovery import build\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.signal import convolve\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is about 30 meters\n",
    "RESOLUTION = 1/3600\n",
    "\n",
    "#the value we use to signify no data at a pixel\n",
    "NO_DATA_VALUE = 65535\n",
    "\n",
    "#bands to despeckle\n",
    "BANDS_TO_DESPECKLE = ['HH', 'HV']\n",
    "\n",
    "#store the training data here\n",
    "TRAINING_DATA_FOLDER = 'training_data'\n",
    "\n",
    "#store the prediction data here\n",
    "PREDICTION_DATA_FOLDER = 'prediction_data'\n",
    "\n",
    "#create training data folder if not exists\n",
    "if TRAINING_DATA_FOLDER not in os.listdir():\n",
    "    os.mkdir(TRAINING_DATA_FOLDER)\n",
    "\n",
    "#create prediction data folder if not exists\n",
    "if PREDICTION_DATA_FOLDER not in os.listdir():\n",
    "    os.mkdir(PREDICTION_DATA_FOLDER)\n",
    "    \n",
    "#features to extract from GEE in the training process\n",
    "FEATURES = {\n",
    "            ('collection','JAXA/ALOS/PALSAR/YEARLY/SAR'): ['HH', 'HV', 'qa'], \n",
    "            ('collection', 'LANDSAT/LC08/C01/T1_8DAY_NDVI'): ['NDVI'], \n",
    "            ('collection', 'LANDSAT/LC08/C01/T1_8DAY_NDWI'): ['NDWI'], \n",
    "            ('image','CGIAR/SRTM90_V4'): ['elevation']\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the shapefile storing training polygons\n",
    "TRAINING_POLYGONS_FILE = 'C:/Users/ritvik/Desktop/JPLProject/mapping-colombia-wetlands/training_polygons/training_polygons.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the shapefile storing prediction polygons\n",
    "PREDICTION_POLYGONS_FILE = 'C:/Users/ritvik/Desktop/JPLProject/mapping-colombia-wetlands/prediction_polygons/prediction_polygons.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the list of bands to use for training. Choose from:\n",
    "#Landsat: ['NDVI', 'NDWI']\n",
    "#ALOS-2: ['HH', 'HV']\n",
    "#CGIAR: ['elevation']\n",
    "\n",
    "SELECTED_BANDS = ['NDVI', 'NDWI', 'HH', 'HV', 'elevation']\n",
    "NUM_FEATURES = len(SELECTED_BANDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#any pixels above this elevation (in meters) will be disregarded from training \n",
    "MAX_CONSIDERED_ELEVATION = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the folder id in Google Drive where to temporarily store the GEE data before locally downloading\n",
    "GOOGLE_EARTH_ENGINE_GDRIVE_FOLDER_ID = '1KvlrUHs_rN7xPlw53qtd9pweeLwmrJSP'\n",
    "\n",
    "#the name of that same Google Drive folder\n",
    "GDRIVE_FOLDER_NAME = 'GoogleEarthEngine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data date range\n",
    "DATE_RANGE = ['2017-01-01', '2020-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to use for prediction. Choices are 'histogram' or 'random_forest'\n",
    "METHOD = 'histogram'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Manipulate data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_google_drive(file_id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "    \n",
    "    max_tries = 10\n",
    "    curr_try = 0\n",
    "    status_code = -1\n",
    "    \n",
    "    while status_code != 200 and curr_try < max_tries:\n",
    "        if curr_try > 0:\n",
    "            sleep(30)\n",
    "        session = requests.Session()\n",
    "        response = session.get(URL, params = { 'id' : file_id }, stream = True)\n",
    "        status_code = response.status_code\n",
    "        curr_try += 1\n",
    "        \n",
    "    if status_code != 200:\n",
    "        return\n",
    "    \n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : file_id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_ids_from_google_drive():\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    result = service.files().list(q=\"parents in '%s'\"%GOOGLE_EARTH_ENGINE_GDRIVE_FOLDER_ID).execute()\n",
    "\n",
    "    file_name_to_file_id = {info['name'].split('-')[0]: info['id'] for info in result['files'] if len(info['name'].split('-')) == 2}\n",
    "    \n",
    "    return file_name_to_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file_from_google_drive_by_file_id(fid):\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    \n",
    "    service.files().delete(fileId=fid).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Download Data From Google Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(polygon_features, features, gdrive_folder, date_range, selected_bands):\n",
    "    \"\"\"\n",
    "    This function accepts the below parameters and querys Google Earth Engine for data. The data is stored in \n",
    "    Google Drive.\n",
    "    \n",
    "    Inputs:\n",
    "        polygon_features: a list of pairs like (index, polygon feature) indicating which polygons of data to download\n",
    "        features: the features to extract from GEE\n",
    "        gdrive_folder: the name of the folder in Google Drive where the downloaded data will live\n",
    "        date_range: the start and end date to gather data\n",
    "        selected_bands: list of bands to subset\n",
    "        \n",
    "    Output:\n",
    "        list of tasks which are ready to be started\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store all started tasks\n",
    "    tasks = {}\n",
    "    \n",
    "    #work through each sub-region \n",
    "    for curr_idx, polygon_feature in polygon_features:\n",
    "        \n",
    "        skip_polygon = False\n",
    "        \n",
    "        filtered_imgs = []\n",
    "\n",
    "        #store the reference coordinates\n",
    "        x1 = polygon_feature.GetGeometryRef().GetEnvelope()[0]\n",
    "        y1 = polygon_feature.GetGeometryRef().GetEnvelope()[2]\n",
    "        ref_coords = (x1,y1)\n",
    "        \n",
    "        #get polygon area coordinates\n",
    "        area_coords = [list(pair) for pair in polygon_feature.GetGeometryRef().GetBoundary().GetPoints()]\n",
    "\n",
    "        #create an area of interest from Earth Engine Geometry\n",
    "        area_of_interest = ee.Geometry.Polygon(coords=area_coords)\n",
    "\n",
    "        #iterate over each data source\n",
    "        for data_type_source, bands in features.items():\n",
    "            data_type = data_type_source[0]\n",
    "            data_source = data_type_source[1]\n",
    "                \n",
    "            print('Working on data source: %s...'%data_source)\n",
    "            \n",
    "            if data_type == 'collection':\n",
    "                #access the Earth Engine image collection with the specified bands\n",
    "                data = ee.ImageCollection(data_source).select(bands)\n",
    "\n",
    "                #filter on date range and area of interest\n",
    "                data_filtered = data.filterBounds(area_of_interest).filterDate(date_range[0], date_range[1])\n",
    "                \n",
    "                #limit on cloud cover if LANDSAT\n",
    "                if data_source == 'LANDSAT/LC08/C01/T1_SR':\n",
    "                    data_filtered = data_filtered.filterMetadata('CLOUD_COVER', 'less_than', 20)\n",
    "                    \n",
    "                #ensure there is at least 1 image\n",
    "                num_items = data_filtered.size().getInfo()\n",
    "                if num_items == 0:\n",
    "                    skip_polygon = True\n",
    "                    break\n",
    "\n",
    "                #if LANDSAT NDVI band, get quality mosaic by that band\n",
    "                if 'LANSAT' in data_source:\n",
    "                    mosaic = data_filtered.qualityMosaic(data_source.split('_')[-1])\n",
    "                #otherwise just do a simple median\n",
    "                else:\n",
    "                    mosaic = data_filtered.median()\n",
    "                   \n",
    "            elif data_type == 'image':\n",
    "                mosaic = ee.Image(data_source).select(bands)\n",
    "\n",
    "            #add this mosaic to the list\n",
    "            filtered_imgs.append(mosaic)\n",
    "        \n",
    "        if skip_polygon:\n",
    "            print('Skipping %s'%fname)\n",
    "            tasks[fname] = None\n",
    "            print('==================================')\n",
    "            continue\n",
    "            \n",
    "        #generate file name\n",
    "        features_str = '_'.join([item[1] for item in features.keys()]).replace('/','_')\n",
    "        fname = '%s-%s'%(curr_idx, features_str)\n",
    "        print(fname)\n",
    "        \n",
    "        #add the various layers on top of each other to create a data cube with all features\n",
    "        final_img = ee.Image()\n",
    "        \n",
    "        for img in filtered_imgs:\n",
    "            final_img = ee.Image.addBands(final_img,img)\n",
    "        \n",
    "        #use the ALOS qa band to filter out invalid pixels\n",
    "        if 'qa' in features[('collection','JAXA/ALOS/PALSAR/YEARLY/SAR')]:\n",
    "            qa_band = final_img.select('qa')\n",
    "            qa_mask = qa_band.eq(0)\n",
    "            final_img = final_img.where(qa_mask, NO_DATA_VALUE)\n",
    "            \n",
    "        #use the SRTM elevation band to filter out invaild pixels\n",
    "        if 'elevation' in features['image','CGIAR/SRTM90_V4']:\n",
    "            elevation_band = final_img.select('elevation')\n",
    "            elevation_mask = elevation_band.gt(MAX_CONSIDERED_ELEVATION)\n",
    "            final_img = final_img.where(elevation_mask, NO_DATA_VALUE)\n",
    "            \n",
    "        #if any of the selected bands has NO_DATA_VALUE, mark that whole pixel as NO_DATA_VALUE\n",
    "        for b in selected_bands:\n",
    "            b_values = final_img.select(b)\n",
    "            b_mask = b_values.eq(NO_DATA_VALUE)\n",
    "            final_img = final_img.where(b_mask, NO_DATA_VALUE)\n",
    "         \n",
    "        #store the result with just the needed bands\n",
    "        selected_bands = sorted(selected_bands)\n",
    "        result = final_img.select(*selected_bands).float()\n",
    "          \n",
    "        #define the task to gather the data\n",
    "        task = ee.batch.Export.image.toDrive(image=result,\n",
    "                                             region=area_of_interest.getInfo()['coordinates'],\n",
    "                                             description=str(curr_idx),\n",
    "                                             folder=gdrive_folder,\n",
    "                                             fileNamePrefix=fname,\n",
    "                                             crs_transform=[RESOLUTION, 0.0, ref_coords[0], 0.0, -RESOLUTION, ref_coords[1]],\n",
    "                                             crs='EPSG:4326')\n",
    "        \n",
    "        #store the task\n",
    "        tasks[fname] = task\n",
    "        \n",
    "        print('==================================')\n",
    "        \n",
    "    return list(tasks.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tasks_in_batches(tasks, batch_size, FOLDER):\n",
    "    \"\"\"\n",
    "    Executes a list of tasks in batches\n",
    "    \n",
    "    Inputs:\n",
    "        tasks: list of tasks\n",
    "        batch_size: number of tasks to execute per batch\n",
    "        FOLDER: the folder to store the downloaded rasters\n",
    "    \"\"\"\n",
    "    \n",
    "    #create mapping of polygon to fname\n",
    "    polygon_to_fname = {}\n",
    "\n",
    "    #process the tasks in small batches to avoid memory running out\n",
    "    for batch_idx in range(ceil(len(tasks) / batch_size)):\n",
    "\n",
    "        #get the current batch of tasks\n",
    "        curr_tasks = tasks[batch_size*batch_idx:batch_size*(batch_idx+1)]\n",
    "        print('Processing Batch %s'%(batch_idx+1))\n",
    "\n",
    "        #start all tasks in that batch\n",
    "        for name,task in curr_tasks:\n",
    "            if task != None:\n",
    "                task.start()\n",
    "\n",
    "        print('Started all tasks in batch')\n",
    "\n",
    "        #wait until all tasks in that batch are done\n",
    "        curr_states = [task.status()['state'] for name,task in curr_tasks if task != None]\n",
    "        while 'RUNNING' in set(curr_states) or 'READY' in set(curr_states):\n",
    "            print('Current states: %s'%curr_states)\n",
    "            sleep(30)\n",
    "            curr_states = [task.status()['state'] for name,task in curr_tasks if task != None]\n",
    "\n",
    "        #once all tasks done, get their file ids on google drive\n",
    "        file_name_to_file_id = get_file_ids_from_google_drive()\n",
    "\n",
    "        #for each file...\n",
    "        for fname, fid in file_name_to_file_id.items():\n",
    "\n",
    "            #get feature file name\n",
    "            features_file_name = '%s/features_%s.tiff'%(FOLDER, fname)\n",
    "\n",
    "            #check if data already downloaded\n",
    "            print('Downloading %s from Drive'%fname)\n",
    "            download_file_from_google_drive(fid, features_file_name)\n",
    "\n",
    "            print('Deleting %s from Drive'%fname)\n",
    "            delete_file_from_google_drive_by_file_id(fid)\n",
    "\n",
    "        print('================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Despeckling and Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_db(img):\n",
    "    return 10 * np.log10(img)\n",
    "\n",
    "def db_to_img(img):\n",
    "    return 10**(img / 10)\n",
    "\n",
    "def tv_denoise(arr, idxs_to_despeckle, weight):\n",
    "    copy_arr = arr.copy()\n",
    "    for idx in idxs_to_despeckle:\n",
    "        #get the layer\n",
    "        layer = copy_arr[:,:,idx]\n",
    "        \n",
    "        orig_valid_mask = ~np.isnan(layer)\n",
    "        \n",
    "        #denoise\n",
    "        img_db = img_to_db(layer)\n",
    "        img_db_tv = denoise_tv_bregman(img_db, weight)\n",
    "        img_tv = db_to_img(img_db_tv)\n",
    "        img_tv[orig_valid_mask & np.isnan(img_tv)] = layer[orig_valid_mask & np.isnan(img_tv)]\n",
    "        \n",
    "        #set denoised into copy of array\n",
    "        copy_arr[:,:,idx] = img_tv\n",
    "        \n",
    "    return copy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sep_metric(feat_file_name, num_clusters_options, band_names_options):\n",
    "    \"\"\"\n",
    "    Get the separability metric for the given array and given possible clusters\n",
    "    \n",
    "    Inputs:\n",
    "        feat_file_name: path to file to analyze\n",
    "        num_clusters_options: list of number of clusters to try\n",
    "        band_names_options: name of the bands to try for clustering\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #read the array\n",
    "    ds = gdal.Open(feat_file_name, gdal.GA_ReadOnly)\n",
    "\n",
    "    #get the band names\n",
    "    band_names = [ds.GetRasterBand(idx+1).GetDescription() for idx in range(ds.RasterCount)]\n",
    "\n",
    "    #get gt and read array\n",
    "    gt = ds.GetGeoTransform()\n",
    "    arr = ds.ReadAsArray()\n",
    "    arr = np.stack([arr[i] for i in range(arr.shape[0])], axis=-1)\n",
    "    ds = None\n",
    "\n",
    "    #transform array to 2d\n",
    "    arr_2d = arr.reshape(-1,arr.shape[-1])\n",
    "    arr_2d[arr_2d == NO_DATA_VALUE] = np.nan\n",
    "    valid_indices = np.where(np.all(~np.isnan(arr_2d), axis=-1))[0]\n",
    "    arr_2d_valid = arr_2d[valid_indices]\n",
    "    \n",
    "    sep_metric_dict = {}\n",
    "    \n",
    "    for k in num_clusters_options:\n",
    "        for b in band_names_options:\n",
    "            \n",
    "            band_idx = band_names.index(b)\n",
    "        \n",
    "            #define model\n",
    "            model = KMeans(n_clusters=k)\n",
    "\n",
    "            #fit model and predict clusters\n",
    "            cluster_preds = model.fit_predict(arr_2d_valid[:,[band_idx]])\n",
    "\n",
    "            mu_vals = np.zeros(k)\n",
    "            dev_vals = np.zeros(k)\n",
    "\n",
    "            for cid in range(k):\n",
    "                cluster_data = arr_2d_valid[cluster_preds == cid]\n",
    "                mu, dev = cluster_data.mean(), cluster_data.std()\n",
    "                mu_vals[cid] = mu\n",
    "                dev_vals[cid] = dev\n",
    "\n",
    "            sep_metric_vals = []\n",
    "            for c1 in range(k):\n",
    "                for c2 in range(c1+1,k):\n",
    "                    sep_metric_vals.append(abs(mu_vals[c1] - mu_vals[c2]) / (dev_vals[c1] + dev_vals[c2]))\n",
    "                    \n",
    "            sep_metric_dict[(b,k)] = {'sep_metric_vals': sep_metric_vals, 'cluster_preds': cluster_preds}\n",
    "    \n",
    "    best_sep_entry = sorted(sep_metric_dict.items(), key=lambda info: -np.min(info[1]['sep_metric_vals']))[0]\n",
    "    print('Best Separating Params: %s'%str(best_sep_entry[0]))\n",
    "    \n",
    "    result = np.ones(arr_2d.shape[0])*-1\n",
    "    result[valid_indices] = best_sep_entry[1]['cluster_preds']\n",
    "    result = result.reshape(arr.shape[:2])\n",
    "\n",
    "    ds = np_array_to_raster('%s_suggested.tiff'%feat_file_name.split('.')[-2], result, gt, no_data=-1, nband=1, gdal_data_type=gdal.GDT_Float64)\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Process Feature Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_files(feat_file_names, confidence_levels=None, preprocess=True):\n",
    "    \"\"\"\n",
    "    This function accepts a list of file names and processes those rasters. \n",
    "    \n",
    "    Inputs:\n",
    "        feat_file_names: a list of names of the downloaded training data files\n",
    "        confidence_levels: a list of confidence levels associated with each file in feat_file_names\n",
    "        preprocess: whether to remove some pixels that likely do not belong\n",
    "        \n",
    "    Outputs:\n",
    "        the processed training data and auxilary data such as geotransforms\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store the numpy array of training data for each file\n",
    "    data = {}\n",
    "    \n",
    "    #this will store auxilary data for each file\n",
    "    feat_file_data = {}\n",
    "    \n",
    "    #iterate over each file\n",
    "    for feat_file_name in feat_file_names:\n",
    "\n",
    "        #open file and get geotransform\n",
    "        ds = gdal.Open(feat_file_name, gdal.GA_ReadOnly)\n",
    "        try:\n",
    "            gt = ds.GetGeoTransform()\n",
    "        except AttributeError:\n",
    "            print('Could not process %s'%feat_file_name)\n",
    "            continue\n",
    "\n",
    "        #despeckle any bands which need to be despeckled\n",
    "        idx_to_despeckle = [idx for idx in range(ds.RasterCount) if ds.GetRasterBand(idx+1).GetDescription() in BANDS_TO_DESPECKLE]\n",
    "        arr = ds.ReadAsArray()\n",
    "        arr = np.stack([arr[i] for i in range(arr.shape[0])], axis=-1)\n",
    "        data_mask = (arr == NO_DATA_VALUE) | np.isnan(arr)\n",
    "        arr[data_mask] = NO_DATA_VALUE\n",
    "\n",
    "        arr = tv_denoise(arr, idx_to_despeckle, 1)\n",
    "        arr[data_mask] = np.nan\n",
    "\n",
    "        ds = None\n",
    "        \n",
    "        arr_2d = arr.reshape(-1,arr.shape[-1])\n",
    "        valid_indices = np.where(np.all(~np.isnan(arr_2d), axis=-1))[0]\n",
    "    \n",
    "        if preprocess:\n",
    "            #define model\n",
    "            model = KMeans(n_clusters=2)\n",
    "\n",
    "            #fit model and predict clusters\n",
    "            try:\n",
    "                cluster_preds = model.fit_predict(arr_2d[valid_indices])\n",
    "            except ValueError:\n",
    "                print('Could not process %s'%feat_file_name)\n",
    "                continue\n",
    "\n",
    "            #get main cluster indices\n",
    "            main_cluster = np.median(cluster_preds)\n",
    "            main_cluster_indices = np.where(cluster_preds == main_cluster)[0]\n",
    "\n",
    "            #refine chosen indices\n",
    "            valid_indices = valid_indices[main_cluster_indices]\n",
    "\n",
    "        #add this training data to the list\n",
    "        data[feat_file_name] = arr_2d[valid_indices]\n",
    "        \n",
    "        #add auxilary information\n",
    "        feat_file_data[feat_file_name] = {'chosen_indices': valid_indices, 'shape': arr.shape, 'gt': gt}\n",
    "        \n",
    "    #if confidence levels supplied, then sample according to those levels\n",
    "    if confidence_levels != None:\n",
    "        num_pixels_to_sample = np.median([item.shape[0] for item in data.values()])\n",
    "        data = {fname: ds[np.random.choice(ds.shape[0], int(num_pixels_to_sample*confidence_levels[fname]))] for fname,ds in data.items()}\n",
    "\n",
    "    return data, feat_file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_histograms(training_datasets):\n",
    "    \"\"\"\n",
    "    Creates a set of histograms, one for each class\n",
    "    \n",
    "    Inputs:\n",
    "        training_histograms: a dictionary mapping class_id to a dataset\n",
    "        \n",
    "    Outputs:\n",
    "        a dictionary mapping class_id to a histogram\n",
    "        a list of histogram bin cuttoffs for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    min_feature_values = [min([min(dataset[:,idx]) for dataset in training_datasets.values()]) for idx in range(NUM_FEATURES)]\n",
    "    max_feature_values = [max([max(dataset[:,idx]) for dataset in training_datasets.values()]) for idx in range(NUM_FEATURES)]\n",
    "    \n",
    "    histogram_ranges = []\n",
    "    training_histograms = {}\n",
    "    num_bins = 5\n",
    "\n",
    "    for idx in range(NUM_FEATURES):\n",
    "        width = (max_feature_values[idx] - min_feature_values[idx]) / num_bins\n",
    "        histogram_ranges.append(np.arange(min_feature_values[idx], max_feature_values[idx]+width*.99, width))\n",
    "    histogram_ranges = np.array(histogram_ranges)\n",
    "\n",
    "    for class_id in class_ids:\n",
    "        training_histograms[class_id] = np.histogramdd(training_datasets[class_id], bins=histogram_ranges, density=True)[0] \n",
    "        \n",
    "    return training_histograms, histogram_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(test_set, training_histograms, histogram_ranges, class_ids, frac=0.25):\n",
    "    \"\"\"\n",
    "    This function accepts a data set and histograms and classifies each pixel and assigns a score\n",
    "    \n",
    "    Inputs:\n",
    "        test_set: the test set of features which we would like to classify\n",
    "        training_histograms: a dictionary of histograms, one for each class\n",
    "        histogram_ranges: the bin cuttoffs for the histograms\n",
    "        class_ids: a set of class ids\n",
    "        frac: between 0 and 1. Higher values mean we require more confidence to classify a pixel as non-NaN\n",
    "        \n",
    "    Output:\n",
    "        an array of predicted classes and corresponding scores\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store the probabilities for each class\n",
    "    class_id_to_probs = {}\n",
    "    \n",
    "    #any probability density below this is considered as 0\n",
    "    min_allowable_density = min([frac*np.max(training_histograms[cid]) for cid in class_ids])\n",
    "    \n",
    "    #iterate over each histogram\n",
    "    for class_id, histogram in training_histograms.items():\n",
    "        \n",
    "        #get the position of each test pixel in the context of this histogram\n",
    "        transposed_ranges = np.transpose(histogram_ranges)\n",
    "        expanded_dims_ranges = np.expand_dims(transposed_ranges, axis=1)\n",
    "        extended_ranges = np.concatenate([expanded_dims_ranges for _ in range(test_set.shape[0])], axis=1)\n",
    "        diffs = extended_ranges - test_set\n",
    "    \n",
    "        #get the probability density at each position\n",
    "        indices = np.argmax(diffs > 0, axis=0) - 1\n",
    "        indices[indices < 0] = 0\n",
    "        indices = indices - np.all((diffs>0)==False, axis=0)\n",
    "        indices = tuple(np.transpose(indices))\n",
    "        probs = histogram[indices]\n",
    "        \n",
    "        #store this in the dictionary\n",
    "        class_id_to_probs[class_id] = probs\n",
    "      \n",
    "    #create matrix of probabilities for each class\n",
    "    prob_mtx = np.stack([class_id_to_probs[cid] for cid in class_ids], axis=-1)\n",
    "    \n",
    "    #sort matrix of probs\n",
    "    sorted_probs = np.sort(prob_mtx, axis=1)\n",
    "    \n",
    "    #any pixel where all classes have 0 probability is NaN\n",
    "    nan_indices = np.where(sorted_probs[:,-1] < min_allowable_density)[0]\n",
    "    \n",
    "    #compute scores based on ratio of most likely class to second most likely class\n",
    "    scores = 1/(1+np.exp(-(sorted_probs[:,-1] / sorted_probs[:,-2])))\n",
    "    \n",
    "    #apply NaN pixels\n",
    "    scores[nan_indices] = np.nan\n",
    "\n",
    "    #get the predicted class\n",
    "    pred_class = np.argmax(prob_mtx, axis=1).astype(float)\n",
    "    \n",
    "    #apply NaN pixels\n",
    "    pred_class[nan_indices] = np.nan\n",
    "    \n",
    "    return pred_class, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Array to TIFF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raster(output_path, columns, rows, nband=1, gdal_data_type=gdal.GDT_Int32, driver=r'GTiff'):\n",
    "    ''' \n",
    "    returns gdal data source raster object \n",
    "    '''\n",
    "    \n",
    "    # create driver\n",
    "    driver = gdal.GetDriverByName(driver)\n",
    "\n",
    "    output_raster = driver.Create(output_path, columns, rows, nband, eType = gdal_data_type)    \n",
    "    \n",
    "    return output_raster\n",
    "\n",
    "def np_array_to_raster(output_path, arr, geotransform, no_data=None, nband=1, gdal_data_type=gdal.GDT_Int32, spatial_reference_system_wkid=4326, driver=r'GTiff'):\n",
    "    ''' \n",
    "    returns a gdal raster data source\n",
    "\n",
    "    keyword arguments:\n",
    "\n",
    "    output_path -- full path to the raster to be written to disk\n",
    "    numpy_array -- numpy array containing data to write to raster\n",
    "    upper_left_tuple -- the upper left point of the numpy array (should be a tuple structured as (x, y))\n",
    "    cell_resolution -- the cell resolution of the output raster\n",
    "    no_data -- value in numpy array that should be treated as no data\n",
    "    nband -- the band to write to in the output raster\n",
    "    gdal_data_type -- gdal data type of raster (see gdal documentation for list of values)\n",
    "    spatial_reference_system_wkid -- well known id (wkid) of the spatial reference of the data\n",
    "    driver -- string value of the gdal driver to use\n",
    "    '''\n",
    "\n",
    "    rows, columns = arr.shape[0], arr.shape[1]\n",
    "\n",
    "    # create output raster\n",
    "    output_raster = create_raster(output_path, columns, rows, nband, gdal_data_type) \n",
    "\n",
    "    spatial_reference = osr.SpatialReference()\n",
    "    spatial_reference.ImportFromEPSG(spatial_reference_system_wkid)\n",
    "    output_raster.SetProjection(spatial_reference.ExportToWkt())\n",
    "    output_raster.SetGeoTransform(geotransform)\n",
    "    \n",
    "    for band_idx in range(1,nband+1):\n",
    "        output_band = output_raster.GetRasterBand(band_idx)\n",
    "        if no_data != None:\n",
    "            output_band.SetNoDataValue(no_data)\n",
    "        if nband > 1:\n",
    "            output_band.WriteArray(arr[:,:,band_idx-1])\n",
    "        else:\n",
    "            output_band.WriteArray(arr)\n",
    "        output_band.FlushCache() \n",
    "        output_band.ComputeStatistics(False)\n",
    "\n",
    "    if os.path.exists(output_path) == False:\n",
    "        raise Exception('Failed to create raster: %s' % output_path)\n",
    "\n",
    "    return output_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Download Prediction Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "dataSource = driver.Open(PREDICTION_POLYGONS_FILE, gdal.GA_ReadOnly)\n",
    "layer = dataSource.GetLayer()\n",
    "\n",
    "prediction_polygon_features = [layer.GetNextFeature() for _ in range(layer.GetFeatureCount())]\n",
    "prediction_polygon_features_to_process = [(idx,f) for idx,f in enumerate(prediction_polygon_features) if 'features_%s.tiff'%(idx) not in os.listdir(PREDICTION_DATA_FOLDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "0-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "1-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "2-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "3-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "4-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "5-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "6-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "7-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "8-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "tasks = get_training_data(prediction_polygon_features_to_process, FEATURES, GDRIVE_FOLDER_NAME, DATE_RANGE, SELECTED_BANDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch 1\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY']\n",
      "Current states: ['RUNNING', 'RUNNING', 'READY']\n",
      "Current states: ['RUNNING', 'RUNNING', 'READY']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['COMPLETED', 'RUNNING', 'RUNNING']\n",
      "Current states: ['COMPLETED', 'RUNNING', 'RUNNING']\n",
      "Current states: ['COMPLETED', 'RUNNING', 'RUNNING']\n",
      "Current states: ['COMPLETED', 'RUNNING', 'RUNNING']\n",
      "Current states: ['COMPLETED', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['COMPLETED', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['COMPLETED', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['COMPLETED', 'RUNNING', 'COMPLETED']\n",
      "Downloading 1 from Drive\n",
      "Deleting 1 from Drive\n",
      "Downloading 2 from Drive\n",
      "Deleting 2 from Drive\n",
      "Downloading 0 from Drive\n",
      "Deleting 0 from Drive\n",
      "================================\n",
      "Processing Batch 2\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY']\n",
      "Current states: ['RUNNING', 'RUNNING', 'READY']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'COMPLETED', 'RUNNING']\n",
      "Current states: ['RUNNING', 'COMPLETED', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'COMPLETED', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'COMPLETED', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'COMPLETED', 'COMPLETED']\n",
      "Downloading 3 from Drive\n",
      "Deleting 3 from Drive\n",
      "Downloading 5 from Drive\n",
      "Deleting 5 from Drive\n",
      "Downloading 4 from Drive\n",
      "Deleting 4 from Drive\n",
      "================================\n",
      "Processing Batch 3\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY']\n",
      "Current states: ['RUNNING', 'RUNNING', 'READY']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['RUNNING', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['RUNNING', 'COMPLETED', 'COMPLETED']\n",
      "Downloading 6 from Drive\n",
      "Deleting 6 from Drive\n",
      "Downloading 7 from Drive\n",
      "Deleting 7 from Drive\n",
      "Downloading 8 from Drive\n",
      "Deleting 8 from Drive\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "execute_tasks_in_batches(tasks, 3, PREDICTION_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritvik\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Prediction Regions\n"
     ]
    }
   ],
   "source": [
    "prediction_feat_file_names = ['%s/%s'%(PREDICTION_DATA_FOLDER, fname) for fname in os.listdir(PREDICTION_DATA_FOLDER) if 'suggested' not in fname and 'predicted' not in fname]\n",
    "prediction_data, prediction_feat_file_data = process_feature_files(prediction_feat_file_names, confidence_levels=None, preprocess=False)\n",
    "print('Processed Prediction Regions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Get Suggested Number of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prediction_data/features_0.tiff\n",
      "Best Separating Params: ('NDWI', 3)\n",
      "----------------------\n",
      "Processing prediction_data/features_1.tiff\n",
      "Best Separating Params: ('elevation', 3)\n",
      "----------------------\n",
      "Processing prediction_data/features_2.tiff\n",
      "Best Separating Params: ('elevation', 3)\n",
      "----------------------\n",
      "Processing prediction_data/features_3.tiff\n",
      "Best Separating Params: ('elevation', 3)\n",
      "----------------------\n",
      "Processing prediction_data/features_4.tiff\n",
      "Best Separating Params: ('elevation', 4)\n",
      "----------------------\n",
      "Processing prediction_data/features_5.tiff\n",
      "Best Separating Params: ('NDVI', 3)\n",
      "----------------------\n",
      "Processing prediction_data/features_6.tiff\n",
      "Best Separating Params: ('NDVI', 3)\n",
      "----------------------\n",
      "Processing prediction_data/features_7.tiff\n",
      "Best Separating Params: ('elevation', 3)\n",
      "----------------------\n",
      "Processing prediction_data/features_8.tiff\n",
      "Best Separating Params: ('NDWI', 4)\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "num_clusters_options = [3,4]\n",
    "\n",
    "sep_bands = [b for b in SELECTED_BANDS if b not in BANDS_TO_DESPECKLE]\n",
    "\n",
    "for feat_file_name in prediction_feat_file_names:\n",
    "    print('Processing %s'%feat_file_name)\n",
    "    get_sep_metric(feat_file_name, num_clusters_options, sep_bands)\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Download Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "\n",
    "dataSource = driver.Open(TRAINING_POLYGONS_FILE, gdal.GA_ReadOnly)\n",
    "\n",
    "layer = dataSource.GetLayer()\n",
    "\n",
    "training_polygon_features = [layer.GetNextFeature() for _ in range(layer.GetFeatureCount())]\n",
    "training_polygon_features_to_process = [(idx,f) for idx,f in enumerate(training_polygon_features) if 'features_%s.tiff'%idx not in os.listdir(TRAINING_DATA_FOLDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "0-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "1-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "2-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "3-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "4-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "5-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "6-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "7-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "8-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "9-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "10-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "11-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "12-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "13-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "14-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "15-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "16-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "17-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "18-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "19-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "20-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "21-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "22-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "23-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "24-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "25-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "26-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "27-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "28-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "29-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "30-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "31-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "32-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "33-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "34-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "35-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "36-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "37-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "38-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "39-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "40-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "41-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "42-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "43-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "44-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "45-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "46-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "47-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "48-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "49-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "50-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "51-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "52-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "53-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "54-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "55-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "56-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "57-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "58-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "59-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "60-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "61-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "62-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "63-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "64-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "65-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "66-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "67-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "68-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "69-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "70-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "71-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "72-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "73-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "74-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "75-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "76-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "77-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "78-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "79-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "80-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "81-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "82-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "83-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "84-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDVI...\n",
      "Working on data source: LANDSAT/LC08/C01/T1_8DAY_NDWI...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "85-JAXA_ALOS_PALSAR_YEARLY_SAR_LANDSAT_LC08_C01_T1_8DAY_NDVI_LANDSAT_LC08_C01_T1_8DAY_NDWI_CGIAR_SRTM90_V4\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "tasks = get_training_data(training_polygon_features_to_process, FEATURES, GDRIVE_FOLDER_NAME, DATE_RANGE, SELECTED_BANDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch 1\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING']\n",
      "Downloading 4 from Drive\n",
      "Deleting 4 from Drive\n",
      "Downloading 3 from Drive\n",
      "Deleting 3 from Drive\n",
      "Downloading 2 from Drive\n",
      "Deleting 2 from Drive\n",
      "Downloading 1 from Drive\n",
      "Deleting 1 from Drive\n",
      "Downloading 0 from Drive\n",
      "Deleting 0 from Drive\n",
      "================================\n",
      "Processing Batch 2\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'RUNNING', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'COMPLETED']\n",
      "Downloading 8 from Drive\n",
      "Deleting 8 from Drive\n",
      "Downloading 9 from Drive\n",
      "Deleting 9 from Drive\n",
      "Downloading 7 from Drive\n",
      "Deleting 7 from Drive\n",
      "Downloading 5 from Drive\n",
      "Deleting 5 from Drive\n",
      "Downloading 6 from Drive\n",
      "Deleting 6 from Drive\n",
      "================================\n",
      "Processing Batch 3\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['RUNNING', 'COMPLETED', 'RUNNING', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING']\n",
      "Downloading 14 from Drive\n",
      "Deleting 14 from Drive\n",
      "Downloading 13 from Drive\n",
      "Deleting 13 from Drive\n",
      "Downloading 10 from Drive\n",
      "Deleting 10 from Drive\n",
      "Downloading 12 from Drive\n",
      "Deleting 12 from Drive\n",
      "Downloading 11 from Drive\n",
      "Deleting 11 from Drive\n",
      "================================\n",
      "Processing Batch 4\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'READY']\n",
      "Downloading 19 from Drive\n",
      "Deleting 19 from Drive\n",
      "Downloading 18 from Drive\n",
      "Deleting 18 from Drive\n",
      "Downloading 17 from Drive\n",
      "Deleting 17 from Drive\n",
      "Downloading 16 from Drive\n",
      "Deleting 16 from Drive\n",
      "Downloading 15 from Drive\n",
      "Deleting 15 from Drive\n",
      "================================\n",
      "Processing Batch 5\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'READY']\n",
      "Downloading 24 from Drive\n",
      "Deleting 24 from Drive\n",
      "Downloading 22 from Drive\n",
      "Deleting 22 from Drive\n",
      "Downloading 23 from Drive\n",
      "Deleting 23 from Drive\n",
      "Downloading 21 from Drive\n",
      "Deleting 21 from Drive\n",
      "Downloading 20 from Drive\n",
      "Deleting 20 from Drive\n",
      "================================\n",
      "Processing Batch 6\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'READY']\n",
      "Downloading 29 from Drive\n",
      "Deleting 29 from Drive\n",
      "Downloading 28 from Drive\n",
      "Deleting 28 from Drive\n",
      "Downloading 27 from Drive\n",
      "Deleting 27 from Drive\n",
      "Downloading 26 from Drive\n",
      "Deleting 26 from Drive\n",
      "Downloading 25 from Drive\n",
      "Deleting 25 from Drive\n",
      "================================\n",
      "Processing Batch 7\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'RUNNING', 'READY', 'READY']\n",
      "Downloading 32 from Drive\n",
      "Deleting 32 from Drive\n",
      "Downloading 34 from Drive\n",
      "Deleting 34 from Drive\n",
      "Downloading 33 from Drive\n",
      "Deleting 33 from Drive\n",
      "Downloading 31 from Drive\n",
      "Deleting 31 from Drive\n",
      "Downloading 30 from Drive\n",
      "Deleting 30 from Drive\n",
      "================================\n",
      "Processing Batch 8\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING']\n",
      "Downloading 39 from Drive\n",
      "Deleting 39 from Drive\n",
      "Downloading 38 from Drive\n",
      "Deleting 38 from Drive\n",
      "Downloading 37 from Drive\n",
      "Deleting 37 from Drive\n",
      "Downloading 35 from Drive\n",
      "Deleting 35 from Drive\n",
      "Downloading 36 from Drive\n",
      "Deleting 36 from Drive\n",
      "================================\n",
      "Processing Batch 9\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING']\n",
      "Downloading 44 from Drive\n",
      "Deleting 44 from Drive\n",
      "Downloading 43 from Drive\n",
      "Deleting 43 from Drive\n",
      "Downloading 42 from Drive\n",
      "Deleting 42 from Drive\n",
      "Downloading 41 from Drive\n",
      "Deleting 41 from Drive\n",
      "Downloading 40 from Drive\n",
      "Deleting 40 from Drive\n",
      "================================\n",
      "Processing Batch 10\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'READY']\n",
      "Downloading 49 from Drive\n",
      "Deleting 49 from Drive\n",
      "Downloading 48 from Drive\n",
      "Deleting 48 from Drive\n",
      "Downloading 47 from Drive\n",
      "Deleting 47 from Drive\n",
      "Downloading 45 from Drive\n",
      "Deleting 45 from Drive\n",
      "Downloading 46 from Drive\n",
      "Deleting 46 from Drive\n",
      "================================\n",
      "Processing Batch 11\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'RUNNING', 'RUNNING', 'RUNNING', 'READY']\n",
      "Downloading 54 from Drive\n",
      "Deleting 54 from Drive\n",
      "Downloading 51 from Drive\n",
      "Deleting 51 from Drive\n",
      "Downloading 53 from Drive\n",
      "Deleting 53 from Drive\n",
      "Downloading 52 from Drive\n",
      "Deleting 52 from Drive\n",
      "Downloading 50 from Drive\n",
      "Deleting 50 from Drive\n",
      "================================\n",
      "Processing Batch 12\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'RUNNING', 'RUNNING', 'RUNNING', 'READY']\n",
      "Downloading 59 from Drive\n",
      "Deleting 59 from Drive\n",
      "Downloading 58 from Drive\n",
      "Deleting 58 from Drive\n",
      "Downloading 56 from Drive\n",
      "Deleting 56 from Drive\n",
      "Downloading 57 from Drive\n",
      "Deleting 57 from Drive\n",
      "Downloading 55 from Drive\n",
      "Deleting 55 from Drive\n",
      "================================\n",
      "Processing Batch 13\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'RUNNING', 'RUNNING', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING']\n",
      "Downloading 64 from Drive\n",
      "Deleting 64 from Drive\n",
      "Downloading 63 from Drive\n",
      "Deleting 63 from Drive\n",
      "Downloading 62 from Drive\n",
      "Deleting 62 from Drive\n",
      "Downloading 61 from Drive\n",
      "Deleting 61 from Drive\n",
      "Downloading 60 from Drive\n",
      "Deleting 60 from Drive\n",
      "================================\n",
      "Processing Batch 14\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'READY']\n",
      "Downloading 69 from Drive\n",
      "Deleting 69 from Drive\n",
      "Downloading 68 from Drive\n",
      "Deleting 68 from Drive\n",
      "Downloading 67 from Drive\n",
      "Deleting 67 from Drive\n",
      "Downloading 66 from Drive\n",
      "Deleting 66 from Drive\n",
      "Downloading 65 from Drive\n",
      "Deleting 65 from Drive\n",
      "================================\n",
      "Processing Batch 15\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'RUNNING', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING']\n",
      "Downloading 74 from Drive\n",
      "Deleting 74 from Drive\n",
      "Downloading 73 from Drive\n",
      "Deleting 73 from Drive\n",
      "Downloading 72 from Drive\n",
      "Deleting 72 from Drive\n",
      "Downloading 71 from Drive\n",
      "Deleting 71 from Drive\n",
      "Downloading 70 from Drive\n",
      "Deleting 70 from Drive\n",
      "================================\n",
      "Processing Batch 16\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING']\n",
      "Downloading 79 from Drive\n",
      "Deleting 79 from Drive\n",
      "Downloading 78 from Drive\n",
      "Deleting 78 from Drive\n",
      "Downloading 77 from Drive\n",
      "Deleting 77 from Drive\n",
      "Downloading 75 from Drive\n",
      "Deleting 75 from Drive\n",
      "Downloading 76 from Drive\n",
      "Deleting 76 from Drive\n",
      "================================\n",
      "Processing Batch 17\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'READY', 'READY']\n",
      "Downloading 84 from Drive\n",
      "Deleting 84 from Drive\n",
      "Downloading 83 from Drive\n",
      "Deleting 83 from Drive\n",
      "Downloading 82 from Drive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 82 from Drive\n",
      "Downloading 81 from Drive\n",
      "Deleting 81 from Drive\n",
      "Downloading 80 from Drive\n",
      "Deleting 80 from Drive\n",
      "================================\n",
      "Processing Batch 18\n",
      "Started all tasks in batch\n",
      "Current states: ['READY']\n",
      "Downloading 85 from Drive\n",
      "Deleting 85 from Drive\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "execute_tasks_in_batches(tasks, 5, TRAINING_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not process training_data/features_52.tiff\n",
      "Could not process training_data/features_55.tiff\n"
     ]
    }
   ],
   "source": [
    "training_feat_file_names = ['%s/%s'%(TRAINING_DATA_FOLDER, fname) for fname in os.listdir(TRAINING_DATA_FOLDER) if 'suggested' not in fname and 'predicted' not in fname]\n",
    "confidence_levels = {name: training_polygon_features[int(''.join([i for i in name if i.isdigit()]))].GetField('confidence') for name in training_feat_file_names}\n",
    "\n",
    "#this creates a dictionary mapping training data file name to the corresponding processed data set\n",
    "input_to_dataset, _ = process_feature_files(training_feat_file_names, confidence_levels, preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dictionary maps prediction data file name to class id to input data file name\n",
    "pred_to_class_to_input = {'%s/%s'%(PREDICTION_DATA_FOLDER, pred_fname): {} for pred_fname in os.listdir(PREDICTION_DATA_FOLDER) if 'suggested' not in pred_fname and 'predicted' not in pred_fname}\n",
    "\n",
    "#for each training polygon...\n",
    "for idx_input, input_poly in enumerate(training_polygon_features):\n",
    "    #check if its data downloaded successfully\n",
    "    if 'features_%s.tiff'%idx_input not in os.listdir(TRAINING_DATA_FOLDER):\n",
    "        continue\n",
    "        \n",
    "    #get file name\n",
    "    input_fname = '%s/features_%s.tiff'%(TRAINING_DATA_FOLDER, idx_input)\n",
    "  \n",
    "    #for each prediction polygon\n",
    "    for idx_pred, pred_poly in enumerate(prediction_polygon_features):\n",
    "        #check if its data downloaded successfully\n",
    "        if 'features_%s.tiff'%idx_pred not in os.listdir(PREDICTION_DATA_FOLDER):\n",
    "            continue\n",
    "        \n",
    "        #get file name\n",
    "        pred_fname = '%s/features_%s.tiff'%(PREDICTION_DATA_FOLDER, idx_pred)\n",
    "        \n",
    "        #check if this training polygon inside the prediction polygon\n",
    "        if input_poly.GetGeometryRef().Centroid().Within(pred_poly.GetGeometryRef()):\n",
    "            \n",
    "            #get class id\n",
    "            class_id = input_poly.GetField('class_id')\n",
    "            \n",
    "            #add this training file to files belonging to this prediction file\n",
    "            if class_id in pred_to_class_to_input[pred_fname]:\n",
    "                pred_to_class_to_input[pred_fname][class_id].append(input_fname)\n",
    "            else:\n",
    "                pred_to_class_to_input[pred_fname][class_id] = [input_fname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each prediction file, construct full dataset\n",
    "for pred_fname, mapping_data in pred_to_class_to_input.items():\n",
    "    for class_id in pred_to_class_to_input[pred_fname]:\n",
    "        pred_to_class_to_input[pred_fname][class_id] = \\\n",
    "        np.concatenate([input_to_dataset[fname] for fname in pred_to_class_to_input[pred_fname][class_id] if fname in input_to_dataset], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Classify Prediction Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty histograms and histogram ranges dictionaries\n",
    "histograms_dict = {}\n",
    "histogram_ranges_dict = {}\n",
    "\n",
    "random_forest_clf_dict = {}\n",
    "\n",
    "\n",
    "for pred_fname in pred_to_class_to_input:\n",
    "    \n",
    "    class_ids = sorted(list(pred_to_class_to_input[pred_fname].keys()))\n",
    "    \n",
    "    #create histograms and histogram ranges for this prediction polygon\n",
    "    histograms, histogram_ranges = create_histograms(pred_to_class_to_input[pred_fname])\n",
    "    histograms_dict[pred_fname] = histograms\n",
    "    histogram_ranges_dict[pred_fname] = histogram_ranges\n",
    "    \n",
    "    #store constructed datasets\n",
    "    features = np.concatenate([pred_to_class_to_input[pred_fname][cid] for cid in class_ids], axis=0)\n",
    "    labels = np.array([single for item in [[cid]*len(pred_to_class_to_input[pred_fname][idx]) for idx,cid in enumerate(class_ids)] for single in item])\n",
    "    \n",
    "    #fit a random forest classifier\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(features, labels)\n",
    "    random_forest_clf_dict[pred_fname] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritvik\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\Users\\ritvik\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Predicted Classes and Scores\n"
     ]
    }
   ],
   "source": [
    "if METHOD == 'histogram':\n",
    "    #get the predicted classes and scores for histogram method\n",
    "    pred_class_scores = [get_classes(d, histograms_dict[fname], histogram_ranges_dict[fname], sorted(list(histograms_dict[fname].keys())), 0) for fname,d in prediction_data.items()]\n",
    "elif METHOD == 'random_forest':\n",
    "    #get the predicted classes and scores for random forest method\n",
    "    pred_class_scores = [[random_forest_clf_dict[fname].predict(d), np.sort(random_forest_clf_dict[fname].predict_proba(d), axis=1)[:,-2:]] for fname,d in prediction_data.items()]\n",
    "    scores = [2/(1+np.exp(1-item[1][:,-1] / item[1][:,-2]))-1 for item in pred_class_scores]\n",
    "    pred_class_scores = [[pred_class_scores[idx][0], scores[idx]] for idx in range(len(scores))]\n",
    "    \n",
    "print('Got Predicted Classes and Scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritvik\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in greater_equal\n"
     ]
    }
   ],
   "source": [
    "#this kernel will be used for the low pass filter\n",
    "ksize = 5\n",
    "kernel = np.ones((ksize,ksize)) / ksize**2\n",
    "\n",
    "#for each prediction file...\n",
    "for idx, feat_file_name in enumerate(prediction_feat_file_names):\n",
    "    \n",
    "    #get the shape of the file\n",
    "    feat_file_shape = prediction_feat_file_data[feat_file_name]['shape'][:2]\n",
    "    shape = feat_file_shape + (2,)\n",
    "    \n",
    "    #create empty matrix to store result\n",
    "    result = np.empty(shape)\n",
    "    result = result.reshape(-1, 2)\n",
    "    result[:] = np.nan\n",
    "    \n",
    "    #these are the valid indices \n",
    "    indices = prediction_feat_file_data[feat_file_name]['chosen_indices']\n",
    "\n",
    "    #load the predicted classes and scores\n",
    "    result[indices, 0] = pred_class_scores[idx][0]\n",
    "    result[indices, 1] = pred_class_scores[idx][1]\n",
    "\n",
    "    #shape back into a 3d matrix\n",
    "    result = result.reshape(shape)\n",
    "    mask = np.isnan(result[:,:,0])\n",
    "    \n",
    "    #get unique class ids\n",
    "    class_ids = np.sort(np.unique(result[:,:,0][result[:,:,0] >= 0]).astype(int))\n",
    "    class_scores = np.zeros(feat_file_shape + (len(class_ids),))\n",
    "    \n",
    "    #get the matrix of T/F wheter each pixel is predicted as each class\n",
    "    class_mtxs = {cid: (result[:,:,0] == cid) for cid in class_ids}\n",
    "    \n",
    "    #for each class id...\n",
    "    for cid in class_ids:\n",
    "        #convolve with the low pass filter\n",
    "        conv_mtx = convolve(class_mtxs[cid], kernel, mode='same')\n",
    "        conv_mtx[np.isnan(result[:,:,0])] = np.nan\n",
    "        class_scores[:,:,cid] = conv_mtx\n",
    "    \n",
    "    result[:,:,0] = np.argmax(class_scores, axis=-1)\n",
    "    result[:,:,0][mask] = np.nan\n",
    "\n",
    "    ds = np_array_to_raster('%s/%s_predicted_%s.tiff'%(PREDICTION_DATA_FOLDER, METHOD, idx), result, prediction_feat_file_data[feat_file_name]['gt'], no_data=-1, nband=2, gdal_data_type=gdal.GDT_Float64)\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
