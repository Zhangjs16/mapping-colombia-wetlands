{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import ee\n",
    "import requests\n",
    "import os\n",
    "from osgeo import gdal, ogr, osr\n",
    "import shutil\n",
    "from time import sleep\n",
    "from skimage.restoration import denoise_tv_bregman\n",
    "from math import ceil\n",
    "from googleapiclient.discovery import build\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is about 10 meters\n",
    "METERS_TO_DECIMAL_DEGREES_CONST = 1/30/3600\n",
    "\n",
    "#the value we use to signify no data at a pixel\n",
    "NO_DATA_VALUE = 65535\n",
    "\n",
    "#bands to despeckle\n",
    "BANDS_TO_DESPECKLE = ['HH', 'HV']\n",
    "\n",
    "#store the training data here\n",
    "DATA_FOLDER = 'training_data'\n",
    "PREDICTION_FOLDER = 'prediction_data'\n",
    "\n",
    "#create training data folder if not exists\n",
    "if DATA_FOLDER not in os.listdir():\n",
    "    os.mkdir(DATA_FOLDER)\n",
    "\n",
    "#create prediction data folder if not exists\n",
    "if PREDICTION_FOLDER not in os.listdir():\n",
    "    os.mkdir(PREDICTION_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the shapefile storing training polygons\n",
    "INPUT_POLYGONS_FILE = 'C:/Users/ritvik/Desktop/JPLProject/mapping-colombia-wetlands/input_polygons/input_polygons.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the shapefile storing prediction polygons\n",
    "PREDICTION_AREA_FILE = 'C:/Users/ritvik/Desktop/JPLProject/mapping-colombia-wetlands/prediction_polygons/prediction_region.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the list of bands to use for training. Choose from:\n",
    "#Sentinel-2: ['B2', 'B3', 'B4', 'B8', 'NDVI', 'NDWI']\n",
    "#ALOS-2: ['HH', 'HV']\n",
    "SELECTED_BANDS = ['B2', 'NDWI', 'NDVI', 'HH', 'HV']\n",
    "NUM_FEATURES = len(SELECTED_BANDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#any pixels above this elevation (in meters) will be disregarded from training \n",
    "MAX_CONSIDERED_ELEVATION = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the folder id in Google Drive where to temporarily store the GEE data before locally downloading\n",
    "GOOGLE_EARTH_ENGINE_GDRIVE_FOLDER_ID = '1KvlrUHs_rN7xPlw53qtd9pweeLwmrJSP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Manipulate data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_google_drive(file_id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : file_id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : file_id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_ids_from_google_drive():\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    result = service.files().list(q=\"parents in '%s'\"%GOOGLE_EARTH_ENGINE_GDRIVE_FOLDER_ID).execute()\n",
    "\n",
    "    file_name_to_file_id = {info['name'].split('-')[0]: info['id'] for info in result['files'] if len(info['name'].split('-')) == 2}\n",
    "    \n",
    "    return file_name_to_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file_from_google_drive_by_file_id(fid):\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    \n",
    "    service.files().delete(fileId=fid).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Download Data From Google Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(polygon_features, features, gdrive_folder, date_range, primary_dataset, selected_bands):\n",
    "    \"\"\"\n",
    "    This function accepts the below parameters and querys Google Earth Engine for data. The data is stored in \n",
    "    Google Drive.\n",
    "    \n",
    "    TODO\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store all started tasks\n",
    "    tasks = {}\n",
    "    \n",
    "    #work through each sub-region \n",
    "    for idx,polygon_feature in enumerate(polygon_features):\n",
    "        \n",
    "        filtered_imgs = []\n",
    "\n",
    "        #store the reference coordinates\n",
    "        x1 = polygon_feature.GetGeometryRef().GetEnvelope()[0]\n",
    "        y1 = polygon_feature.GetGeometryRef().GetEnvelope()[2]\n",
    "        ref_coords = (x1,y1)\n",
    "        \n",
    "        #get polygon area coordinates\n",
    "        area_coords = [list(pair) for pair in polygon_feature.GetGeometryRef().GetBoundary().GetPoints()]\n",
    "\n",
    "        #create an area of interest from Earth Engine Geometry\n",
    "        area_of_interest = ee.Geometry.Polygon(coords=area_coords)\n",
    "\n",
    "        #iterate over each data source\n",
    "        for data_type_source, bands in features.items():\n",
    "            data_type = data_type_source[0]\n",
    "            data_source = data_type_source[1]\n",
    "                \n",
    "            print('Working on data source: %s...'%data_source)\n",
    "            \n",
    "            if data_type == 'collection':\n",
    "                #access the Earth Engine image collection with the specified bands\n",
    "                data = ee.ImageCollection(data_source).select(bands)\n",
    "\n",
    "                #filter on date range\n",
    "                data_filtered = data.filterBounds(area_of_interest).filterDate(date_range[0], date_range[1])\n",
    "\n",
    "                #ensure there is at least 1 image\n",
    "                num_items = data_filtered.size().getInfo()\n",
    "                if num_items == 0:\n",
    "                    print('no items found, returning started tasks.')\n",
    "                    return tasks\n",
    "\n",
    "                band_info = data_filtered.first().getInfo()['bands'][0]\n",
    "\n",
    "                #if crs is already EPSG 4326, get resolution directly, otherwise need to transform from meters\n",
    "                if band_info['crs'] == 'EPSG:4326':\n",
    "                    res = band_info['crs_transform'][0]\n",
    "                else:\n",
    "                    res = band_info['crs_transform'][0] * METERS_TO_DECIMAL_DEGREES_CONST\n",
    "\n",
    "                #if this is the eventual primary dataset, store its resolution\n",
    "                if data_source == primary_dataset:\n",
    "                    eventual_res = res\n",
    "\n",
    "                #get a mosaic as median of all returned images\n",
    "                mosaic = ee.Image(data_filtered.median())\n",
    "\n",
    "                if data_source == 'COPERNICUS/S2_SR':\n",
    "\n",
    "                    #calculate NDVI\n",
    "                    if 'B4' in features[('collection','COPERNICUS/S2_SR')] and 'B8' in features[('collection','COPERNICUS/S2_SR')]:\n",
    "                        ndvi = mosaic.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "                        mosaic = ee.Image.addBands(mosaic, ndvi)\n",
    "\n",
    "                    #calculate NDWI\n",
    "                    if 'B3' in features[('collection','COPERNICUS/S2_SR')] and 'B8' in features[('collection','COPERNICUS/S2_SR')]:\n",
    "                        ndwi = mosaic.normalizedDifference(['B8', 'B3']).rename('NDWI')\n",
    "                        mosaic = ee.Image.addBands(mosaic, ndwi)\n",
    "                        \n",
    "            elif data_type == 'image':\n",
    "                mosaic = ee.Image(data_source).select(bands)\n",
    "\n",
    "            #add this mosaic to the list\n",
    "            filtered_imgs.append(mosaic)\n",
    "            \n",
    "        \n",
    "        #generate file name\n",
    "        features_str = '_'.join([item[1] for item in features.keys()]).replace('/','_')\n",
    "        fname = '%s-%s'%(idx, features_str)\n",
    "        print(fname)\n",
    "        \n",
    "        #add the various layers on top of each other to create a data cube with all features\n",
    "        final_img = ee.Image()\n",
    "        \n",
    "        for img in filtered_imgs:\n",
    "            final_img = ee.Image.addBands(final_img,img)\n",
    "        \n",
    "        #use the ALOS qa band to filter out invalid pixels\n",
    "        if 'qa' in features[('collection','JAXA/ALOS/PALSAR/YEARLY/SAR')]:\n",
    "            qa_band = final_img.select('qa')\n",
    "            qa_mask = qa_band.eq(0)\n",
    "            final_img = final_img.where(qa_mask, NO_DATA_VALUE)\n",
    "        \n",
    "        #use the Sentinel-2 SCL band to filter out invalid pixels\n",
    "        if 'SCL' in features[('collection','COPERNICUS/S2_SR')]:\n",
    "            scl_band = final_img.select('SCL')\n",
    "            scl_nodata_vals = [0,3,8,9,10]\n",
    "            scl_mask = scl_band.eq(0)\n",
    "            for v in scl_nodata_vals:\n",
    "                scl_mask = scl_mask.Or(scl_band.eq(v))\n",
    "            final_img = final_img.where(scl_mask, NO_DATA_VALUE)\n",
    "            \n",
    "        #use the SRTM elevationband to filter out invaild pixels\n",
    "        if 'elevation' in features['image','CGIAR/SRTM90_V4']:\n",
    "            elevation_band = final_img.select('elevation')\n",
    "            elevation_mask = elevation_band.gt(MAX_CONSIDERED_ELEVATION)\n",
    "            final_img = final_img.where(elevation_mask, NO_DATA_VALUE)\n",
    "            \n",
    "        #if any of the selected bands has NO_DATA_VALUE, mark that whole pixel as NO_DATA_VALUE\n",
    "        for b in selected_bands:\n",
    "            b_values = final_img.select(b)\n",
    "            b_mask = b_values.eq(NO_DATA_VALUE)\n",
    "            final_img = final_img.where(b_mask, NO_DATA_VALUE)\n",
    "         \n",
    "        #store the result with just the needed bands\n",
    "        selected_bands = sorted(selected_bands)\n",
    "        result = final_img.select(*selected_bands).float()\n",
    "          \n",
    "        #define the task to gather the data\n",
    "        task = ee.batch.Export.image.toDrive(image=result,\n",
    "                                             region=area_of_interest.getInfo()['coordinates'],\n",
    "                                             description=str(idx),\n",
    "                                             folder=gdrive_folder,\n",
    "                                             fileNamePrefix=fname,\n",
    "                                             crs_transform=[eventual_res, 0.0, ref_coords[0], 0.0, -eventual_res, ref_coords[1]],\n",
    "                                             crs='EPSG:4326')\n",
    "        \n",
    "        #store the task\n",
    "        tasks[fname] = task\n",
    "        \n",
    "        print('==================================')\n",
    "    \n",
    "    return list(tasks.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tasks_in_batches(tasks, batch_size, FOLDER):\n",
    "\n",
    "    #process the tasks in small batches to avoid memory running out\n",
    "    for batch_idx in range(ceil(len(tasks) / batch_size)):\n",
    "\n",
    "        #get the current batch of tasks\n",
    "        curr_tasks = tasks[batch_size*batch_idx:batch_size*(batch_idx+1)]\n",
    "        print('Processing Batch %s'%(batch_idx+1))\n",
    "\n",
    "        #start all tasks in that batch\n",
    "        for name,task in curr_tasks:\n",
    "            task.start()\n",
    "\n",
    "        print('Started all tasks in batch')\n",
    "\n",
    "        #wait until all tasks in that batch are done\n",
    "        curr_states = [task.status()['state'] for name,task in curr_tasks]\n",
    "        while set(curr_states) != {'COMPLETED'}:\n",
    "            print('Current states: %s'%curr_states)\n",
    "            sleep(30)\n",
    "            curr_states = [task.status()['state'] for name,task in curr_tasks]\n",
    "\n",
    "        #once all tasks done, get their file ids on google drive\n",
    "        file_name_to_file_id = get_file_ids_from_google_drive()\n",
    "\n",
    "        #for each file...\n",
    "        for fname, fid in file_name_to_file_id.items():\n",
    "\n",
    "            #get feature file name\n",
    "            features_file_name = '%s/features_%s.tiff'%(FOLDER, fname)\n",
    "\n",
    "            #check if data already downloaded\n",
    "            print('Downloading %s from Drive'%fname)\n",
    "            download_file_from_google_drive(fid, features_file_name)\n",
    "\n",
    "            print('Deleting %s from Drive'%fname)\n",
    "            delete_file_from_google_drive_by_file_id(fid)\n",
    "\n",
    "        print('================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Despeckling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_db(img):\n",
    "    return 10 * np.log10(img)\n",
    "\n",
    "def db_to_img(img):\n",
    "    return 10**(img / 10)\n",
    "\n",
    "def tv_denoise(arr, idxs_to_despeckle, weight):\n",
    "    copy_arr = arr.copy()\n",
    "    for idx in idxs_to_despeckle:\n",
    "        #get the layer\n",
    "        layer = copy_arr[:,:,idx]\n",
    "        \n",
    "        #denoise\n",
    "        img_db = img_to_db(layer)\n",
    "        img_db_tv = denoise_tv_bregman(img_db, weight)\n",
    "        img_tv = db_to_img(img_db_tv)\n",
    "        \n",
    "        #set denoised into copy of array\n",
    "        copy_arr[:,:,idx] = img_tv\n",
    "        \n",
    "    return copy_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Process Feature Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_files(feat_file_names, confidence_levels=None, combine=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        feat_file_names: a list of names of the downloaded training data files\n",
    "        confidence_levels: a list of confidence levels associated with each file in feat_file_names\n",
    "        combine: True if we wish to return a single numpy matrix. False if we wish to return a list of numpy matrices\n",
    "        \n",
    "    Outputs:\n",
    "        the processed training data and auxilary data like geotransforms\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store the numpy array of training data for each file\n",
    "    data = []\n",
    "    \n",
    "    #this will store auxilary data for each file\n",
    "    feat_file_data = {}\n",
    "    \n",
    "    #iterate over each file\n",
    "    for feat_file_name in feat_file_names:\n",
    "\n",
    "        #open file and get geotransform\n",
    "        ds = gdal.Open(feat_file_name, gdal.GA_ReadOnly)\n",
    "        gt = ds.GetGeoTransform()\n",
    "\n",
    "        #despeckle any bands which need to be despeckled\n",
    "        idx_to_despeckle = [idx for idx in range(ds.RasterCount) if ds.GetRasterBand(idx+1).GetDescription() in BANDS_TO_DESPECKLE]\n",
    "        arr = ds.ReadAsArray()\n",
    "        arr = np.stack([arr[i] for i in range(arr.shape[0])], axis=-1)\n",
    "        data_mask = (arr == NO_DATA_VALUE) | np.isnan(arr)\n",
    "        arr[data_mask] = NO_DATA_VALUE\n",
    "\n",
    "        arr = tv_denoise(arr, idx_to_despeckle, 1)\n",
    "        arr[data_mask] = np.nan\n",
    "\n",
    "        ds = None\n",
    "        \n",
    "        #only pick indices where no bands are NaN\n",
    "        chosen_indices = np.where(np.all(~np.isnan(arr), axis=-1))\n",
    "\n",
    "        #add this training data to the list\n",
    "        data.append(arr[chosen_indices])\n",
    "        \n",
    "        #add auxilary information\n",
    "        feat_file_data[feat_file_name] = {'chosen_indices': chosen_indices, 'shape': arr.shape, 'gt': gt}\n",
    "\n",
    "    #if combining, resample by confidence score and then concatenate training data \n",
    "    if combine:\n",
    "        num_pixels_to_sample = np.median([item.shape[0] for item in data])\n",
    "        data = [d[np.random.choice(d.shape[0], int(num_pixels_to_sample*confidence_levels[idx]))] for idx,d in enumerate(data)]\n",
    "        return np.concatenate(data, axis=0), feat_file_data\n",
    "    else:\n",
    "        return data, feat_file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(test_set, training_histograms, histogram_ranges, class_ids):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        test_set: the test set of features which we would like to classify\n",
    "        training_histograms: a dictionary of histograms, one for each class\n",
    "        histogram_ranges: the bin cuttoffs for the histograms\n",
    "        class_ids: a set of class ids\n",
    "        \n",
    "    Output:\n",
    "        an array of predicted classes and corresponding scores\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store the probabilities for each class\n",
    "    class_id_to_probs = {}\n",
    "    \n",
    "    #any probability density below this is considered as 0\n",
    "    min_allowable_density = min([0.25*np.max(training_histograms[cid]) for cid in class_ids])\n",
    "    \n",
    "    #iterate over each histogram\n",
    "    for class_id, histogram in training_histograms.items():\n",
    "        \n",
    "        #get the position of each test pixel in the context of this histogram\n",
    "        transposed_ranges = np.transpose(histogram_ranges)\n",
    "        expanded_dims_ranges = np.expand_dims(transposed_ranges, axis=1)\n",
    "        extended_ranges = np.concatenate([expanded_dims_ranges for _ in range(test_set.shape[0])], axis=1)\n",
    "        diffs = extended_ranges - test_set\n",
    "    \n",
    "        #get the probability density at each position\n",
    "        indices = np.argmax(diffs > 0, axis=0) - 1\n",
    "        indices[indices < 0] = 0\n",
    "        indices = indices - np.all((diffs>0)==False, axis=0)\n",
    "        indices = tuple(np.transpose(indices))\n",
    "        probs = histogram[indices]\n",
    "        \n",
    "        #store this in the dictionary\n",
    "        class_id_to_probs[class_id] = probs\n",
    "      \n",
    "    #create matrix of probabilities for each class\n",
    "    prob_mtx = np.stack([class_id_to_probs[cid] for cid in class_ids], axis=-1)\n",
    "    \n",
    "    #sort matrix of probs\n",
    "    sorted_probs = np.sort(prob_mtx, axis=1)\n",
    "    \n",
    "    #any pixel where all classes have 0 probability is NaN\n",
    "    nan_indices = np.where(sorted_probs[:,-1] < min_allowable_density)[0]\n",
    "    \n",
    "    #compute scores based on ratio of most likely class to second most likely class\n",
    "    scores = 1/(1+np.exp(-(sorted_probs[:,-1] / sorted_probs[:,-2])))\n",
    "    \n",
    "    #apply NaN pixels\n",
    "    scores[nan_indices] = np.nan\n",
    "\n",
    "    #get the predicted class\n",
    "    pred_class = np.argmax(prob_mtx, axis=1).astype(float)\n",
    "    \n",
    "    #apply NaN pixels\n",
    "    pred_class[nan_indices] = np.nan\n",
    "    \n",
    "    return pred_class, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Array to TIFF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raster(output_path, columns, rows, nband=1, gdal_data_type=gdal.GDT_Int32, driver=r'GTiff'):\n",
    "    ''' \n",
    "    returns gdal data source raster object \n",
    "    '''\n",
    "    \n",
    "    # create driver\n",
    "    driver = gdal.GetDriverByName(driver)\n",
    "\n",
    "    output_raster = driver.Create(output_path, columns, rows, nband, eType = gdal_data_type)    \n",
    "    \n",
    "    return output_raster\n",
    "\n",
    "def np_array_to_raster(output_path, arr, geotransform, no_data=None, nband=1, gdal_data_type=gdal.GDT_Int32, spatial_reference_system_wkid=4326, driver=r'GTiff'):\n",
    "    ''' \n",
    "    returns a gdal raster data source\n",
    "\n",
    "    keyword arguments:\n",
    "\n",
    "    output_path -- full path to the raster to be written to disk\n",
    "    numpy_array -- numpy array containing data to write to raster\n",
    "    upper_left_tuple -- the upper left point of the numpy array (should be a tuple structured as (x, y))\n",
    "    cell_resolution -- the cell resolution of the output raster\n",
    "    no_data -- value in numpy array that should be treated as no data\n",
    "    nband -- the band to write to in the output raster\n",
    "    gdal_data_type -- gdal data type of raster (see gdal documentation for list of values)\n",
    "    spatial_reference_system_wkid -- well known id (wkid) of the spatial reference of the data\n",
    "    driver -- string value of the gdal driver to use\n",
    "    '''\n",
    "\n",
    "    rows, columns = arr.shape[0], arr.shape[1]\n",
    "\n",
    "    # create output raster\n",
    "    output_raster = create_raster(output_path, columns, rows, nband, gdal_data_type) \n",
    "\n",
    "    spatial_reference = osr.SpatialReference()\n",
    "    spatial_reference.ImportFromEPSG(spatial_reference_system_wkid)\n",
    "    output_raster.SetProjection(spatial_reference.ExportToWkt())\n",
    "    output_raster.SetGeoTransform(geotransform)\n",
    "    \n",
    "    for band_idx in range(1,nband+1):\n",
    "        output_band = output_raster.GetRasterBand(band_idx)\n",
    "        if no_data != None:\n",
    "            output_band.SetNoDataValue(no_data)\n",
    "        if nband > 1:\n",
    "            output_band.WriteArray(arr[:,:,band_idx-1])\n",
    "        else:\n",
    "            output_band.WriteArray(arr)\n",
    "        output_band.FlushCache() \n",
    "        output_band.ComputeStatistics(False)\n",
    "\n",
    "    if os.path.exists(output_path) == False:\n",
    "        raise Exception('Failed to create raster: %s' % output_path)\n",
    "\n",
    "    return output_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Download Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "\n",
    "dataSource = driver.Open(INPUT_POLYGONS_FILE, gdal.GA_ReadOnly)\n",
    "\n",
    "layer = dataSource.GetLayer()\n",
    "\n",
    "polygon_features = [layer.GetNextFeature() for _ in range(layer.GetFeatureCount())]\n",
    "\n",
    "class_ids = set([f.GetField('class_id') for f in polygon_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {('collection','JAXA/ALOS/PALSAR/YEARLY/SAR'): ['HH', 'HV', 'qa'], ('collection', 'COPERNICUS/S2_SR'): ['B2', 'B3', 'B4', 'B8', 'SCL'], ('image','CGIAR/SRTM90_V4'): ['elevation']}\n",
    "date_range = ['2017-01-01', '2019-01-01']\n",
    "gdrive_folder = 'GoogleEarthEngine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "0-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "1-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "2-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "3-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "4-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "5-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "6-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "7-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "8-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "9-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "10-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "11-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "tasks = get_training_data(polygon_features, features, gdrive_folder, date_range, 'COPERNICUS/S2_SR', SELECTED_BANDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch 1\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['RUNNING', 'COMPLETED', 'RUNNING', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'COMPLETED']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'COMPLETED']\n",
      "Downloading 8 from Drive\n",
      "Deleting 8 from Drive\n",
      "Downloading 9 from Drive\n",
      "Deleting 9 from Drive\n",
      "Downloading 7 from Drive\n",
      "Deleting 7 from Drive\n",
      "Downloading 6 from Drive\n",
      "Deleting 6 from Drive\n",
      "Downloading 5 from Drive\n",
      "Deleting 5 from Drive\n",
      "Downloading 4 from Drive\n",
      "Deleting 4 from Drive\n",
      "Downloading 3 from Drive\n",
      "Deleting 3 from Drive\n",
      "Downloading 2 from Drive\n",
      "Deleting 2 from Drive\n",
      "Downloading 0 from Drive\n",
      "Deleting 0 from Drive\n",
      "Downloading 1 from Drive\n",
      "Deleting 1 from Drive\n",
      "================================\n",
      "Processing Batch 2\n",
      "Started all tasks in batch\n",
      "Current states: ['READY', 'READY']\n",
      "Current states: ['RUNNING', 'RUNNING']\n",
      "Downloading 11 from Drive\n",
      "Deleting 11 from Drive\n",
      "Downloading 10 from Drive\n",
      "Deleting 10 from Drive\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "execute_tasks_in_batches(tasks, 10, DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Get Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datasets = {}\n",
    "testing_datasets = {}\n",
    "        \n",
    "for class_id in class_ids:\n",
    "    feat_file_names_and_confidence = [('%s/features_%s.tiff'%(DATA_FOLDER, idx), f.GetField('confidence')) for idx,f in enumerate(polygon_features) if f.GetField('class_id') == class_id]\n",
    "    feat_file_names = [item[0] for item in feat_file_names_and_confidence]\n",
    "    confidence_levels = [item[1] for item in feat_file_names_and_confidence]\n",
    "    \n",
    "    data, _ = process_feature_files(feat_file_names, confidence_levels)\n",
    "\n",
    "    lower_lims = data.mean(axis=0) - 2*data.std(axis=0)\n",
    "    upper_lims = data.mean(axis=0) + 2*data.std(axis=0)\n",
    "\n",
    "    valid_indices = np.all(data < upper_lims, axis=1) & np.all(data > lower_lims, axis=1)\n",
    "    data = data[valid_indices]\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    train_len = int(len(data)*0.75)\n",
    "\n",
    "    training_datasets[class_id] = data[:train_len]\n",
    "    testing_datasets[class_id] = data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_feature_values = [min([min(dataset[:,idx]) for dataset in training_datasets.values()]) for idx in range(NUM_FEATURES)]\n",
    "max_feature_values = [max([max(dataset[:,idx]) for dataset in training_datasets.values()]) for idx in range(NUM_FEATURES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_ranges = []\n",
    "training_histograms = {}\n",
    "num_bins = 3\n",
    "\n",
    "for idx in range(NUM_FEATURES):\n",
    "    width = (max_feature_values[idx] - min_feature_values[idx]) / num_bins\n",
    "    histogram_ranges.append(np.arange(min_feature_values[idx], max_feature_values[idx]+width*.99, width))\n",
    "histogram_ranges = np.array(histogram_ranges)\n",
    "\n",
    "for class_id in class_ids:\n",
    "    training_histograms[class_id] = np.histogramdd(training_datasets[class_id], bins=histogram_ranges, density=True)[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Class 1 : 0.9611449855856316\n",
      "Unique Labels: [1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritvik\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "for cid in class_ids:\n",
    "    pred_class, scores = get_classes(testing_datasets[cid], training_histograms, histogram_ranges, sorted(list(class_ids)))\n",
    "    print('Accuracy for Class %s : %s'%(cid, np.mean(pred_class == cid)))\n",
    "    print('Unique Labels: %s'%np.unique(pred_class[~np.isnan(pred_class)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Use Model to Predict Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "dataSource = driver.Open(PREDICTION_AREA_FILE, gdal.GA_ReadOnly)\n",
    "layer = dataSource.GetLayer()\n",
    "polygon_features = [layer.GetNextFeature() for _ in range(layer.GetFeatureCount())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "0-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "tasks = get_training_data(polygon_features, features, gdrive_folder, date_range, 'COPERNICUS/S2_SR', SELECTED_BANDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch 1\n",
      "Started all tasks in batch\n",
      "Current states: ['READY']\n",
      "Current states: ['RUNNING']\n",
      "Current states: ['RUNNING']\n",
      "Current states: ['RUNNING']\n",
      "Current states: ['RUNNING']\n",
      "Current states: ['RUNNING']\n",
      "Current states: ['RUNNING']\n",
      "Current states: ['RUNNING']\n",
      "Current states: ['RUNNING']\n",
      "Current states: ['RUNNING']\n",
      "Current states: ['RUNNING']\n",
      "Current states: ['RUNNING']\n",
      "Current states: ['RUNNING']\n",
      "Downloading 0 from Drive\n",
      "Deleting 0 from Drive\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "execute_tasks_in_batches(tasks, 10, PREDICTION_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Prediction Regions\n"
     ]
    }
   ],
   "source": [
    "feat_file_names = ['%s/features_%s.tiff'%(PREDICTION_FOLDER, idx) for idx,f in enumerate(polygon_features)]\n",
    "data, feat_file_data = process_feature_files(feat_file_names, combine=False)\n",
    "print('Processed Prediction Regions')\n",
    "pred_class_scores = [get_classes(d, training_histograms, histogram_ranges, sorted(list(class_ids))) for d in data]\n",
    "print('Got Predicted Classes and Scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, feat_file_name in enumerate(feat_file_names):\n",
    "    shape = feat_file_data[feat_file_name]['shape'][:2] + (2,)\n",
    "    result = np.empty(shape)\n",
    "    result[:] = np.nan\n",
    "    \n",
    "    idx_classes = feat_file_data[feat_file_name]['chosen_indices'] + (np.array([0]*len(feat_file_data[feat_file_name]['chosen_indices'][0])),)\n",
    "    idx_scores = feat_file_data[feat_file_name]['chosen_indices'] + (np.array([1]*len(feat_file_data[feat_file_name]['chosen_indices'][0])),)\n",
    "    \n",
    "    result[idx_classes] = pred_class_scores[idx][0]\n",
    "    result[idx_scores] = pred_class_scores[idx][1]\n",
    "    \n",
    "    ds = np_array_to_raster('%s/predicted_%s.tiff'%(PREDICTION_FOLDER, idx), result, feat_file_data[feat_file_name]['gt'], no_data=-1, nband=2, gdal_data_type=gdal.GDT_Float64)\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
