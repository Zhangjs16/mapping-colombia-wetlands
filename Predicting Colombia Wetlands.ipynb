{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "from time import sleep\n",
    "from math import ceil\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import ee\n",
    "from osgeo import gdal, ogr, osr\n",
    "from skimage.restoration import denoise_tv_bregman\n",
    "from googleapiclient.discovery import build\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.signal import convolve\n",
    "\n",
    "#run this line if you've never authenticated earthengine\n",
    "#ee.Authenticate()\n",
    "\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is about 30 meters\n",
    "RESOLUTION = 1/3600\n",
    "\n",
    "#the value we use to signify no data at a pixel\n",
    "NO_DATA_VALUE = 65535\n",
    "\n",
    "#bands to despeckle\n",
    "BANDS_TO_DESPECKLE = ['HH', 'HV']\n",
    "\n",
    "#store the roi data here\n",
    "ROI_DATA_FOLDER = 'roi_data'\n",
    "\n",
    "#store the suggested roi classes here\n",
    "ROI_SUGGESTED_FOLDER = 'roi_suggested'\n",
    "\n",
    "#store the predicted roi classes here\n",
    "ROI_PREDICTED_FOLDER = 'roi_predicted'\n",
    "\n",
    "for FOLDER in [ROI_DATA_FOLDER, ROI_SUGGESTED_FOLDER, ROI_PREDICTED_FOLDER]:\n",
    "    #create training data folder if not exists\n",
    "    if FOLDER not in os.listdir():\n",
    "        os.mkdir(FOLDER)\n",
    "    \n",
    "#features to extract from GEE in the training process\n",
    "FEATURES = {\n",
    "            ('collection','JAXA/ALOS/PALSAR/YEARLY/SAR'): ['HH', 'HV', 'qa'], \n",
    "            ('collection', 'LANDSAT/LC08/C01/T1_8DAY_NDVI'): ['NDVI'], \n",
    "            ('collection', 'LANDSAT/LC08/C01/T1_8DAY_NDWI'): ['NDWI'],\n",
    "            ('collection', 'LANDSAT/LC08/C01/T1_SR'): ['B3', 'B2', 'B4', 'B5', 'pixel_qa'],\n",
    "            ('image','CGIAR/SRTM90_V4'): ['elevation']\n",
    "           }\n",
    "\n",
    "#the shapefile storing training polygons\n",
    "TRAINING_POLYGONS_FILE = 'C:/Users/ritvik/Desktop/JPLProject/mapping-colombia-wetlands/training_polygons/training_polygons.shp'\n",
    "\n",
    "#the shapefile storing roi polygons\n",
    "ROI_POLYGONS_FILE = 'C:/Users/ritvik/Desktop/JPLProject/mapping-colombia-wetlands/roi_polygons/roi_polygons.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the list of bands to use for training. Choose from:\n",
    "#Landsat: ['NDVI', 'NDWI', 'B2', 'B3', 'B4', 'B5']\n",
    "#ALOS-2: ['HH', 'HV']\n",
    "#CGIAR: ['elevation']\n",
    "\n",
    "SELECTED_BANDS = ['NDVI', 'NDWI', 'B3', 'B2', 'B4', 'elevation']\n",
    "\n",
    "#update the features dictionary\n",
    "FEATURES = {collection: bands for collection,bands in FEATURES.items() if len([b for b in SELECTED_BANDS if b in bands]) != 0 or collection == ('image','CGIAR/SRTM90_V4')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#any pixels above this elevation (in meters) will be disregarded from training \n",
    "MAX_CONSIDERED_ELEVATION = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data date range\n",
    "DATE_RANGE = ['2017-01-01', '2020-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to use for prediction. Choices are 'histogram' or 'random_forest'\n",
    "METHOD = 'random_forest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whether or not this notebook will be online. \n",
    "#If True then we will download data from Google Earth Engine, requiring an internet connection.\n",
    "#If False, the user is expected to populate the folder \"roi_data\" with existing rasters. No internet connection needed.\n",
    "ONLINE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the folder id in Google Drive where to temporarily store the GEE data before locally downloading\n",
    "GOOGLE_EARTH_ENGINE_GDRIVE_FOLDER_ID = '1KvlrUHs_rN7xPlw53qtd9pweeLwmrJSP'\n",
    "\n",
    "#the name of that same Google Drive folder\n",
    "GDRIVE_FOLDER_NAME = 'GoogleEarthEngine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Purpose Functions to Read Google Drive Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_google_drive(file_id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "    \n",
    "    max_tries = 10\n",
    "    curr_try = 0\n",
    "    status_code = -1\n",
    "    \n",
    "    while status_code != 200 and curr_try < max_tries:\n",
    "        if curr_try > 0:\n",
    "            sleep(30)\n",
    "        session = requests.Session()\n",
    "        response = session.get(URL, params = { 'id' : file_id }, stream = True)\n",
    "        status_code = response.status_code\n",
    "        curr_try += 1\n",
    "        \n",
    "    if status_code != 200:\n",
    "        return\n",
    "    \n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : file_id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_ids_from_google_drive():\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    result = service.files().list(q=\"parents in '%s'\"%GOOGLE_EARTH_ENGINE_GDRIVE_FOLDER_ID).execute()\n",
    "\n",
    "    file_name_to_file_id = {info['name']: info['id'] for info in result['files']}\n",
    "    \n",
    "    return file_name_to_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file_from_google_drive_by_file_id(fid):\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    \n",
    "    service.files().delete(fileId=fid).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Purpose Functions to Download Data From Google Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_landsat_sr(image):\n",
    "    cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
    "    cloudsBitMask = ee.Number(2).pow(5).int()\n",
    "\n",
    "    # Get the pixel QA band.\n",
    "    qa = image.select('pixel_qa')\n",
    "\n",
    "    # Both flags should be set to zero, indicating clear conditions.\n",
    "    mask = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
    "\n",
    "    return image.updateMask(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_quality_bands(image):\n",
    "    return image.addBands(image.normalizedDifference(['B5', 'B4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(polygon_features, features, gdrive_folder, date_range, selected_bands):\n",
    "    \"\"\"\n",
    "    This function accepts the below parameters and querys Google Earth Engine for data. The data is stored in \n",
    "    Google Drive.\n",
    "    \n",
    "    Inputs:\n",
    "        polygon_features: a list of pairs like (index, polygon feature) indicating which polygons of data to download\n",
    "        features: the features to extract from GEE\n",
    "        gdrive_folder: the name of the folder in Google Drive where the downloaded data will live\n",
    "        date_range: the start and end date to gather data\n",
    "        selected_bands: list of bands to subset\n",
    "        \n",
    "    Output:\n",
    "        list of tasks which are ready to be started\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store all started tasks\n",
    "    tasks = {}\n",
    "    \n",
    "    #work through each sub-region \n",
    "    for curr_idx, polygon_feature in polygon_features:\n",
    "        \n",
    "        skip_polygon = False\n",
    "        \n",
    "        filtered_imgs = []\n",
    "\n",
    "        #store the reference coordinates\n",
    "        x1 = polygon_feature.GetGeometryRef().GetEnvelope()[0]\n",
    "        y1 = polygon_feature.GetGeometryRef().GetEnvelope()[2]\n",
    "        ref_coords = (x1,y1)\n",
    "        \n",
    "        #generate file name\n",
    "        fname = hash(''.join([str(item) for item in polygon_feature.GetGeometryRef().GetEnvelope()]))\n",
    "        \n",
    "        #get polygon area coordinates\n",
    "        area_coords = [list(pair) for pair in polygon_feature.GetGeometryRef().GetBoundary().GetPoints()]\n",
    "\n",
    "        #create an area of interest from Earth Engine Geometry\n",
    "        area_of_interest = ee.Geometry.Polygon(coords=area_coords)\n",
    "\n",
    "        #iterate over each data source\n",
    "        for data_type_source, bands in features.items():\n",
    "            data_type = data_type_source[0]\n",
    "            data_source = data_type_source[1]\n",
    "                \n",
    "            print('Working on data source: %s...'%data_source)\n",
    "            \n",
    "            if data_type == 'collection':\n",
    "                #access the Earth Engine image collection with the specified bands\n",
    "                data = ee.ImageCollection(data_source).select(bands)\n",
    "\n",
    "                #filter on date range and area of interest\n",
    "                data_filtered = data.filterBounds(area_of_interest).filterDate(date_range[0], date_range[1])\n",
    "                \n",
    "                #if data source is landsat SR, further filter by cloud cover\n",
    "                if data_source == 'LANDSAT/LC08/C01/T1_SR':\n",
    "                    data_filtered = data_filtered.filterMetadata('CLOUD_COVER', 'less_than', 50).map(mask_landsat_sr).map(add_quality_bands)\n",
    "                    \n",
    "                #ensure there is at least 1 image\n",
    "                num_items = data_filtered.size().getInfo()\n",
    "                print('%s images in collection'%num_items)\n",
    "                if num_items == 0:\n",
    "                    skip_polygon = True\n",
    "                    break\n",
    "\n",
    "                #if LANDSAT, quality mosaic by its respective metric\n",
    "                if 'LANDSAT' in data_source:\n",
    "                    #derived LANDSAT band\n",
    "                    if '8DAY' in data_source:\n",
    "                        mosaic = data_filtered.qualityMosaic(features[data_type_source][0])\n",
    "                    #LANDSAT Surface Reflectance\n",
    "                    else:\n",
    "                        mosaic = data_filtered.qualityMosaic('nd')\n",
    "                #otherwise just do a simple median\n",
    "                else:\n",
    "                    mosaic = data_filtered.median()\n",
    "                   \n",
    "            elif data_type == 'image':\n",
    "                mosaic = ee.Image(data_source).select(bands)\n",
    "\n",
    "            #add this mosaic to the list\n",
    "            filtered_imgs.append(mosaic)\n",
    "        \n",
    "        if skip_polygon:\n",
    "            print('Skipping Polygon%s'%curr_idx)\n",
    "            tasks[fname] = None\n",
    "            print('==================================')\n",
    "            continue\n",
    "        \n",
    "        #add the various layers on top of each other to create a data cube with all features\n",
    "        final_img = ee.Image()\n",
    "        \n",
    "        for img in filtered_imgs:\n",
    "            final_img = ee.Image.addBands(final_img,img)\n",
    "        \n",
    "        #use the ALOS qa band to filter out invalid pixels\n",
    "        if ('collection','JAXA/ALOS/PALSAR/YEARLY/SAR') in features and 'qa' in features[('collection','JAXA/ALOS/PALSAR/YEARLY/SAR')]:\n",
    "            qa_band = final_img.select('qa')\n",
    "            qa_mask = qa_band.eq(0)\n",
    "            final_img = final_img.where(qa_mask, NO_DATA_VALUE)\n",
    "            \n",
    "        #use the SRTM elevation band to filter out invaild pixels\n",
    "        if ('image','CGIAR/SRTM90_V4') in features and 'elevation' in features[('image','CGIAR/SRTM90_V4')]:\n",
    "            elevation_band = final_img.select('elevation')\n",
    "            elevation_mask = elevation_band.gt(MAX_CONSIDERED_ELEVATION)\n",
    "            final_img = final_img.where(elevation_mask, NO_DATA_VALUE)\n",
    "            \n",
    "        #if any of the selected bands has NO_DATA_VALUE, mark that whole pixel as NO_DATA_VALUE\n",
    "        for b in selected_bands:\n",
    "            b_values = final_img.select(b)\n",
    "            b_mask = b_values.eq(NO_DATA_VALUE)\n",
    "            final_img = final_img.where(b_mask, NO_DATA_VALUE)\n",
    "         \n",
    "        #store the result with just the needed bands\n",
    "        selected_bands = sorted(selected_bands)\n",
    "        result = final_img.select(*selected_bands).float()\n",
    "          \n",
    "        #define the task to gather the data\n",
    "        task = ee.batch.Export.image.toDrive(image=result,\n",
    "                                             region=area_of_interest.getInfo()['coordinates'],\n",
    "                                             description=str(curr_idx),\n",
    "                                             folder=gdrive_folder,\n",
    "                                             fileNamePrefix=str(fname),\n",
    "                                             crs_transform=[RESOLUTION, 0.0, ref_coords[0], 0.0, -RESOLUTION, ref_coords[1]],\n",
    "                                             crs='EPSG:4326')\n",
    "        \n",
    "        #store the task\n",
    "        tasks[fname] = task\n",
    "        \n",
    "        print('==================================')\n",
    "        \n",
    "    return list(tasks.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tasks_in_batches(tasks, batch_size, FOLDER, wait_seconds=30):\n",
    "    \"\"\"\n",
    "    Executes a list of tasks in batches\n",
    "    \n",
    "    Inputs:\n",
    "        tasks: list of tasks\n",
    "        batch_size: number of tasks to execute per batch\n",
    "        FOLDER: the folder to store the downloaded rasters\n",
    "    \"\"\"\n",
    "    \n",
    "    fnames = []\n",
    "\n",
    "    #process the tasks in small batches to avoid memory running out\n",
    "    for batch_idx in range(ceil(len(tasks) / batch_size)):\n",
    "\n",
    "        #get the current batch of tasks\n",
    "        curr_tasks = tasks[batch_size*batch_idx:batch_size*(batch_idx+1)]\n",
    "        print('Processing Batch %s'%(batch_idx+1))\n",
    "\n",
    "        #start all tasks in that batch\n",
    "        for name,task in curr_tasks:\n",
    "            if task != None:\n",
    "                task.start()\n",
    "\n",
    "        print('Started all tasks in batch')\n",
    "\n",
    "        #wait until all tasks in that batch are done\n",
    "        curr_states = [task.status()['state'] for name,task in curr_tasks if task != None]\n",
    "        while 'RUNNING' in set(curr_states) or 'READY' in set(curr_states):\n",
    "            print('Current states: %s'%curr_states)\n",
    "            sleep(wait_seconds)\n",
    "            curr_states = [task.status()['state'] for name,task in curr_tasks if task != None]\n",
    "\n",
    "        #once all tasks done, get their file ids on google drive\n",
    "        file_name_to_file_id = get_file_ids_from_google_drive()\n",
    "\n",
    "        #for each file...\n",
    "        for fname, fid in file_name_to_file_id.items():\n",
    "\n",
    "            #get feature file name\n",
    "            features_file_name = '%s/features_%s'%(FOLDER, fname)\n",
    "\n",
    "            #check if data already downloaded\n",
    "            print('Downloading %s from Drive'%fname)\n",
    "            download_file_from_google_drive(fid, features_file_name)\n",
    "\n",
    "            print('Deleting %s from Drive'%fname)\n",
    "            delete_file_from_google_drive_by_file_id(fid)\n",
    "            \n",
    "            fnames.append(features_file_name)\n",
    "\n",
    "        print('================================')\n",
    "        \n",
    "    return fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Purpose Despeckling, Clustering, and Raster Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_db(img):\n",
    "    return 10 * np.log10(img+.00001)\n",
    "\n",
    "def db_to_img(img):\n",
    "    return 10**(img / 10)\n",
    "\n",
    "def tv_denoise(arr, idxs_to_despeckle, weight):\n",
    "    copy_arr = arr.copy()\n",
    "    for idx in idxs_to_despeckle:\n",
    "        #get the layer\n",
    "        layer = copy_arr[:,:,idx]\n",
    "        \n",
    "        orig_valid_mask = ~np.isnan(layer)\n",
    "        \n",
    "        #denoise\n",
    "        img_db = img_to_db(layer)\n",
    "        img_db_tv = denoise_tv_bregman(img_db, weight)\n",
    "        img_tv = db_to_img(img_db_tv)\n",
    "        img_tv[orig_valid_mask & np.isnan(img_tv)] = layer[orig_valid_mask & np.isnan(img_tv)]\n",
    "        \n",
    "        #set denoised into copy of array\n",
    "        copy_arr[:,:,idx] = img_tv\n",
    "        \n",
    "    return copy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sep_metric(feat_file_name, num_clusters_options):\n",
    "    \"\"\"\n",
    "    Get the separability metric for the given array and given possible clusters\n",
    "    \n",
    "    Inputs:\n",
    "        feat_file_name: path to file to analyze\n",
    "        num_clusters_options: list of number of clusters to try\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #read the array\n",
    "    ds = gdal.Open('%s/%s'%(ROI_DATA_FOLDER, feat_file_name), gdal.GA_ReadOnly)\n",
    "\n",
    "    #get gt and read array\n",
    "    gt = ds.GetGeoTransform()\n",
    "    arr = ds.ReadAsArray()\n",
    "    arr = np.stack([arr[i] for i in range(arr.shape[0])], axis=-1)\n",
    "    ds = None\n",
    "\n",
    "    #transform array to 2d\n",
    "    arr_2d = arr.reshape(-1,arr.shape[-1])\n",
    "    num_bands = arr_2d.shape[-1]\n",
    "    arr_2d[arr_2d == NO_DATA_VALUE] = np.nan\n",
    "    valid_indices = np.where(np.all(~np.isnan(arr_2d), axis=-1))[0]\n",
    "    arr_2d_valid = arr_2d[valid_indices]\n",
    "    \n",
    "    #this will store the separability metric values for differet k\n",
    "    sep_metric_dict = {}\n",
    "    \n",
    "    #for each value of k (number clusters)\n",
    "    for k in num_clusters_options:\n",
    "        \n",
    "        print('trying %s clusters...'%k)\n",
    "\n",
    "        #define model\n",
    "        model = KMeans(n_clusters=k)\n",
    "\n",
    "        #fit model and predict clusters\n",
    "        cluster_preds = model.fit_predict(arr_2d_valid)\n",
    "\n",
    "        #these will store the average and deviation of each band for each cluster\n",
    "        mu_vals = np.zeros((k,num_bands))\n",
    "        dev_vals = np.zeros((k,num_bands))\n",
    "\n",
    "        #populate above arrays\n",
    "        for cid in range(k):\n",
    "            cluster_data = arr_2d_valid[cluster_preds == cid]\n",
    "            mu, dev = cluster_data.mean(axis=0), cluster_data.std(axis=0)\n",
    "            mu_vals[cid] = mu\n",
    "            dev_vals[cid] = dev\n",
    "\n",
    "        #get pairwise separability metric values and put in list\n",
    "        sep_metric_vals = []\n",
    "        for c1 in range(k):\n",
    "            for c2 in range(c1+1,k):\n",
    "                sep_metric_vals.append(np.mean(abs(mu_vals[c1] - mu_vals[c2]) / (dev_vals[c1] + dev_vals[c2])))\n",
    "\n",
    "        #add information to dictionary\n",
    "        sep_metric_dict[k] = {'sep_metric_vals': sep_metric_vals, 'cluster_preds': cluster_preds}\n",
    "    \n",
    "    #best separating k has the highest median separation between each pair of clusters\n",
    "    best_sep_entry = sorted(sep_metric_dict.items(), key=lambda info: -np.median(info[1]['sep_metric_vals']))[0]\n",
    "    print('Best Separating k: %s'%str(best_sep_entry[0]))\n",
    "    \n",
    "    #put the result in an array for viewing in a GIS editor\n",
    "    result = np.ones(arr_2d.shape[0])\n",
    "    result[:] = np.nan\n",
    "    result[valid_indices] = best_sep_entry[1]['cluster_preds']\n",
    "    result = result.reshape(arr.shape[:2])\n",
    "    \n",
    "    #apply a majority filter to smooth the clusters out\n",
    "    result = apply_majority_filter(result, 5)\n",
    "\n",
    "    #save to array\n",
    "    ds = np_array_to_raster('%s/suggested_%s'%(ROI_SUGGESTED_FOLDER, feat_file_name), result, ['class_id'], gt, no_data=-1, nband=1, gdal_data_type=gdal.GDT_Float64)\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_feature_around_raster(raster_fname):\n",
    "    \"\"\"\n",
    "    Gets a bounding osgeo.ogr.Feature around raster with given file name\n",
    "    \"\"\"\n",
    "    \n",
    "    #get raster\n",
    "    raster = gdal.Open(raster_fname, gdal.GA_ReadOnly)\n",
    "\n",
    "    # Get raster geometry\n",
    "    transform = raster.GetGeoTransform()\n",
    "\n",
    "    pixelWidth = transform[1]\n",
    "    pixelHeight = transform[5]\n",
    "    cols = raster.RasterXSize\n",
    "    rows = raster.RasterYSize\n",
    "\n",
    "    raster = None\n",
    "\n",
    "    #get extents\n",
    "    xLeft = transform[0]\n",
    "    yTop = transform[3]\n",
    "    xRight = xLeft+cols*pixelWidth\n",
    "    yBottom = yTop+rows*pixelHeight\n",
    "\n",
    "    #form raster geometry\n",
    "    ring = ogr.Geometry(ogr.wkbLinearRing)\n",
    "    ring.AddPoint(xLeft, yTop)\n",
    "    ring.AddPoint(xLeft, yBottom)\n",
    "    ring.AddPoint(xRight, yBottom)\n",
    "    ring.AddPoint(xRight, yTop)\n",
    "    ring.AddPoint(xLeft, yTop)\n",
    "\n",
    "    rasterGeometry = ogr.Geometry(ogr.wkbPolygon)\n",
    "    rasterGeometry.AddGeometry(ring)\n",
    "    \n",
    "    return rasterGeometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_temporary_files():\n",
    "    \"\"\"\n",
    "    removes temporary files created in the prediction process. \n",
    "    These files are designated to start with \"temp_\"\n",
    "    \"\"\"\n",
    "    \n",
    "    #remove temporary files\n",
    "    for temp_fname in [fname for fname in os.listdir() if 'temp_' in fname]:\n",
    "        try:\n",
    "            os.remove(temp_fname)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        except PermissionError:\n",
    "            print('Failed to remove %s due to PermissionError; please attempt to remove manually'%temp_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_majority_filter(arr, ksize):\n",
    "    \"\"\"\n",
    "    Apply majority filter to array of predicted class ids. Predicted labels assumed to be >= 0.\n",
    "    \n",
    "    Inputs:\n",
    "        arr: the array\n",
    "        ksize: the size of the kernel. Bigger means more neighboring pixels considered in the decision\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #this kernel will be used for the majority filter on predicted class id\n",
    "    ksize = 5\n",
    "    kernel = np.ones((ksize,ksize)) / ksize**2\n",
    "    \n",
    "    #get the original NaN values\n",
    "    mask = np.isnan(arr)\n",
    "    \n",
    "    #get unique class ids\n",
    "    class_ids = np.sort(np.unique(arr[arr >= 0]).astype(int))\n",
    "    class_scores = np.zeros(arr.shape + (len(class_ids),))\n",
    "    \n",
    "    #get the matrix of T/F wheter each pixel is predicted as each class\n",
    "    class_mtxs = {cid: (arr == cid) for cid in class_ids}\n",
    "    \n",
    "    #for each class id...\n",
    "    for cid in class_ids:\n",
    "        #convolve with the low pass filter\n",
    "        conv_mtx = convolve(class_mtxs[cid], kernel, mode='same')\n",
    "        conv_mtx[np.isnan(arr)] = np.nan\n",
    "        class_scores[:,:,cid] = conv_mtx\n",
    "    \n",
    "    filtered_arr = np.argmax(class_scores, axis=-1).astype(float)\n",
    "    filtered_arr[mask] = np.nan\n",
    "    \n",
    "    return filtered_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Process Feature Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_files(feat_file_names):\n",
    "    \"\"\"\n",
    "    This function accepts a list of file names and processes those rasters. \n",
    "    \n",
    "    Inputs:\n",
    "        feat_file_names: a list of names of the downloaded training data files\n",
    "        confidence_levels: a list of confidence levels associated with each file in feat_file_names\n",
    "        preprocess: whether to remove some pixels that likely do not belong\n",
    "        \n",
    "    Outputs:\n",
    "        the processed training data and auxilary data such as geotransforms\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store the numpy array of training data for each file\n",
    "    data = {}\n",
    "    \n",
    "    #this will store auxilary data for each file\n",
    "    feat_file_data = {}\n",
    "    \n",
    "    #iterate over each file\n",
    "    for feat_file_name in feat_file_names:\n",
    "        \n",
    "        feat_file_name = '%s/%s'%(ROI_DATA_FOLDER, feat_file_name)\n",
    "\n",
    "        #open file and get geotransform\n",
    "        orig_ds = gdal.Open(feat_file_name, gdal.GA_ReadOnly)\n",
    "        \n",
    "        try:\n",
    "            gt = orig_ds.GetGeoTransform()\n",
    "        except AttributeError:\n",
    "            print('Could not process %s'%feat_file_name)\n",
    "            continue\n",
    "\n",
    "        #despeckle any bands which need to be despeckled\n",
    "        band_names = [orig_ds.GetRasterBand(idx+1).GetDescription() for idx in range(orig_ds.RasterCount)]\n",
    "        idx_to_despeckle = [idx for idx in range(orig_ds.RasterCount) if orig_ds.GetRasterBand(idx+1).GetDescription() in BANDS_TO_DESPECKLE]\n",
    "        arr = orig_ds.ReadAsArray()\n",
    "        arr = np.stack([arr[i] for i in range(arr.shape[0])], axis=-1)\n",
    "        data_mask = np.any(arr == NO_DATA_VALUE, axis=-1) | np.any(np.isnan(arr), axis=-1)\n",
    "        arr[data_mask] = NO_DATA_VALUE\n",
    "\n",
    "        arr = tv_denoise(arr, idx_to_despeckle, 1)\n",
    "        arr[data_mask] = np.nan\n",
    "\n",
    "        orig_ds = None\n",
    "        \n",
    "        out_ds = np_array_to_raster(feat_file_name, arr, band_names, gt, no_data=NO_DATA_VALUE, nband=arr.shape[-1], gdal_data_type=gdal.GDT_Float64)\n",
    "        out_ds = None\n",
    "        \n",
    "        print('Processed %s'%feat_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_histograms(training_datasets):\n",
    "    \"\"\"\n",
    "    Creates a set of histograms, one for each class\n",
    "    \n",
    "    Inputs:\n",
    "        training_histograms: a dictionary mapping class_id to a dataset\n",
    "        \n",
    "    Outputs:\n",
    "        a dictionary mapping class_id to a histogram\n",
    "        a list of histogram bin cuttoffs for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    num_features = list(training_datasets.values())[0].shape[-1]\n",
    "    \n",
    "    min_feature_values = [min([min(dataset[:,idx]) for dataset in training_datasets.values()]) for idx in range(num_features)]\n",
    "    max_feature_values = [max([max(dataset[:,idx]) for dataset in training_datasets.values()]) for idx in range(num_features)]\n",
    "    \n",
    "    histogram_ranges = []\n",
    "    training_histograms = {}\n",
    "    num_bins = 5\n",
    "\n",
    "    for idx in range(num_features):\n",
    "        width = (max_feature_values[idx] - min_feature_values[idx]) / num_bins\n",
    "        histogram_ranges.append(np.arange(min_feature_values[idx], max_feature_values[idx]+width*.99, width))\n",
    "    histogram_ranges = np.array(histogram_ranges)\n",
    "\n",
    "    for class_id in class_ids:\n",
    "        training_histograms[class_id] = np.histogramdd(training_datasets[class_id], bins=histogram_ranges, density=True)[0] \n",
    "        \n",
    "    return training_histograms, histogram_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(test_set, training_histograms, histogram_ranges, class_ids, frac=0.25):\n",
    "    \"\"\"\n",
    "    This function accepts a data set and histograms and classifies each pixel and assigns a score\n",
    "    \n",
    "    Inputs:\n",
    "        test_set: the test set of features which we would like to classify\n",
    "        training_histograms: a dictionary of histograms, one for each class\n",
    "        histogram_ranges: the bin cuttoffs for the histograms\n",
    "        class_ids: a set of class ids\n",
    "        frac: between 0 and 1. Higher values mean we require more confidence to classify a pixel as non-NaN\n",
    "        \n",
    "    Output:\n",
    "        an array of predicted classes and corresponding scores\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store the probabilities for each class\n",
    "    class_id_to_probs = {}\n",
    "    \n",
    "    #any probability density below this is considered as 0\n",
    "    min_allowable_density = min([frac*np.max(training_histograms[cid]) for cid in class_ids])\n",
    "    \n",
    "    #iterate over each histogram\n",
    "    for class_id, histogram in training_histograms.items():\n",
    "        \n",
    "        #get the position of each test pixel in the context of this histogram\n",
    "        transposed_ranges = np.transpose(histogram_ranges)\n",
    "        expanded_dims_ranges = np.expand_dims(transposed_ranges, axis=1)\n",
    "        extended_ranges = np.concatenate([expanded_dims_ranges for _ in range(test_set.shape[0])], axis=1)\n",
    "        diffs = extended_ranges - test_set\n",
    "    \n",
    "        #get the probability density at each position\n",
    "        indices = np.argmax(diffs > 0, axis=0) - 1\n",
    "        indices[indices < 0] = 0\n",
    "        indices = indices - np.all((diffs>0)==False, axis=0)\n",
    "        indices = tuple(np.transpose(indices))\n",
    "        probs = histogram[indices]\n",
    "        \n",
    "        #store this in the dictionary\n",
    "        class_id_to_probs[class_id] = probs\n",
    "      \n",
    "    #create matrix of probabilities for each class\n",
    "    prob_mtx = np.stack([class_id_to_probs[cid] for cid in class_ids], axis=-1)\n",
    "    \n",
    "    #sort matrix of probs\n",
    "    sorted_probs = np.sort(prob_mtx, axis=1)\n",
    "    \n",
    "    #any pixel where all classes have 0 probability is NaN\n",
    "    nan_indices = np.where(sorted_probs[:,-1] < min_allowable_density)[0]\n",
    "    \n",
    "    #compute scores based on ratio of most likely class to second most likely class\n",
    "    scores = 1/(1+np.exp(-(sorted_probs[:,-1] / sorted_probs[:,-2])))\n",
    "    \n",
    "    #apply NaN pixels\n",
    "    scores[nan_indices] = np.nan\n",
    "\n",
    "    #get the predicted class\n",
    "    pred_class = np.argmax(prob_mtx, axis=1).astype(float)\n",
    "    \n",
    "    #apply NaN pixels\n",
    "    pred_class[nan_indices] = np.nan\n",
    "    \n",
    "    return pred_class, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Array to TIFF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raster(output_path, columns, rows, nband=1, gdal_data_type=gdal.GDT_Int32, driver=r'GTiff'):\n",
    "    ''' \n",
    "    returns gdal data source raster object \n",
    "    '''\n",
    "    \n",
    "    # create driver\n",
    "    driver = gdal.GetDriverByName(driver)\n",
    "\n",
    "    output_raster = driver.Create(output_path, columns, rows, nband, eType = gdal_data_type)    \n",
    "    \n",
    "    return output_raster\n",
    "\n",
    "def np_array_to_raster(output_path, arr, band_names, geotransform, no_data=None, nband=1, gdal_data_type=gdal.GDT_Int32, spatial_reference_system_wkid=4326, driver=r'GTiff'):\n",
    "    ''' \n",
    "    returns a gdal raster data source\n",
    "\n",
    "    keyword arguments:\n",
    "\n",
    "    output_path -- full path to the raster to be written to disk\n",
    "    numpy_array -- numpy array containing data to write to raster\n",
    "    band_names -- the names of the bands. must be same length as last axis of numpy_array\n",
    "    upper_left_tuple -- the upper left point of the numpy array (should be a tuple structured as (x, y))\n",
    "    cell_resolution -- the cell resolution of the output raster\n",
    "    no_data -- value in numpy array that should be treated as no data\n",
    "    nband -- the band to write to in the output raster\n",
    "    gdal_data_type -- gdal data type of raster (see gdal documentation for list of values)\n",
    "    spatial_reference_system_wkid -- well known id (wkid) of the spatial reference of the data\n",
    "    driver -- string value of the gdal driver to use\n",
    "    '''\n",
    "\n",
    "    rows, columns = arr.shape[0], arr.shape[1]\n",
    "\n",
    "    # create output raster\n",
    "    output_raster = create_raster(output_path, columns, rows, nband, gdal_data_type) \n",
    "\n",
    "    spatial_reference = osr.SpatialReference()\n",
    "    spatial_reference.ImportFromEPSG(spatial_reference_system_wkid)\n",
    "    output_raster.SetProjection(spatial_reference.ExportToWkt())\n",
    "    output_raster.SetGeoTransform(geotransform)\n",
    "    \n",
    "    for band_idx in range(1,nband+1):\n",
    "        output_band = output_raster.GetRasterBand(band_idx)\n",
    "        output_band.SetDescription(band_names[band_idx-1])\n",
    "        if no_data != None:\n",
    "            output_band.SetNoDataValue(no_data)\n",
    "        if nband > 1:\n",
    "            output_band.WriteArray(arr[:,:,band_idx-1])\n",
    "        else:\n",
    "            output_band.WriteArray(arr)\n",
    "        output_band.FlushCache() \n",
    "        output_band.ComputeStatistics(False)\n",
    "\n",
    "    if os.path.exists(output_path) == False:\n",
    "        raise Exception('Failed to create raster: %s' % output_path)\n",
    "\n",
    "    return output_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Download ROI Polygon Data\n",
    "\n",
    "## This code will download data from Google Earth Engine according to the ROI polygons in the folder \"roi_polygons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "dataSource = driver.Open(ROI_POLYGONS_FILE, gdal.GA_ReadOnly)\n",
    "layer = dataSource.GetLayer()\n",
    "\n",
    "#get all roi polygons\n",
    "roi_polygon_features = [layer.GetNextFeature() for _ in range(layer.GetFeatureCount())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dictionary maps roi polygons to the corresponding raster file name\n",
    "roi_polygon_to_fname = {}\n",
    "\n",
    "existing_raster_names = os.listdir(ROI_DATA_FOLDER)\n",
    "\n",
    "#for all existing rasters...\n",
    "for fname in existing_raster_names:\n",
    "    fname = '%s/%s'%(ROI_DATA_FOLDER, fname)\n",
    "    \n",
    "    #get the raster geometry\n",
    "    raster_geometry = get_bounding_feature_around_raster(fname)\n",
    "    feature_defn = ogr.FeatureDefn()\n",
    "    raster_feat = ogr.Feature(feature_defn)\n",
    "    raster_feat.SetGeometry(raster_geometry)\n",
    "    \n",
    "    #if we chose ONLINE mode, then try to match roi polygons to exsting rasters\n",
    "    if ONLINE:\n",
    "        \n",
    "        #for each roi polygon...\n",
    "        for roi_polygon in roi_polygon_features:\n",
    "\n",
    "            #if this roi_polygon bounds the current raster\n",
    "            if roi_polygon.GetGeometryRef().Centroid().Within(raster_feat.GetGeometryRef()):\n",
    "                roi_polygon_to_fname[roi_polygon] = fname\n",
    "            \n",
    "    \n",
    "    #we failed to find any roi polygon which contains this raster\n",
    "    if fname not in roi_polygon_to_fname.values():\n",
    "        #generate a new roi polygon\n",
    "        roi_polygon_to_fname[raster_feat] = fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fnames = []\n",
    "if ONLINE:\n",
    "    roi_polygon_features_to_process = [p for p in roi_polygon_features if p not in roi_polygon_to_fname]\n",
    "    tasks = get_training_data(enumerate(roi_polygon_features_to_process), FEATURES, GDRIVE_FOLDER_NAME, DATE_RANGE, SELECTED_BANDS)\n",
    "    new_fnames = execute_tasks_in_batches(tasks, 3, ROI_DATA_FOLDER, 10)\n",
    "    for poly,fname in zip(roi_polygon_features_to_process, new_fnames):\n",
    "        roi_polygon_to_fname[poly] = fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all files in the roi data folder\n",
    "roi_feat_file_names = [item for item in os.listdir(ROI_DATA_FOLDER) if item[-4:] == 'tiff' or item[-3:] == 'tif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the newly gathered files\n",
    "process_feature_files([fname.split('/')[-1] for fname in new_fnames])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Get Suggested Number of Classes\n",
    "\n",
    "## This code will run K-Means on each ROI with different values for K. It will report the \"best\" number of clusters based on a separability metric. The results are stored graphically in the folder \"roi_suggested\" and can be viewed in a GIS editor like QGIS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_clusters_options = [6,7,8,9,10]\n",
    "\n",
    "for feat_file_name in roi_feat_file_names:\n",
    "    print('Processing %s'%feat_file_name)\n",
    "    get_sep_metric(feat_file_name, num_clusters_options)\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## At this point please pause and review the suggested number of clusters and create your training polygons on the map using a GIS editor like QGIS. A sample file of training polygons is stored in the folder \"training_polygons\" but you should create your own set of training polygons based on the ROI you chose in the beginning of this notebook. Make sure that each training polygon lives inside a ROI polygon and also ensure that each training polygon you draw has a:\n",
    "\n",
    "## *class_id* : integer in 0,1,2... indicating which class this training polygon belongs to\n",
    "\n",
    "## *confidence*: value between 0 and 1 indicating how confident you are about this training polygon belonging to the labeled class_id\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Extract Training Data\n",
    "\n",
    "## This code will extact data for the training polygons from the existing data for the ROI polygons. It uses this extracted data to create a dictionary mapping ROI polygon to class_id to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "dataSource = driver.Open(TRAINING_POLYGONS_FILE, gdal.GA_ReadOnly)\n",
    "layer = dataSource.GetLayer()\n",
    "training_polygon_features = [layer.GetNextFeature() for _ in range(layer.GetFeatureCount())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#this dictionary maps roi data file name to class_id to training data file name\n",
    "roi_to_class_id_to_training = {}\n",
    "\n",
    "#this dictionary maps roi data file name to data information\n",
    "roi_to_data = {}\n",
    "\n",
    "remove_temporary_files()\n",
    "\n",
    "#for each roi polygon...\n",
    "for roi_polygon, roi_fname in roi_polygon_to_fname.items():\n",
    "    print('Processing %s...'%roi_fname)\n",
    "    \n",
    "    roi_to_class_id_to_training[roi_fname] = {}\n",
    "    roi_to_data[roi_fname] = {}\n",
    "    \n",
    "    #get the dataset\n",
    "    ds_roi = gdal.Open(roi_fname, gdal.GA_ReadOnly)\n",
    "    roi_data = ds_roi.ReadAsArray()\n",
    "    roi_data = np.stack([roi_data[i] for i in range(roi_data.shape[0])], axis=-1)\n",
    "    \n",
    "    #mask out NaN or NO DATA values\n",
    "    mask = np.any(roi_data == NO_DATA_VALUE, axis=-1) | np.any(np.isnan(roi_data), axis=-1)\n",
    "    roi_data = roi_data[~mask]\n",
    "    \n",
    "    #map roi_fname to data and auxilary data information like geotransform, shape, etc.\n",
    "    roi_to_data[roi_fname]['data'] = roi_data\n",
    "    roi_to_data[roi_fname]['indices'] = np.where(~mask)\n",
    "    roi_to_data[roi_fname]['gt'] = ds_roi.GetGeoTransform()\n",
    "    roi_to_data[roi_fname]['shape'] = mask.shape\n",
    "    \n",
    "    #get list of training polygons inside this roi polygon\n",
    "    contained_training_polygons = [f for f in training_polygon_features if f.GetGeometryRef().Centroid().Within(roi_polygon.GetGeometryRef())]\n",
    "    \n",
    "    #get unique class ids within this roi polygon\n",
    "    class_ids = sorted(list(set([f.GetField('class_id') for f in contained_training_polygons])))\n",
    "    \n",
    "    #for each class id...\n",
    "    for cid in class_ids:\n",
    "        print('Processing class_id=%s'%cid)\n",
    "        \n",
    "        #get list of training polygons of this class\n",
    "        class_specific_polygons = [f for f in contained_training_polygons if f.GetField('class_id') == cid]\n",
    "        total_confidence = sum([f.GetField('confidence') for f in class_specific_polygons])\n",
    "        training_polygon_weights = [f.GetField('confidence') / total_confidence for f in class_specific_polygons]\n",
    "        \n",
    "        #initialize the training data for this specific roi polygon and class\n",
    "        class_specific_training_data = []\n",
    "        \n",
    "        #for each individual training polygon...\n",
    "        for fid, feature in enumerate(class_specific_polygons):\n",
    "        \n",
    "            #create a temporary shapefile with just this training polygon\n",
    "            temp_poly_name = 'temp_poly_%s_%s_%s.shp'%(''.join([c for c in roi_fname if c.isdigit() or c == '-']), cid, fid)\n",
    "            created_shp_file = driver.CreateDataSource(temp_poly_name)\n",
    "            outLayer = created_shp_file.CreateLayer('tmp')\n",
    "            outLayer.CreateFeature(feature)\n",
    "            created_shp_file.Destroy()\n",
    "\n",
    "            #get the training data in these polygons\n",
    "            temp_raster_name = 'temp_raster_%s_%s_%s.tiff'%(''.join([c for c in roi_fname if c.isdigit() or c == '-']), cid, fid)\n",
    "            options = gdal.WarpOptions(format='GTiff', cutlineDSName=temp_poly_name, srcNodata=NO_DATA_VALUE)\n",
    "            ds_result = gdal.Warp(destNameOrDestDS=temp_raster_name, srcDSOrSrcDSTab=ds_roi, options=options)\n",
    "        \n",
    "            #extract training data as array\n",
    "            training_data = ds_result.ReadAsArray()\n",
    "            training_data = np.stack([training_data[i] for i in range(training_data.shape[0])], axis=-1)\n",
    "            mask = np.any(training_data == NO_DATA_VALUE, axis=-1) | np.any(np.isnan(training_data), axis=-1)\n",
    "            training_data = training_data[~mask]\n",
    "            ds_result = None\n",
    "        \n",
    "            #add this training data to the list\n",
    "            class_specific_training_data.append(training_data)\n",
    "            \n",
    "            print('Processed feature %s: %s samples'%(fid, training_data.shape[0]))\n",
    "            \n",
    "        \n",
    "        #get the total number of pixels in all training sets\n",
    "        tot_pixels = sum([ds.shape[0] for ds in class_specific_training_data])\n",
    "        \n",
    "        #get number of pixels to sample by considering confidence of each polygon\n",
    "        pixels_to_sample = [int(tot_pixels*w) for w in training_polygon_weights]\n",
    "        \n",
    "        #sample training data by confidence\n",
    "        class_specific_training_data = [ds[np.random.choice(ds.shape[0], p)] for ds,p in zip(class_specific_training_data, pixels_to_sample) if ds.shape[0] > 0]\n",
    "        class_specific_training_data = np.concatenate(class_specific_training_data, axis=0)\n",
    "        \n",
    "        #run a K-means with 2 clusters in case user accidentially included some noise \n",
    "        try:\n",
    "            model = KMeans(n_clusters=2)\n",
    "            cluster_preds = model.fit_predict(class_specific_training_data)\n",
    "        \n",
    "            #only filter out data if there is a distinctly small second cluster\n",
    "            if abs(np.mean(cluster_preds) - 0.5) > 0.25:\n",
    "                main_cluster = np.median(cluster_preds)\n",
    "                class_specific_training_data = class_specific_training_data[cluster_preds == main_cluster]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        #store combined training data for this roi and class_id\n",
    "        if class_specific_training_data.shape[0] == 0:\n",
    "            del roi_to_class_id_to_training[roi_fname]\n",
    "            print('No data gathered for %s'%roi_fname)\n",
    "            break\n",
    "            \n",
    "        roi_to_class_id_to_training[roi_fname][cid] = class_specific_training_data\n",
    "        print('constructed dataset of size %s'%str(class_specific_training_data.shape))\n",
    "        \n",
    "    ds_roi = None\n",
    "    print('--------------------')\n",
    "    \n",
    "remove_temporary_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Classify ROI Areas\n",
    "\n",
    "## this code builds the model (histogram or random_forest) to predict the ROI areas and then performs the prediction, storing the results in the folder \"roi_predicted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty histograms and histogram ranges dictionaries\n",
    "histograms_dict = {}\n",
    "histogram_ranges_dict = {}\n",
    "\n",
    "random_forest_clf_dict = {}\n",
    "\n",
    "for roi_fname in roi_to_class_id_to_training:\n",
    "    \n",
    "    class_ids = sorted([cid for cid in roi_to_class_id_to_training[roi_fname].keys() if type(cid) == int])\n",
    "    \n",
    "    #create histograms and histogram ranges for this roi polygon\n",
    "    histograms, histogram_ranges = create_histograms(roi_to_class_id_to_training[roi_fname])\n",
    "    histograms_dict[roi_fname] = histograms\n",
    "    histogram_ranges_dict[roi_fname] = histogram_ranges\n",
    "    \n",
    "    #store constructed datasets\n",
    "    features = np.concatenate([roi_to_class_id_to_training[roi_fname][cid] for cid in class_ids], axis=0)\n",
    "    labels = np.array([single for item in [[cid]*len(roi_to_class_id_to_training[roi_fname][idx]) for idx,cid in enumerate(class_ids)] for single in item])\n",
    "    \n",
    "    #fit a random forest classifier\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(features, labels)\n",
    "    random_forest_clf_dict[roi_fname] = clf\n",
    "    \n",
    "    print('Processed %s'%roi_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if METHOD == 'histogram':\n",
    "    #get the predicted classes and scores for histogram method\n",
    "    pred_class_scores = {fname: get_classes(d['data'], histograms_dict[fname], histogram_ranges_dict[fname], sorted(list(histograms_dict[fname].keys())), 0) for fname,d in roi_to_data.items()}\n",
    "elif METHOD == 'random_forest':\n",
    "    #get the predicted classes and scores for random forest method\n",
    "    pred_class_scores = {fname: [random_forest_clf_dict[fname].predict(d['data']), np.sort(random_forest_clf_dict[fname].predict_proba(d['data']), axis=1)[:,-2:]] for fname,d in roi_to_data.items()}\n",
    "    scores = {fname: 2/(1+np.exp(1-item[1][:,-1] / item[1][:,-2]))-1 for fname, item in pred_class_scores.items()}\n",
    "    pred_class_scores = {fname: [item[0], scores[fname]] for fname, item in pred_class_scores.items()}\n",
    "    \n",
    "print('Got Predicted Classes and Scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for each roi file...\n",
    "for idx, feat_file_name in enumerate(roi_feat_file_names):\n",
    "    \n",
    "    feat_file_name = '%s/%s'%(ROI_DATA_FOLDER, feat_file_name)\n",
    "    \n",
    "    #get the shape of the file\n",
    "    feat_file_shape = roi_to_data[feat_file_name]['shape']\n",
    "    shape = feat_file_shape + (2,)\n",
    "    \n",
    "    #create empty matrix to store result\n",
    "    result = np.empty(shape)\n",
    "    result[:] = np.nan\n",
    "    \n",
    "    #these are the valid indices \n",
    "    indices = roi_to_data[feat_file_name]['indices']\n",
    "\n",
    "    #load the predicted classes and scores\n",
    "    result[indices] = np.transpose(pred_class_scores[feat_file_name])\n",
    "    \n",
    "    result[:,:,0] = apply_majority_filter(result[:,:,0], 5)\n",
    "\n",
    "    preds_fname = '%s/%s_predicted_%s.tiff'%(ROI_PREDICTED_FOLDER, METHOD, idx)\n",
    "    ds = np_array_to_raster(preds_fname, result, ['class_id', 'confidence'], roi_to_data[feat_file_name]['gt'], no_data=-1, nband=2, gdal_data_type=gdal.GDT_Float64)\n",
    "    ds = None\n",
    "    print('saved %s'%preds_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
