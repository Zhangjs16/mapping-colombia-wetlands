{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to get Sentinel-2 and ALOS PALSAR data from Google Earth Engine according to parameters (region of interest, time range, etc.) specified by the user. Upon success, TIFF files for small pieces of the desired area of interest will be stored on the user's Google Drive, and then downloaded locally in batches. The downloaded rasters will be used in the next notebook: (2) CacheTrainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from shapely.geometry import box\n",
    "import geopandas as gpd\n",
    "import ee\n",
    "import pickle\n",
    "import requests\n",
    "import os\n",
    "from osgeo import gdal, ogr\n",
    "from common_functions import *\n",
    "import shutil\n",
    "from time import sleep\n",
    "from math import ceil\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is about 10 meters\n",
    "METERS_TO_DECIMAL_DEGREES_CONST = 1/30/3600\n",
    "\n",
    "#the value we use to signify no data at a pixel\n",
    "NO_DATA_VALUE = 65535\n",
    "\n",
    "#this is the biggest region we allow to avoid data overflow errors and keep files manageable\n",
    "MAX_REGION_SIZE = 0.5\n",
    "\n",
    "#the base region of interest folder\n",
    "BASE_ROI_FOLDER = 'regions'\n",
    "if BASE_ROI_FOLDER not in os.listdir():\n",
    "    os.mkdir(BASE_ROI_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input Area"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=== SET THIS: this is the path to the baseline CIFOR wetland, stored as cifor_wetlands_colombia.tif on Google Drive ===\n",
    "Available at this link: https://drive.google.com/file/d/1GHmrHCjlvCJecxAGdkp8MG02i6aptyft/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_WETLAND_PATH = 'C://Users/ritvik/Desktop/JPLProject/cifor_wetlands_colombia.tif'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=== SET THIS: this is the path to the area of interest shapefile ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AREA_OF_INTEREST_FILE = 'area_of_interest/mangrove.shp'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=== SET THIS: the Google Drive folder id where all data is stored ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_EARTH_ENGINE_GDRIVE_FOLDER_ID = '1KvlrUHs_rN7xPlw53qtd9pweeLwmrJSP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Drive Access Instructions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will be using the Google Drive Python API, whose setup steps are documented in the following link: \n",
    "\n",
    "https://developers.google.com/drive/api/v3/quickstart/python\n",
    "\n",
    "The only change you should make is to the \"SCOPES\" variable which should be set as:\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "You can confirm success by checking to make sure a file called token.pickle has been generated in your working directory. This file allows your Python code to interact with your google drive. Keep this file on your local machine and do not distribute."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=== SET THIS: any pixels above this elevation (meters) will be disregarded ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONSIDERED_ELEVATION = 6"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=== SET THIS: bands used for prediction ===\n",
    "\n",
    "You may choose from: \n",
    "\n",
    "Sentinel-2: 'B2', 'B3', 'B4, 'B8', 'NDVI', 'NDWI'\n",
    "\n",
    "ALOS/PALSAR: 'HH', 'HV'\n",
    "\n",
    "*Note that the more bands used, the longer the prediction process will take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_BANDS = ['B2', 'NDVI', 'HH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Download File from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_google_drive(file_id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : file_id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : file_id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_ids_from_google_drive():\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    result = service.files().list(q=\"parents in '%s'\"%GOOGLE_EARTH_ENGINE_GDRIVE_FOLDER_ID).execute()\n",
    "\n",
    "    folder_name_to_file_id = {info['name'].split('-')[0]: info['id'] for info in result['files'] if len(info['name'].split('-')) == 2}\n",
    "    \n",
    "    return folder_name_to_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file_from_google_drive_by_file_id(fid):\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    \n",
    "    service.files().delete(fileId=fid).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_raster_by_shapefile(source_raster, shapefile_path, save_path):\n",
    "    \"\"\"\n",
    "    Given some raster, this function clips the raster gien the shape of another raster,\n",
    "    \n",
    "    source_raster: a TIFF that you wish to crop\n",
    "    shapefile_path: a SHP whose bounds you will use to crop the source_raster\n",
    "    save_path: the eventual place to save the cropped TIFF\n",
    "    \"\"\"\n",
    "    source_ds = gdal.Open(source_raster, gdal.GA_ReadOnly)\n",
    "    \n",
    "    options = gdal.WarpOptions(format='GTiff', cutlineDSName=shapefile_path, cropToCutline=True)\n",
    "    ds = gdal.Warp(save_path, source_ds, options=options)\n",
    "    \n",
    "    ds = None\n",
    "    source_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_region_folders(minx, miny, maxx, maxy, path):\n",
    "    \"\"\"\n",
    "    Using the given extent, create a folder with shapefile info at the given path\n",
    "    \n",
    "    minx, miny, maxx, maxy: the extent of the region we wish to analyze\n",
    "    path: the directory where to store all the sub-reion subdirectories\n",
    "    \"\"\"\n",
    "    \n",
    "    diff_x = (maxx - minx)\n",
    "    diff_y = (maxy - miny)\n",
    "    \n",
    "    num_x = int(diff_x // MAX_REGION_SIZE + 1)\n",
    "    num_y = int(diff_y // MAX_REGION_SIZE + 1)\n",
    "    \n",
    "    size_x = diff_x / num_x\n",
    "    size_y = diff_y / num_y\n",
    "    \n",
    "    print(num_x, num_y, size_x, size_y)\n",
    "    \n",
    "    for i in range(num_x):\n",
    "        for j in range(num_y):\n",
    "            geo_box = box(minx+i*size_x, miny+j*size_y, minx+(i+1)*size_x, miny+(j+1)*size_y)\n",
    "            df = gpd.GeoDataFrame(geometry=[geo_box], crs={'init':'epsg:4326'})\n",
    "            df.to_file('%s_%s_%s'%(path, i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bands_from_region(folders_to_process, features, gdrive_folder, date_range, primary_dataset, wetland_code, selected_bands):\n",
    "    \"\"\"\n",
    "    This function accepts the below parameters and querys Google Earth Engine for data. The data is stored in \n",
    "    Google Drive.\n",
    "    \n",
    "    folders_to_process: the folders where to find the regions of interest\n",
    "    features: a dictionary of features to include in the resulting data cubes\n",
    "    gdrive_folder: the name of the folder on Google Drive to store the results\n",
    "    date_range: the date range for this data\n",
    "    primary_dataset: the dataset to use for the eventual image resolution\n",
    "    wetland_code: the code for the sub-type of wetland we will analyze\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store all started tasks\n",
    "    tasks = {}\n",
    "    \n",
    "    #work through each sub-region \n",
    "    for region_folder in folders_to_process:\n",
    "        \n",
    "        filtered_imgs = []\n",
    "        region_name = region_folder.split('/')[-1]\n",
    "        \n",
    "        print('Working on region folder: %s...'%region_name)\n",
    "        \n",
    "        print('Created Baseline Wetlands Raster...')\n",
    "        #clip the baseline map of wetlands and store in sub-directory\n",
    "        print('%s/%s.shp'%(region_folder, region_name))\n",
    "        baseline_file_name = '%s/baseline_%s.tiff'%(region_folder, region_name)\n",
    "        clip_raster_by_shapefile(BASELINE_WETLAND_PATH, '%s/%s.shp'%(region_folder, region_name), baseline_file_name)\n",
    "        ds = gdal.Open(baseline_file_name, gdal.GA_ReadOnly)\n",
    "        arr = ds.ReadAsArray()\n",
    "        ds = None\n",
    "        pct_wetland = np.mean(arr == wetland_code)\n",
    "        print(pct_wetland)\n",
    "        if pct_wetland < 0.01:\n",
    "            print('Deleting Region Folder')\n",
    "            print('==================================')\n",
    "            shutil.rmtree(region_folder)\n",
    "            continue\n",
    "        \n",
    "    \n",
    "        #read the area of interest\n",
    "        df = gpd.read_file(region_folder)\n",
    "\n",
    "        #get the coordinates of that area\n",
    "        area_coords = df.geometry[0].exterior.coords[:]\n",
    "        area_coords = [list(pair) for pair in area_coords]\n",
    "\n",
    "        #get the minx, miny, maxx, maxy\n",
    "        x1 = min([item[0] for item in area_coords])\n",
    "        y1 = max([item[1] for item in area_coords])\n",
    "\n",
    "        x2 = max([item[0] for item in area_coords])\n",
    "        y2 = min([item[1] for item in area_coords])\n",
    "\n",
    "        #store the reference coordinates\n",
    "        ref_coords = (x1,y1)\n",
    "\n",
    "        #create an area of interest from Earth Engine Geometry\n",
    "        area_of_interest = ee.Geometry.Polygon(coords=area_coords)\n",
    "\n",
    "        #iterate over each data source\n",
    "        for data_type_source, bands in features.items():\n",
    "            data_type = data_type_source[0]\n",
    "            data_source = data_type_source[1]\n",
    "                \n",
    "            print('Working on data source: %s...'%data_source)\n",
    "            \n",
    "            if data_type == 'collection':\n",
    "                #access the Earth Engine image collection with the specified bands\n",
    "                data = ee.ImageCollection(data_source).select(bands)\n",
    "\n",
    "                #filter on date range\n",
    "                data_filtered = data.filterBounds(area_of_interest).filterDate(date_range[0], date_range[1])\n",
    "\n",
    "                #ensure there is at least 1 image\n",
    "                num_items = data_filtered.size().getInfo()\n",
    "                if num_items == 0:\n",
    "                    print('no items found, returning started tasks.')\n",
    "                    return tasks\n",
    "\n",
    "                band_info = data_filtered.first().getInfo()['bands'][0]\n",
    "\n",
    "                #if crs is already EPSG 4326, get resolution directly, otherwise need to transform from meters\n",
    "                if band_info['crs'] == 'EPSG:4326':\n",
    "                    res = band_info['crs_transform'][0]\n",
    "                else:\n",
    "                    res = band_info['crs_transform'][0] * METERS_TO_DECIMAL_DEGREES_CONST\n",
    "\n",
    "                #if this is the eventual primary dataset, store its resolution\n",
    "                if data_source == primary_dataset:\n",
    "                    eventual_res = res\n",
    "\n",
    "                #get a mosaic as median of all returned images\n",
    "                mosaic = ee.Image(data_filtered.median())\n",
    "\n",
    "                if data_source == 'COPERNICUS/S2_SR':\n",
    "\n",
    "                    #calculate NDVI\n",
    "                    if 'B4' in features[('collection','COPERNICUS/S2_SR')] and 'B8' in features[('collection','COPERNICUS/S2_SR')]:\n",
    "                        ndvi = mosaic.normalizedDifference(['B4', 'B8']).rename('NDVI')\n",
    "                        mosaic = ee.Image.addBands(mosaic, ndvi)\n",
    "\n",
    "                    #calculate NDWI\n",
    "                    if 'B3' in features[('collection','COPERNICUS/S2_SR')] and 'B8' in features[('collection','COPERNICUS/S2_SR')]:\n",
    "                        ndwi = mosaic.normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
    "                        mosaic = ee.Image.addBands(mosaic, ndwi)\n",
    "                        \n",
    "            elif data_type == 'image':\n",
    "                mosaic = ee.Image(data_source).select(bands)\n",
    "\n",
    "            #add this mosaic to the list\n",
    "            filtered_imgs.append(mosaic)\n",
    "            \n",
    "        \n",
    "        #generate file name\n",
    "        features_str = '_'.join([item[1] for item in features.keys()]).replace('/','_')\n",
    "        fname = '%s-%s'%(region_folder.split('/')[-1], features_str)\n",
    "        print(fname)\n",
    "        \n",
    "        #add the various layers on top of each other to create a data cube with all features\n",
    "        final_img = ee.Image()\n",
    "        \n",
    "        for img in filtered_imgs:\n",
    "            final_img = ee.Image.addBands(final_img,img)\n",
    "        \n",
    "        #use the ALOS qa band to filter out invalid pixels\n",
    "        if 'qa' in features[('collection','JAXA/ALOS/PALSAR/YEARLY/SAR')]:\n",
    "            qa_band = final_img.select('qa')\n",
    "            qa_mask = qa_band.lt(51)\n",
    "            final_img = final_img.where(qa_mask, NO_DATA_VALUE)\n",
    "        \n",
    "        #use the Sentinel-2 SCL band to filter out invalid pixels\n",
    "        if 'SCL' in features[('collection','COPERNICUS/S2_SR')]:\n",
    "            scl_band = final_img.select('SCL')\n",
    "            scl_nodata_vals = [0,3,6,8,9,10]\n",
    "            scl_mask = scl_band.eq(0)\n",
    "            for v in scl_nodata_vals:\n",
    "                scl_mask = scl_mask.Or(scl_band.eq(v))\n",
    "            final_img = final_img.where(scl_mask, NO_DATA_VALUE)\n",
    "            \n",
    "        #use the SRTM elevationband to filter out invaild pixels\n",
    "        if 'elevation' in features['image','CGIAR/SRTM90_V4']:\n",
    "            elevation_band = final_img.select('elevation')\n",
    "            elevation_mask = elevation_band.gt(MAX_CONSIDERED_ELEVATION)\n",
    "            final_img = final_img.where(elevation_mask, NO_DATA_VALUE)\n",
    "            \n",
    "        #if any of the selected bands has NO_DATA_VALUE, mark that whole pixel as NO_DATA_VALUE\n",
    "        for b in selected_bands:\n",
    "            b_values = final_img.select(b)\n",
    "            b_mask = b_values.eq(NO_DATA_VALUE)\n",
    "            final_img = final_img.where(b_mask, NO_DATA_VALUE)\n",
    "         \n",
    "        #store the result with just the needed bands\n",
    "        selected_bands = sorted(selected_bands)\n",
    "        result = final_img.select(*selected_bands).float()\n",
    "          \n",
    "        #define the task to gather the data\n",
    "        task = ee.batch.Export.image.toDrive(image=result,\n",
    "                                             region=area_of_interest.getInfo()['coordinates'],\n",
    "                                             description=region_folder.split('/')[-1],\n",
    "                                             folder=gdrive_folder,\n",
    "                                             fileNamePrefix=fname,\n",
    "                                             crs_transform=[eventual_res, 0.0, ref_coords[0], 0.0, -eventual_res, ref_coords[1]],\n",
    "                                             crs='EPSG:4326')\n",
    "        \n",
    "        #store the task\n",
    "        tasks[fname] = task\n",
    "        \n",
    "        print('==================================')\n",
    "    \n",
    "    return list(tasks.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 0.3342691523898793 0.32751624022038595\n",
      "1 1 0.47347826836990237 0.339933115752757\n",
      "1 3 0.3035117104935239 0.48157191398307003\n",
      "2 2 0.45526756574030003 0.26102007102443725\n"
     ]
    }
   ],
   "source": [
    "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "\n",
    "dataSource = driver.Open(AREA_OF_INTEREST_FILE, gdal.GA_ReadOnly)\n",
    "\n",
    "layer = dataSource.GetLayer()\n",
    "\n",
    "wetland_type = layer.GetDescription().replace('/','').replace(' ','').lower()\n",
    "wetland_type_file = open('wetland_type.txt', 'w')\n",
    "wetland_type_file.write(wetland_type)\n",
    "wetland_type_file.close()\n",
    "\n",
    "wetland_code = feature_to_code[wetland_type]\n",
    "\n",
    "curr_feature = layer.GetNextFeature()\n",
    "curr_subregion_idx = 0\n",
    "\n",
    "while curr_feature != None:\n",
    "    minx, maxx, miny, maxy = curr_feature.GetGeometryRef().GetEnvelope()\n",
    "    generate_region_folders(minx, miny, maxx, maxy, '%s/region_%s'%(BASE_ROI_FOLDER, curr_subregion_idx))\n",
    "    \n",
    "    curr_feature = layer.GetNextFeature()\n",
    "    curr_subregion_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_area_name = 'region'\n",
    "folders_to_process = ['%s/%s'%(BASE_ROI_FOLDER, item) for item in os.listdir(BASE_ROI_FOLDER) if search_area_name in item]\n",
    "features = {('collection','JAXA/ALOS/PALSAR/YEARLY/SAR'): ['HH', 'HV', 'qa'], ('collection', 'COPERNICUS/S2_SR'): ['B2', 'B3', 'B4', 'B8', 'SCL'], ('image','CGIAR/SRTM90_V4'): ['elevation']}\n",
    "date_range = ['2017-01-01', '2019-01-01']\n",
    "gdrive_folder = 'GoogleEarthEngine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on region folder: region_0_0_0...\n",
      "Created Baseline Wetlands Raster...\n",
      "regions/region_0_0_0/region_0_0_0.shp\n",
      "0.10341269841269841\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "region_0_0_0-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on region folder: region_0_0_1...\n",
      "Created Baseline Wetlands Raster...\n",
      "regions/region_0_0_1/region_0_0_1.shp\n",
      "0.1104421768707483\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "region_0_0_1-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on region folder: region_0_1_0...\n",
      "Created Baseline Wetlands Raster...\n",
      "regions/region_0_1_0/region_0_1_0.shp\n",
      "0.05307256235827664\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "region_0_1_0-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on region folder: region_0_1_1...\n",
      "Created Baseline Wetlands Raster...\n",
      "regions/region_0_1_1/region_0_1_1.shp\n",
      "0.03875283446712018\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "region_0_1_1-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on region folder: region_1_0_0...\n",
      "Created Baseline Wetlands Raster...\n",
      "regions/region_1_0_0/region_1_0_0.shp\n",
      "0.10773384763741563\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "region_1_0_0-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on region folder: region_2_0_0...\n",
      "Created Baseline Wetlands Raster...\n",
      "regions/region_2_0_0/region_2_0_0.shp\n",
      "0.02589699074074074\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "region_2_0_0-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on region folder: region_2_0_1...\n",
      "Created Baseline Wetlands Raster...\n",
      "regions/region_2_0_1/region_2_0_1.shp\n",
      "0.03855187908496732\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "region_2_0_1-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on region folder: region_2_0_2...\n",
      "Created Baseline Wetlands Raster...\n",
      "regions/region_2_0_2/region_2_0_2.shp\n",
      "0.04491761982570806\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "region_2_0_2-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on region folder: region_3_0_0...\n",
      "Created Baseline Wetlands Raster...\n",
      "regions/region_3_0_0/region_3_0_0.shp\n",
      "0.18358305644369213\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "region_3_0_0-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on region folder: region_3_0_1...\n",
      "Created Baseline Wetlands Raster...\n",
      "regions/region_3_0_1/region_3_0_1.shp\n",
      "0.0630054542034982\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "region_3_0_1-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on region folder: region_3_1_0...\n",
      "Created Baseline Wetlands Raster...\n",
      "regions/region_3_1_0/region_3_1_0.shp\n",
      "0.10201032328171693\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "region_3_1_0-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n",
      "Working on region folder: region_3_1_1...\n",
      "Created Baseline Wetlands Raster...\n",
      "regions/region_3_1_1/region_3_1_1.shp\n",
      "0.1649426368252774\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "Working on data source: CGIAR/SRTM90_V4...\n",
      "region_3_1_1-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR_CGIAR_SRTM90_V4\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "tasks = get_bands_from_region(folders_to_process, features, gdrive_folder, date_range, 'COPERNICUS/S2_SR', wetland_code, SELECTED_BANDS)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After launching tasks, you can periodically run the below cell to check the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch 1\n",
      "Started all tasks in batch\n",
      "Current states: ['RUNNING', 'RUNNING', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['RUNNING', 'RUNNING', 'RUNNING', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'RUNNING', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'RUNNING', 'READY', 'READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'RUNNING', 'READY', 'READY', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'RUNNING', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'RUNNING', 'READY', 'READY', 'READY']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING', 'RUNNING', 'RUNNING']\n",
      "Current states: ['COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'COMPLETED', 'RUNNING']\n",
      "Downloading region_3_1_1 from Drive\n",
      "Deleting region_3_1_1 from Drive\n",
      "Downloading region_3_0_1 from Drive\n",
      "Deleting region_3_0_1 from Drive\n",
      "Downloading region_3_1_0 from Drive\n",
      "Deleting region_3_1_0 from Drive\n",
      "Downloading region_2_0_2 from Drive\n",
      "Deleting region_2_0_2 from Drive\n",
      "Downloading region_3_0_0 from Drive\n",
      "Deleting region_3_0_0 from Drive\n",
      "Downloading region_2_0_1 from Drive\n",
      "Deleting region_2_0_1 from Drive\n",
      "Downloading region_2_0_0 from Drive\n",
      "Deleting region_2_0_0 from Drive\n",
      "Downloading region_1_0_0 from Drive\n",
      "Deleting region_1_0_0 from Drive\n",
      "Downloading region_0_1_1 from Drive\n",
      "Deleting region_0_1_1 from Drive\n",
      "Downloading region_0_0_0 from Drive\n",
      "Deleting region_0_0_0 from Drive\n",
      "Downloading region_0_1_0 from Drive\n",
      "Deleting region_0_1_0 from Drive\n",
      "Downloading region_0_0_1 from Drive\n",
      "Deleting region_0_0_1 from Drive\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "\n",
    "#process the tasks in small batches to avoid memory running out\n",
    "for batch_idx in range(ceil(len(tasks) / batch_size)):\n",
    "    \n",
    "    #get the current batch of tasks\n",
    "    curr_tasks = tasks[batch_size*batch_idx:batch_size*(batch_idx+1)]\n",
    "    print('Processing Batch %s'%(batch_idx+1))\n",
    "\n",
    "    #start all tasks in that batch\n",
    "    for name,task in curr_tasks:\n",
    "        task.start()\n",
    "        \n",
    "    print('Started all tasks in batch')\n",
    "     \n",
    "    #wait until all tasks in that batch are done, wait 1 minute between checks\n",
    "    curr_states = [task.status()['state'] for name,task in curr_tasks]\n",
    "    while set(curr_states) != {'COMPLETED'}:\n",
    "        print('Current states: %s'%curr_states)\n",
    "        sleep(120)\n",
    "        curr_states = [task.status()['state'] for name,task in curr_tasks]\n",
    "      \n",
    "    #once all tasks done, get their file ids on google drive\n",
    "    folder_name_to_file_id = get_file_ids_from_google_drive()\n",
    "    \n",
    "    #for each file...\n",
    "    for roi_folder, fid in folder_name_to_file_id.items():\n",
    "    \n",
    "        #get feature file name\n",
    "        features_file_name = '%s/%s/features_%s.tiff'%(BASE_ROI_FOLDER, roi_folder, roi_folder)\n",
    "        \n",
    "        #check if data already downloaded\n",
    "        if features_file_name.split('/')[-1] not in os.listdir('%s/%s'%(BASE_ROI_FOLDER, roi_folder)):\n",
    "            print('Downloading %s from Drive'%roi_folder)\n",
    "            download_file_from_google_drive(fid, features_file_name)\n",
    "         \n",
    "        print('Deleting %s from Drive'%roi_folder)\n",
    "        delete_file_from_google_drive_by_file_id(folder_name_to_file_id[roi_folder])\n",
    "    \n",
    "    print('================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
