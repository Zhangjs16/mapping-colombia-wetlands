{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to get Sentinel-2 and ALOS PALSAR data from Google Earth Engine according to parameters (region of interest, time range, etc.) specified by the user. Upon success, TIFF files for small pieces of the desired area of interest will be stored on the user's Google Drive, ready to be used in the next notebook: (2) CacheTrainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from shapely.geometry import box\n",
    "import geopandas as gpd\n",
    "import ee\n",
    "import os\n",
    "from osgeo import gdal\n",
    "\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is about 10 meters\n",
    "METERS_TO_DECIMAL_DEGREES_CONST = 1/30/3600\n",
    "\n",
    "#the value we use to signify no data at a pixel\n",
    "NO_DATA_VALUE = 65535\n",
    "\n",
    "#this is the biggest region we allow to avoid data overflow errors and keep files manageable\n",
    "MAX_REGION_SIZE = 0.5\n",
    "\n",
    "#the base region of interest folder\n",
    "BASE_ROI_FOLDER = 'regions'\n",
    "if BASE_ROI_FOLDER not in os.listdir():\n",
    "    os.mkdir(BASE_ROI_FOLDER)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=== SET THIS: this is the path to the baseline CIFOR wetland, stored as cifor_wetlands_colombia.tif on Google Drive ===\n",
    "Available at this link: https://drive.google.com/file/d/1GHmrHCjlvCJecxAGdkp8MG02i6aptyft/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_WETLAND_PATH = 'C://Users/ritvik/Desktop/JPLProject/data/CIFORWetlands/cifor_wetlands_colombia.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_raster_by_shapefile(source_raster, shapefile_path, save_path):\n",
    "    \"\"\"\n",
    "    Given some raster, this function clips the raster gien the shape of another raster,\n",
    "    \n",
    "    source_raster: a TIFF that you wish to crop\n",
    "    shapefile_path: a SHP whose bounds you will use to crop the source_raster\n",
    "    save_path: the eventual place to save the cropped TIFF\n",
    "    \"\"\"\n",
    "    source_ds = gdal.Open(source_raster, gdal.GA_ReadOnly)\n",
    "    \n",
    "    options = gdal.WarpOptions(format='GTiff', cutlineDSName=shapefile_path, cropToCutline=True)\n",
    "    ds = gdal.Warp(save_path, source_ds, options=options)\n",
    "    \n",
    "    ds = None\n",
    "    source_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_region_folders(minx, miny, maxx, maxy, path):\n",
    "    \"\"\"\n",
    "    Using the given extent, create a folder with shapefile info at the given path\n",
    "    \n",
    "    minx, miny, maxx, maxy: the extent of the region we wish to analyze\n",
    "    path: the directory where to store all the sub-reion subdirectories\n",
    "    \"\"\"\n",
    "    \n",
    "    diff_x = (maxx - minx)\n",
    "    diff_y = (maxy - miny)\n",
    "    \n",
    "    num_x = int(diff_x // MAX_REGION_SIZE + 1)\n",
    "    num_y = int(diff_y // MAX_REGION_SIZE + 1)\n",
    "    \n",
    "    size_x = diff_x / num_x\n",
    "    size_y = diff_y / num_y\n",
    "    \n",
    "    print(num_x, num_y, size_x, size_y)\n",
    "    \n",
    "    for i in range(num_x):\n",
    "        for j in range(num_y):\n",
    "            geo_box = box(minx+i*size_x, miny+j*size_y, minx+(i+1)*size_x, miny+(j+1)*size_y)\n",
    "            df = gpd.GeoDataFrame(geometry=[geo_box], crs={'init':'epsg:4326'})\n",
    "            df.to_file('%s_%s_%s'%(path, i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bands_from_region(folders_to_process, features, gdrive_folder, date_range, aux_data, primary_dataset):\n",
    "    \"\"\"\n",
    "    This function accepts the below parameters and querys Google Earth Engine for data. The data is stored in \n",
    "    Google Drive.\n",
    "    \n",
    "    folders_to_process: the folders where to find the regions of interest\n",
    "    features: a dictionary of features to include in the resulting data cubes\n",
    "    gdrive_folder: the name of the folder on Google Drive to store the results\n",
    "    date_range: the date range for this data\n",
    "    aux_data: dictionary of source-specific info such as cloudy pixel percentage for Sentinel, etc.\n",
    "    primary_dataset: the dataset to use for the eventual image resolution\n",
    "    \"\"\"\n",
    "    \n",
    "    #this will store all started tasks\n",
    "    tasks = {}\n",
    "    \n",
    "    #work through each sub-region \n",
    "    for region_folder in folders_to_process:\n",
    "        \n",
    "        filtered_imgs = []\n",
    "        region_name = region_folder.split('/')[-1]\n",
    "        \n",
    "        print('Working on region folder: %s...'%region_name)\n",
    "        \n",
    "        print('Created Baseline Wetlands Raster...')\n",
    "        #clip the baseline map of wetlands and store in sub-directory\n",
    "        clip_raster_by_shapefile(BASELINE_WETLAND_PATH, '%s/%s.shp'%(region_folder, region_name), '%s/baseline_%s.tiff'%(region_folder, region_name))\n",
    "    \n",
    "        #read the area of interest\n",
    "        df = gpd.read_file(region_folder)\n",
    "\n",
    "        #get the coordinates of that area\n",
    "        area_coords = df.geometry[0].exterior.coords[:]\n",
    "        area_coords = [list(pair) for pair in area_coords]\n",
    "\n",
    "        #get the minx, miny, maxx, maxy\n",
    "        x1 = min([item[0] for item in area_coords])\n",
    "        y1 = max([item[1] for item in area_coords])\n",
    "\n",
    "        x2 = max([item[0] for item in area_coords])\n",
    "        y2 = min([item[1] for item in area_coords])\n",
    "\n",
    "        #store the reference coordinates\n",
    "        ref_coords = (x1,y1)\n",
    "\n",
    "        #create an area of interest from Earth Engine Geometry\n",
    "        area_of_interest = ee.Geometry.Polygon(coords=area_coords)\n",
    "\n",
    "        #iterate over each data source\n",
    "        for data_source, bands in features.items():\n",
    "                \n",
    "            print('Working on data source: %s...'%data_source)\n",
    "            \n",
    "            #access the Earth Engine image collection with the specified bands\n",
    "            data = ee.ImageCollection(data_source).select(bands)\n",
    "\n",
    "            #filter on date range\n",
    "            data_filtered = data.filterBounds(area_of_interest).filterDate(date_range[0], date_range[1])\n",
    "\n",
    "            #filter on auxilary data, if any\n",
    "            for title, info in aux_data:\n",
    "                data_filtered = data_filtered.filterMetadata(title, info['relation'], info['value'])\n",
    "\n",
    "            #ensure there is at least 1 image\n",
    "            num_items = data_filtered.size().getInfo()\n",
    "            if num_items == 0:\n",
    "                print('no items found, returning started tasks.')\n",
    "                return tasks\n",
    "\n",
    "            band_info = data_filtered.first().getInfo()['bands'][0]\n",
    "\n",
    "            #if crs is already EPSG 4326, get resolution directly, otherwise need to transform from meters\n",
    "            if band_info['crs'] == 'EPSG:4326':\n",
    "                res = band_info['crs_transform'][0]\n",
    "            else:\n",
    "                res = band_info['crs_transform'][0] * METERS_TO_DECIMAL_DEGREES_CONST\n",
    "\n",
    "            #if this is the eventual primary dataset, store its resolution\n",
    "            if data_source == primary_dataset:\n",
    "                eventual_res = res\n",
    "\n",
    "            #get a mosaic as median of all returned images\n",
    "            mosaic = ee.Image(data_filtered.median())\n",
    "\n",
    "            #add this mosaic to the list\n",
    "            filtered_imgs.append(mosaic)\n",
    "        \n",
    "        #generate file name\n",
    "        fname = '%s-%s'%(region_folder.split('/')[-1], '_'.join(list(features.keys())).replace('/','_'))\n",
    "        print(fname)\n",
    "        \n",
    "        #add the various layers on top of each other to create a data cube with all features\n",
    "        final_img = ee.Image()\n",
    "        for img in filtered_imgs:\n",
    "            final_img = ee.Image.addBands(final_img,img)\n",
    "        \n",
    "        #use the ALOS qa band to filter out invalid pixels\n",
    "        if 'qa' in features['JAXA/ALOS/PALSAR/YEARLY/SAR']:\n",
    "            qa_band = final_img.select('qa')\n",
    "            qa_mask = qa_band.lt(51)\n",
    "            final_img = final_img.where(qa_mask, NO_DATA_VALUE)\n",
    "        \n",
    "        #use the Sentinel-2 SCL band to filter out invalid pixels\n",
    "        if 'SCL' in features['COPERNICUS/S2_SR']:\n",
    "            scl_band = final_img.select('SCL')\n",
    "            scl_nodata_vals = [0,3,6,8,9,10]\n",
    "            scl_mask = scl_band.eq(0)\n",
    "            for v in scl_nodata_vals:\n",
    "                scl_mask = scl_mask.Or(scl_band.eq(v))\n",
    "            final_img = final_img.where(scl_mask, NO_DATA_VALUE)\n",
    "         \n",
    "        #store the result with just the needed bands\n",
    "        selected_bands = sorted([b for bands in features.values() for b in bands if b != 'qa' and b != 'SCL'])\n",
    "        result = final_img.select(*selected_bands)\n",
    "          \n",
    "        #define the task to gather the data\n",
    "        task = ee.batch.Export.image.toDrive(image=result,\n",
    "                                             region=area_of_interest.getInfo()['coordinates'],\n",
    "                                             description=region_folder.split('/')[-1],\n",
    "                                             folder=gdrive_folder,\n",
    "                                             fileNamePrefix=fname,\n",
    "                                             crs_transform=[eventual_res, 0.0, ref_coords[0], 0.0, -eventual_res, ref_coords[1]],\n",
    "                                             crs='EPSG:4326')\n",
    "        \n",
    "        #start up the task\n",
    "        task.start()\n",
    "        \n",
    "        #store the task\n",
    "        tasks[fname] = task\n",
    "        \n",
    "        print('==================================')\n",
    "    \n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=== SET YOUR AREA OF INTEREST HERE ===\n",
    "PARAMETERS ARE: (minx, miny, maxx, maxy, path_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3 0.48799999999999955 0.43433333333333346\n"
     ]
    }
   ],
   "source": [
    "generate_region_folders(-68.894, 2.296, -67.43, 3.599, '%s/region'%BASE_ROI_FOLDER)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=== SET YOUR PARAMETERS HERE ===\n",
    "\n",
    "search_area_name: the prefix of the folders you would like to process\n",
    "features: which features (data source + bands) to use in the wetland prediction\n",
    "date_range: the date range to get data from Google Earth Engine\n",
    "gdrive_folder: the name of the folder on Google Drive where your data will go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_area_name = 'region_2'\n",
    "folders_to_process = ['%s/%s'%(BASE_ROI_FOLDER, item) for item in os.listdir(BASE_ROI_FOLDER) if search_area_name in item]\n",
    "features = {'JAXA/ALOS/PALSAR/YEARLY/SAR': ['HH', 'HV', 'qa'], 'COPERNICUS/S2_SR': ['B2', 'B3', 'B4', 'B8', 'SCL']}\n",
    "date_range = ['2017-01-01', '2019-01-01']\n",
    "gdrive_folder = 'GoogleEarthEngine'\n",
    "aux_data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on region folder: region_2_0...\n",
      "Created Baseline Wetlands Raster...\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "region_2_0-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR\n",
      "==================================\n",
      "Working on region folder: region_2_1...\n",
      "Created Baseline Wetlands Raster...\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "region_2_1-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR\n",
      "==================================\n",
      "Working on region folder: region_2_2...\n",
      "Created Baseline Wetlands Raster...\n",
      "Working on data source: JAXA/ALOS/PALSAR/YEARLY/SAR...\n",
      "Working on data source: COPERNICUS/S2_SR...\n",
      "region_2_2-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "tasks = get_bands_from_region(folders_to_process, features, gdrive_folder, date_range, aux_data, 'COPERNICUS/S2_SR')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After launching tasks, you can periodically run the below cell to check the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_2_0-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR COMPLETED\n",
      "region_2_1-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR COMPLETED\n",
      "region_2_2-JAXA_ALOS_PALSAR_YEARLY_SAR_COPERNICUS_S2_SR COMPLETED\n"
     ]
    }
   ],
   "source": [
    "for name,task in tasks.items():\n",
    "    print(name, task.status()['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
