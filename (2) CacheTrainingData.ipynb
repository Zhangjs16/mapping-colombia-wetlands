{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to download the Google Earth Engine data from Google Drive and generate a training data set for each sub region. Each training data set has two parts: an interior training set and an exterior training set, both defined below:\n",
    "\n",
    "interior training set: using regions of the baseline map of wetlands which are confidently marked as a specfic sub-type of wetland (eg. Marsh, Mangrove, Swamp), we get an N x k data set where N is the number of training points and k is the number of bands\n",
    "\n",
    "interior training set: using regions of the baseline map of wetlands which are confidently marked as NOT a specfic sub-type of wetland (eg. Marsh, Mangrove, Swamp), we get an N x k data set where N is the number of training points and k is the number of bands\n",
    "\n",
    "All training data is stored in a Python dictionary which is then pickled (https://wiki.python.org/moin/UsingPickle) into a file called stored_training_data.p which will be used in the next notebook: (3) WetlandPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import pickle\n",
    "from common_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_ROI_FOLDER = 'regions'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=== SET THIS ===\n",
    "the Google Drive folder id where all data was stored in (1) DataDownload.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_EARTH_ENGINE_GDRIVE_FOLDER_ID = '1KvlrUHs_rN7xPlw53qtd9pweeLwmrJSP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Download File from Google Drive"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following three functions are used to download a public file from Google Drive given the file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_google_drive(file_id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : file_id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : file_id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Cache Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interior_exterior_edge_values(mode, arr_train_labels, split_arr_features_train, no_data_value, kernel_size=3, thresh_learning=0.1, data_limit=100000):\n",
    "    \"\"\"\n",
    "    This function gets training data for areas of the region which are confidently a sub-type of wetland\n",
    "    and for areas of the region which are confidently NOT a sub-type of wetland. It proceeds with the following steps:\n",
    "        (1) apply a high pass filter on the binary wetland labels\n",
    "        (2) confident wetlands (interior pixels) are those having low filtered values \n",
    "            and designated as wetlands in the baseline\n",
    "        (3) confident NON wetlands (exterior pixels) are those having low filtered values \n",
    "            and designated as non-wetlands in the baseline\n",
    "        (4) edge pixels are those having high filtered values \n",
    "        (5) return a sampling of interior, exterior, and/or edge pixels\n",
    "    \n",
    "    mode: a list with any of \"interior\", \"exterior\", \"edge\". Designates which data to return.\n",
    "    arr_train_labels: the array of training data wetland labels\n",
    "    split_arr_features_train: the features to use for training\n",
    "    no_data_value: the vaule used to signify no data at a pixel\n",
    "    kernel_size: the kernel size used in the process of identifying interior and enxterior points\n",
    "    thresh_learning: between 0 and 1, this threshold is used to distinuish interior (wetland) and exterior (non wetland) pixels\n",
    "    data_limit: the max number of interior/exterior/edge points to return\n",
    "    \"\"\"\n",
    "    \n",
    "    #get number of bands\n",
    "    num_bands = split_arr_features_train.shape[-1]\n",
    "    \n",
    "    #let the edge detection threshold be half of the learning threshold\n",
    "    thresh_edge_detection = thresh_learning / 2\n",
    "    \n",
    "    #apply a filter to the baseline wetlands image to find edge regions\n",
    "    kernel = -np.ones((kernel_size,kernel_size)) / (kernel_size**2 - 1)\n",
    "    kernel[kernel_size//2,kernel_size//2] = 1\n",
    "    \n",
    "    vicinity_score = abs(apply_convolution(arr_train_labels, kernel))\n",
    "    \n",
    "    interior, exterior, edges = None, None, None\n",
    "    \n",
    "    if 'interior' in mode:\n",
    "        #interior points are wetlands confidently below the threshold\n",
    "        interior_condition = (arr_train_labels==1)&(vicinity_score < thresh_learning)\n",
    "        interiors = np.where(interior_condition == 1)\n",
    "        interior_vicinity_scores = vicinity_score[interiors]\n",
    "        \n",
    "        if len(interiors[0]) == 0:\n",
    "            print('empty interior')\n",
    "            return None\n",
    "        \n",
    "        #get the values in the SAR image lining up with interior points, sample according to confidence\n",
    "        chosen_interiors = sample_by_vicinity_scores(interiors, interior_vicinity_scores, thresh_learning)\n",
    "        interior_vals = split_arr_features_train[chosen_interiors].reshape(-1, num_bands)\n",
    "        interior_vals = interior_vals[(interior_vals != no_data_value).all(axis=1)]\n",
    "        interior_sampled = np.random.choice(np.arange(interior_vals.shape[0]), min(data_limit, interior_vals.shape[0]), replace=False)\n",
    "        interior_vals = interior_vals[interior_sampled]\n",
    "        \n",
    "    if 'exterior' in mode:\n",
    "        #exterior points are non-wetlands confidently below the threshold\n",
    "        exterior_condition = (arr_train_labels==0)&(vicinity_score < thresh_learning)\n",
    "        exteriors = np.where(exterior_condition == 1)\n",
    "        exterior_vicinity_scores = vicinity_score[exteriors]\n",
    "        \n",
    "        if len(exteriors[0]) == 0:\n",
    "            print('empty exterior')\n",
    "            return None\n",
    "        \n",
    "        #get the values in the SAR image lining up with exterior points, sample according to confidence\n",
    "        chosen_exteriors = sample_by_vicinity_scores(exteriors, exterior_vicinity_scores, thresh_learning)\n",
    "        exterior_vals = split_arr_features_train[chosen_exteriors].reshape(-1, num_bands)\n",
    "        exterior_vals = exterior_vals[(exterior_vals != no_data_value).all(axis=1)]\n",
    "        exterior_sampled = np.random.choice(np.arange(exterior_vals.shape[0]), min(data_limit, exterior_vals.shape[0]), replace=False)\n",
    "        exterior_vals = exterior_vals[exterior_sampled]\n",
    "        \n",
    "    if 'edge' in mode:\n",
    "        #edge points are above the threshold\n",
    "        edge_condition = (vicinity_score >= thresh_learning)\n",
    "        edges = np.where(edge_condition == 1)\n",
    "        \n",
    "        if len(edges[0]) == 0:\n",
    "            print('empty edge')\n",
    "            return None\n",
    "    \n",
    "    return interior_vals, exterior_vals, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder Name to File ID"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We need to populate the below dictionary, which will map region sub-directory names to their respective TIFF file ids on Google Drive. One option is to do this manually, which is rather tedious and copy/paste error prone especially if you have lots of files. A much beter idea is to use the Google Drive Python API, but does require a bit of setup, well documented in the following link:\n",
    "\n",
    "https://developers.google.com/drive/api/v3/quickstart/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name_to_file_id = {}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After going through all steps on that page, set the below constant and run the following code to automatically populate the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "creds = None\n",
    "if os.path.exists('token.pickle'):\n",
    "    with open('token.pickle', 'rb') as token:\n",
    "        creds = pickle.load(token)\n",
    "        \n",
    "service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "result = service.files().list(q=\"parents in '%s'\"%GOOGLE_EARTH_ENGINE_GDRIVE_FOLDER_ID).execute()\n",
    "\n",
    "folder_name_to_file_id = {info['name'].split('-')[0]: info['id'] for info in result['files'] if len(info['name'].split('-')) == 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'region_2_2': '1qiG6uXSjVQbJLu5519uaRm8-HjUFqts8',\n",
       " 'region_2_0': '1h3nBXtIdwUVjKTLetElPRfg8l67epC3D',\n",
       " 'region_2_1': '1tUGJ1qzGtIkfJ60McV0xGYyVnwIMW6I9'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_name_to_file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=== SET THIS: CHOOSE THE WETLAND SUB-TYPE YOU WOULD LIKE TO PREDICT ===\n",
    "Options: \n",
    "\"Mangrove\", \"Swamp / Bog\", \"Fen\", \"Riverine\", \"Floodswamp\", \"Floodplain\", \"Marsh\", \"Wetland in dry areas\", \"Wet meadow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wetland_type = 'Swamp / Bog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_training_data = {'aux_info': {'wetland_type': wetland_type}, 'training_data': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing region_2_2...\n",
      "Downloading training data from drive...\n",
      "Getting formatted features and labels...\n",
      "Getting Interior/Exterior/Edge info...\n",
      "Storing in dictionary...\n",
      "=======================================\n",
      "Processing region_2_0...\n",
      "Downloading training data from drive...\n",
      "Getting formatted features and labels...\n",
      "Getting Interior/Exterior/Edge info...\n",
      "Storing in dictionary...\n",
      "=======================================\n",
      "Processing region_2_1...\n",
      "Downloading training data from drive...\n",
      "Getting formatted features and labels...\n",
      "Getting Interior/Exterior/Edge info...\n",
      "Storing in dictionary...\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "for roi_folder, fid in folder_name_to_file_id.items():\n",
    "    \n",
    "    print('Processing %s...'%roi_folder)\n",
    "    features_file_name = '%s/%s/features_%s.tiff'%(BASE_ROI_FOLDER, roi_folder, roi_folder)\n",
    "    baseline_file_name = '%s/%s/baseline_%s.tiff'%(BASE_ROI_FOLDER, roi_folder, roi_folder)\n",
    "    \n",
    "    #check if data already downloaded\n",
    "    if features_file_name.split('/')[-1] not in os.listdir('%s/%s'%(BASE_ROI_FOLDER, roi_folder)):\n",
    "        print('Downloading training data from drive...')\n",
    "        download_file_from_google_drive(fid, features_file_name)\n",
    "     \n",
    "    #check if training data already cached\n",
    "    if roi_folder not in stored_training_data['training_data']:\n",
    "    \n",
    "        ds_features = gdal.Open(features_file_name, gdal.GA_ReadOnly)\n",
    "        ds_labels = gdal.Open(baseline_file_name, gdal.GA_ReadOnly)\n",
    "\n",
    "        print('Getting formatted features and labels...')\n",
    "        arr_labels, split_arr_features, gt = preprocess_data_set_pair(ds_features, ds_labels, wetland_type) \n",
    "\n",
    "        print('Getting Interior/Exterior/Edge info...')\n",
    "        interior_vals, exterior_vals, edges = get_interior_exterior_edge_values(['interior', 'exterior'], arr_labels, split_arr_features, NO_DATA_VALUE)\n",
    "\n",
    "        print('Storing in dictionary...')\n",
    "        stored_training_data['training_data'][roi_folder] = {}\n",
    "        stored_training_data['training_data'][roi_folder]['interior'] = interior_vals\n",
    "        stored_training_data['training_data'][roi_folder]['exterior'] = exterior_vals\n",
    "\n",
    "        pickle.dump(stored_training_data, open(\"stored_training_data.p\", \"wb\"))\n",
    "\n",
    "        print('=======================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
